\begin{abstract}

Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM. However, existing methods still rely on \textit{autoregressive drafting}, which remains sequential and constrains practical speedups.
Diffusion LLMs offer a promising alternative by enabling parallel generation, but current diffusion models typically underperform compared with autoregressive models.
In this paper, we introduce \textbf{DFlash}, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. We show that speculative decoding provides a natural and effective setting for diffusion models. By generating draft tokens in a single forward pass, DFlash enables efficient drafting, and by conditioning the draft model on context features extracted from the target model, it achieves high-quality drafts with higher acceptance rates.
Experiments show that DFlash achieves over 6$\times$ lossless acceleration across a range of models and tasks, delivering up to 2.5$\times$ higher speedup than the state-of-the-art speculative decoding method EAGLE-3.

\textbf{Links:}\hspace{2pt} \href{https://github.com/z-lab/dflash}{Code} (GitHub) $\vert$ \href{https://hf.co/collections/z-lab/dflash}{Models} (Hugging Face)

\end{abstract}
