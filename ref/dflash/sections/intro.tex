\section{Introduction}

Large language models (LLMs) have enabled a wide range of powerful applications, including conversational agents~\cite{yang2025qwen3technicalreport, Guo_2025} and automated programming tools. Despite their success, LLM inference remains dominated by a sequential, token-by-token generation process, where each output depends on the full preceding context. This inherent seriality creates a major performance bottleneck: inference is slow, memory-bound, and fails to fully utilize modern GPUs. With the recent emergence of long Chain-of-Thought (CoT) reasoning models \cite{openai2024openaio1card, Guo_2025}, this bottleneck has become increasingly critical, as prolonged inference times now dominate the generation process.

Speculative decoding \cite{leviathan2023fastinferencetransformersspeculative, li2025eaglespeculativesamplingrequires, li2024eagle2fasterinferencelanguage, li2025eagle3scalinginferenceacceleration, cai2024medusasimplellminference} has emerged as a primary solution to this bottleneck. This paradigm employs a lightweight \textit{draft model} to speculate a sequence of future tokens, which are then verified in parallel by the large \textit{target model}. While this approach achieves lossless acceleration and has been widely integrated into production frameworks, state-of-the-art methods like EAGLE-3 \cite{li2025eagle3scalinginferenceacceleration} still rely on autoregressive drafting. This serial drafting process is not only inherently inefficient but also susceptible to error accumulation, which effectively caps achievable speedups at approximately 2$-$3$\times$.

Recently, Diffusion LLMs (dLLMs) \cite{nie2025largelanguagediffusionmodels} offer a promising alternative to autoregressive LLMs by enabling parallel text generation and bidirectional context modeling. Block diffusion models \cite{arriola2025blockdiffusioninterpolatingautoregressive,cheng2025sdarsynergisticdiffusionautoregressionparadigm, wu2025fastdllmv2efficientblockdiffusion} can denoise a block of masked tokens simultaneously. However, current open-source dLLMs typically underperform their autoregressive counterparts in terms of generation quality. Furthermore, maintaining acceptable output quality often necessitates a high number of denoising steps, which significantly diminishes their raw inference speed \cite{qian2026d3llmultrafastdiffusionllm}.

This landscape reveals a critical trade-off: autoregressive models deliver superior performance but suffer from sequential latency, while diffusion models allow for fast, parallel generation but often at the cost of accuracy. A natural research question follows: \textit{Can we combine the strengths of both paradigms while mitigating their respective weaknesses?} A compelling solution lies in leveraging diffusion models for high-speed, parallel drafting, while relying on high-quality autoregressive models for verification to ensure the final output remains lossless.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/dflash_speedup.pdf}
    \caption{Speedup comparison between DFlash, EAGLE-3 against Autoregressive Decoding on Qwen3-8B~\citep{yang2025qwen3technicalreport} with the Transformers backend. Overall, DFlash achieves more than 2.5Ã— higher speedup than EAGLE-3.}
    \label{fig:speedup}
\end{figure*}

However, utilizing diffusion for drafting is non-trivial, and existing methods are either impractical or offer limited speedups. Methods such as DiffuSpec~\cite{li2025diffuspecunlockingdiffusionlanguage} and SpecDiff-2~\cite{sandler2025specdiff2scalingdiffusiondrafter} utilize massive (\eg, 7B parameter) draft models. This significant memory footprint is often prohibitively expensive for real-world serving. Furthermore, while these large drafters offer relatively high quality draft tokens and acceptance lengths, the high drafting latency limits their practical speedups to a modest 3$-$4$\times$. In contrast, PARD \cite{an2025pardacceleratingllminference} trains small autoregressive models to mimic diffusion-style parallel generation, and then perform speculative decoding for target LLMs. However, the resulting small models lack the modeling capacity of the target LLMs, leading to limited acceptance lengths and a speedup ceiling of approximately 3$\times$.

\textit{Is there truly ``no free lunch''? Can we build a diffusion drafter that is both lightweight and highly accurate?}

In this paper, we introduce \method, a speculative decoding framework that uses a lightweight block diffusion model to achieve both fast and high-quality drafting. Our key insight is simple: \textbf{the target knows best}. As observed by \citet{samragh2025llmknowsfutureuncovering}, large autoregressive LLMs' hidden features implicitly contain information about multiple future tokens. \method utilizes these hidden features as context, conditioning the draft model to predict future blocks of tokens in parallel. In effect, the draft model becomes a diffusion adapter that efficiently leverages the deep context features modeled by the large target model. Instead of requiring a tiny draft model to reason from scratch, \method fuses the reasoning capabilities of the target model with the parallel generation speed of a small diffusion drafter. 

We evaluate \method across a wide range of models and benchmarks, and demonstrate its practical benefits under realistic serving setups using SGLang~\cite{zheng2024sglangefficientexecutionstructured}. As shown in \autoref{fig:speedup}, \method achieves up to a \textbf{6.1$\boldsymbol{\times}$} speedup on Qwen3-8B~\citep{yang2025qwen3technicalreport}, and is nearly \textbf{2.5$\boldsymbol{\times}$} faster than the state-of-the-art EAGLE-3 across most benchmarks. We believe \method represents a significant step forward in accelerating LLM inference and democratizing high-performance AI.
