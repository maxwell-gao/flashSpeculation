\section{Experiments}
\label{sec:experiments}

\textbf{Models and Evaluations.  } We conduct experiments on LLaMA-3.1 Instruct (8B) and Qwen3 (4B, 8B, Coder-30B-A3B-Instruct) pre-trained models. We evaluate tasks in three categories: \textbf{Math:} GSM8K~\citep{cobbe2021training}, MATH~\citep{lightman2023let}, and AIME25~\citep{AIME}; \textbf{Code:} HumanEval~\citep{chen2021evaluating}, MBPP~\citep{austin2021program}, and LiveCodeBench~\citep{jain2024livecodebench}; \textbf{Chat:} MT-Bench~\citep{zheng2023judging} and Alpaca~\citep{alpaca}. For each task, we assess the performance of the draft models using average acceptance length ($\tau$) and end-to-end decoding speedup over the autoregressive baseline. We conduct all experiments on NVIDIA H200 GPUs unless otherwise specified.

\textbf{Datasets.  } To provide a diverse set of training data, we collect a mixture of around 800K samples from NVIDIA Nemotron Post-Training Dataset V2~\citep{NemotronPostTrainingDatasetV2} and CodeAlpaca~\citep{codealpaca}.
Instead of directly using the original dataset, we construct our training set with the responses generated by the target model for better target alignment.

\textbf{Implementation.  } For \method draft models, we set the number of layers to 5 (8 for Qwen3 Coder) and use a block size of 16 (10 for LLaMA 3.1). The target hidden features are extracted from 5 layers uniformly selected between the second layer and the third-to-last layer of the target model. More training details are presented in \autoref{appd:implementation}.

\textbf{Baselines.  } We compare \method with the vanilla autoregressive decoding (baseline) and state-of-the-art speculative decoding method EAGLE-3~\citep{li2025eagle3scalinginferenceacceleration}. We did not include comparisons with other dLLM-based speculative decoding methods~\citep{liu2025tidarthinkdiffusiontalk,samragh2025llmknowsfutureuncovering,li2025diffuspecunlockingdiffusionlanguage,sandler2025specdiff2scalingdiffusiondrafter} due to lack of open-source implementation.
For comparisons with EAGLE-3 on Qwen3 models (\autoref{sec:exp-instruct}), we use the checkpoints released by AngelSlim~\citep{AngelSlim2025}; for LLaMA-3.1-Instruct (\autoref{sec:exp-llama}), we use the official checkpoint released by EAGLE-3 team.

\subsection{Instruct Models}
\label{sec:exp-instruct}

In this section, we evaluate \method against EAGLE-3 on Qwen3 models with thinking mode disabled, using the Transformers backend. For EAGLE-3, we consider two settings: a tree size of 16, which matches \method with block size 16 for a fair drafting-budget comparison, and a tree size of 60, as used in the EAGLE-3 paper to maximize acceptance length with higher verification cost. In both cases, the draft steps and top-$k$ are set to 7 and 10, respectively.

As shown in \autoref{tab:main-results}, \method consistently outperforms EAGLE-3 across all tasks and settings. Under greedy decoding (temperature = 0), \method achieves an average speedup of 4.9$\times$ over the autoregressive baseline, corresponding to a 2.4$\times$ improvement over EAGLE-3 (16). Under non-greedy sampling (temperature = 1), \method maintains a 4.1$\times$ speedup over baseline and a 2.2$\times$ improvement over EAGLE-3. Notably, \method also surpasses EAGLE-3 with tree size 60, achieving higher acceptance length while incurring substantially lower verification overhead. These results demonstrate the effectiveness and efficiency of diffusion-based drafting in \method.

\subsection{Reasoning Models}
\label{sec:exp-reason}

In this section, we evaluate \method for Qwen3 models with thinking mode enabled using Transformers. The draft models are trained on target-model outputs with reasoning traces.

As shown in \autoref{tab:reasoning-results}, \method maintains the high acceptance length, achieving speedups of roughly 4.5$\times$ and 3.9$\times$ over the baseline. This efficiency gain is particularly valuable for the practical deployment of reasoning models, given their prolonged generation time.

\input{tables/results/reasoning}

\subsection{Performance on SGLang}
\label{sec:exp-sgl}
\begin{table}[h!]
\caption{Throughput (tok/s), speedup over baseline, and average acceptance length $\tau$ on SGLang (FA4 backend).}
\label{tab:sglang-all}
\resizebox{\linewidth}{!}{
\footnotesize
\centering
\setlength{\tabcolsep}{2pt}
\renewcommand{\arraystretch}{0.9}
\begin{tabular}{l l ccccc c}
\toprule
\multirow{2}[2]{*}{Task} & \multirow{2}[2]{*}{Method} 
& \multicolumn{5}{c}{Concurrency} & \multirow{2}[2]{*}{\textit{Avg.} $\tau$} \\
\cmidrule(lr){3-7}
& & 1 & 4 & 8 & 16 & 32 &  \\
\midrule

\multicolumn{8}{l}{\textbf{Qwen3-4B}} \\
\midrule
\multirow{3}[2]{*}{Math500}
& Baseline
& 316 & 1145 & 2201 & 4100 & 7136 & -- \\
\cmidrule{2-8}
& \multirow{2}{*}{\method}
& 1531 & 4943 & 9066 & 14477 & 20417 & \multirow{2}{*}{8.01} \\
& & 4.8$\times$ & 4.3$\times$ & 4.1$\times$ & 3.5$\times$ & 2.9$\times$
&  \\

\midrule
\multirow{3}[2]{*}{\makecell[l]{Human-\\Eval}}
& Baseline
& 312 & 1162 & 2217 & 4184 & 7143 & -- \\
\cmidrule{2-8}
& \multirow{2}{*}{\method}
& 1247 & 4147 & 6997 & 11234 & 15703 & \multirow{2}{*}{6.63} \\
&
& 4.0$\times$ & 3.6$\times$ & 3.2$\times$ & 2.7$\times$ & 2.2$\times$
&  \\

\midrule
\multicolumn{8}{l}{\textbf{Qwen3-8B}} \\
\midrule
\multirow{3}[2]{*}{Math500}
& Baseline
& 230 & 861 & 1666 & 3133 & 5694 & -- \\
\cmidrule{2-8}
& \multirow{2}{*}{\method}
& 1175 & 3884 & 7485 & 12268 & 16076 & \multirow{2}{*}{8.01} \\
&
& 5.1$\times$ & 4.5$\times$ & 4.5$\times$ & 3.9$\times$ & 2.8$\times$
& \\

\midrule
\multirow{3}[2]{*}{\makecell[l]{Human-\\Eval}}
& Baseline
& 229 & 868 & 1649 & 3253 & 5462 & -- \\
\cmidrule{2-8}
& \multirow{2}{*}{\method}
& 955 & 3092 & 6010 & 9919 & 13116 & \multirow{2}{*}{6.50} \\
&
& 4.2$\times$ & 3.6$\times$ & 3.6$\times$ & 3.0$\times$ & 2.4$\times$
& \\

\midrule
\multicolumn{8}{l}{\textbf{Qwen3-Coder-30B-A3B}} \\
\midrule
\multirow{3}[2]{*}{\makecell[l]{Human-\\Eval}}
& Baseline
& 229 & 686 & 1068 & 1681 & 2713 & -- \\
\cmidrule{2-8}
& \multirow{2}{*}{\method}
& 802 & 2078 & 3442 & 5429 & 8314 & \multirow{2}{*}{8.09} \\
&
& 3.5$\times$ & 3.0$\times$ & 3.2$\times$ & 3.2$\times$ & 3.1$\times$
& \\

\midrule
\multirow{3}[2]{*}{LCB}
& Baseline
& 220 & 681 & 1112 & 1733 & 2823 & -- \\
\cmidrule{2-8}
& \multirow{2}{*}{\method}
& 569 & 1621 & 2554 & 4160 & 6401 & \multirow{2}{*}{6.42} \\
&
& 2.6$\times$ & 2.4$\times$ & 2.3$\times$ & 2.4$\times$ & 2.3$\times$
& \\

\midrule
\multirow{3}[2]{*}{MBPP}
& Baseline
& 228 & 682 & 1057 & 1697 & 2735 & -- \\
\cmidrule{2-8}
& \multirow{2}{*}{\method}
& 720 & 2052 & 3360 & 5522 & 8538 & \multirow{2}{*}{7.23} \\
&
& 3.2$\times$ & 3.0$\times$ & 3.2$\times$ & 3.3$\times$ & 3.1$\times$
& \\

\bottomrule
\end{tabular}
}
\end{table}
In this section, we evaluate the performance of DFlash on the popular open-source inference framework SGLang using Qwen3-4B, Qwen3-8B, and Qwen3-Coder-30B-A3B-Instruct. All experiments are conducted on a single B200 GPU with the FlashAttention-4 (FA4) backend. We enable Spec-v2 scheduling overlap to maximize achievable throughput. 

As shown in \autoref{tab:sglang-all}, DFlash consistently provides speedups across all three models over concurrency levels ranging from 1 to 32, achieving up to a 5.1$\times$ speedup on Qwen3-8B. These results demonstrate the practical value of DFlash in real-world serving scenarios, where it can substantially reduce serving cost.

\subsection{Ablation Study}
\label{sec:exp-ablation}
In this section, we ablate the impact of training data and several key design choices of the DFlash draft model. Unless otherwise specified, all ablation models are trained on 100K samples randomly drawn from the full data mixture. All experiments are conducted on a single H200 GPU with greedy decoding, except those evaluated on SGLang.

\subsubsection{Training Data}
\label{sec:exp-llama}
\begin{table}[h!]
\caption{Speedup over baseline and average acceptance length $\tau$ for LLaMA-3.1-8B-Instruct on SGLang (Flashinfer backend, single B200 GPU). Baseline reports absolute throughput (TPS; tokens/s). EAGLE-3 uses 7 draft steps with top-$k{=}10$ and either 10 or 60 draft tokens. DFlash uses block size 10.}
\label{tab:dflash_vs_eagle3_llama31_acc}
\centering
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{0.95}

\resizebox{\linewidth}{!}{
\begin{tabular}{l ccccc c}
\toprule
\multirow{2}[2]{*}{Method} & \multicolumn{5}{c}{Concurrency} & \multirow{2}[2]{*}{\textit{Avg.} $\tau$} \\
\cmidrule(lr){2-6}
& 1 & 4 & 8 & 16 & 32 &  \\
\midrule

\multicolumn{7}{l}{\textbf{GSM8K}} \\
\midrule
Baseline (TPS)
& 249 & 923 & 1739 & 3245 & 5349 & -- \\

EAGLE-3 (10)
& 1.6$\times$ & 1.5$\times$ & 1.4$\times$ & 1.2$\times$ & 1.0$\times$
& 3.49 \\

EAGLE-3 (60)
& 1.9$\times$ & 1.6$\times$ & 1.3$\times$ & 0.9$\times$ & 0.6$\times$
& 4.55 \\

\textbf{DFlash (10)}
& \textbf{2.4$\times$} & \textbf{2.2$\times$} & \textbf{2.1$\times$} & \textbf{1.8$\times$} & \textbf{1.6$\times$}
& \textbf{4.32} \\

\midrule
\multicolumn{7}{l}{\textbf{HumanEval}} \\
\midrule
Baseline (TPS)
& 245 & 922 & 1778 & 3336 & 5854 & -- \\

EAGLE-3 (10)
& 2.0$\times$ & 1.9$\times$ & 1.8$\times$ & 1.5$\times$ & 1.2$\times$
& 3.62 \\

EAGLE-3 (60)
& 2.0$\times$ & 1.7$\times$ & 1.3$\times$ & 0.9$\times$ & 0.6$\times$
& 4.65 \\

\textbf{DFlash (10)}
& \textbf{2.8$\times$} & \textbf{2.6$\times$} & \textbf{2.5$\times$} & \textbf{2.1$\times$} & \textbf{1.8$\times$}
& \textbf{4.91} \\

\midrule
\multicolumn{7}{l}{\textbf{Alpaca}} \\
\midrule
Baseline (TPS)
& 245 & 906 & 1745 & 3237 & 5434 & -- \\

EAGLE-3 (10)
& 1.5$\times$ & 1.4$\times$ & 1.4$\times$ & 1.1$\times$ & 0.9$\times$
& 3.11 \\

EAGLE-3 (60)
& 1.8$\times$ & 1.5$\times$ & 1.2$\times$ & 0.8$\times$ & 0.5$\times$
& 4.07 \\

\textbf{DFlash (10)}
& \textbf{2.2$\times$} & \textbf{2.0$\times$} & \textbf{1.8$\times$} & \textbf{1.5$\times$} & \textbf{1.4$\times$}
& \textbf{3.73} \\

\bottomrule
\end{tabular}
}
\end{table}
We compare DFlash against EAGLE-3 on LLaMA-3.1-8B-Instruct. DFlash is trained on UltraChat~\citep{ding2023enhancingchatlanguagemodels} and ShareGPT, using the exactly same training data as EAGLE-3, and is evaluated against the official EAGLE-3 checkpoints. The DFlash draft model uses a block size of 10, with other configurations matching those of the DFlash Qwen3-8B draft model. All experiments are conducted using SGLang with Spec-v1 (without scheduling overlap), as Spec-v2 does not support tree-based drafting for EAGLE-3. Evaluations are performed on a single B200 GPU.

As shown in \autoref{tab:dflash_vs_eagle3_llama31_acc}, DFlash consistently outperforms EAGLE-3 across all tasks, concurrency levels, and EAGLE-3 tree-size configurations. This performance gap holds for math, code, and chat benchmarks, demonstrating the robustness and efficiency advantages of DFlash over autoregressive tree-based speculative decoding.


\subsubsection{Number of Draft Layers}
\label{sec:ablation_layers}
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{0.9}
\caption{5-layer draft model has the best average speedup. All DFlash draft models are trained with block size 16 and hidden features extracted from 5 layers of the target model.}
\label{tab:ablation_draft_layers}
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{c cc cc cc}
\toprule
\multirow{2}[2]{*}{Setting}
& \multicolumn{2}{c}{Math500} 
& \multicolumn{2}{c}{HumanEval} 
& \multicolumn{2}{c}{MT-Bench} \\
\cmidrule{2-7}
& Speedup & $\tau$
& Speedup & $\tau$
& Speedup & $\tau$ \\
\midrule
3-L & 4.69$\times$ & 5.64 & 3.90$\times$ & 4.61 & 2.38$\times$ & 3.18 \\
5-L & 4.71$\times$ & 5.99 & 3.96$\times$ & 4.94 & 2.35$\times$ & 3.37 \\
8-L & 4.64$\times$ & 6.33 & 3.96$\times$ & 5.29 & 2.23$\times$ & 3.50 \\
\bottomrule
\end{tabular}
\end{table}
One advantage of DFlash is that acceptance length scales effectively with the depth of the draft model. However, this comes with a trade-off between drafting cost and draft quality. Deeper draft models are more expressive and achieve higher acceptance lengths, but they also incur higher drafting latency. As a result, the optimal number of layers depends on the deployment setting. As shown in \autoref{tab:ablation_draft_layers}, while the 8-layer draft model achieves longer acceptance lengths, the 5-layer model attains higher overall speedup due to a better balance between drafting cost and quality.

\subsubsection{Number of Target Hidden Features}
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{0.9}
\caption{More hidden features from target model increases the acceptance length. All DFlash draft models use 3 draft layers and are trained with block size 16.}
\label{tab:ablation_target_hiddens}
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{ccccccc}
\toprule
\multirow{2}[2]{*}{Setting}
& \multicolumn{2}{c}{Math500} 
& \multicolumn{2}{c}{HumanEval} 
& \multicolumn{2}{c}{MT-Bench} \\
\cmidrule{2-7}
& Speedup & $\tau$ 
& Speedup & $\tau$
& Speedup & $\tau$\\
\midrule
3-H & 4.49$\times$ & 5.38 & 3.80$\times$ & 4.47 & 2.32$\times$ & 3.07 \\
5-H & 4.69$\times$ & 5.64 & 3.90$\times$ & 4.61 & 2.38$\times$ & 3.18 \\
\bottomrule
\end{tabular}
\end{table}
The number of target hidden features affects both acceptance length and end-to-end speedup. Extracting features from more target layers provides richer semantic and future-token information, improving draft quality. As shown in \autoref{tab:ablation_target_hiddens}, conditioning on five hidden features consistently outperforms using three. However, this benefit comes at higher training cost: in offline training, the storage required to cache target hidden states increases linearly with the number of extracted features.


\subsubsection{Training-Inference Time Block Size}
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{0.9}
\caption{Ablation study of trainingâ€“inference block size (BS) mismatch. All draft models use 8 layers and 5 target hidden features.}
\label{tab:ablation_block_size}
\resizebox{\columnwidth}{!}{
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{cccccccc}
\toprule
\multirow{2}[2]{*}{\makecell{Train\\BS}}
& \multirow{2}[2]{*}{\makecell{Test\\BS}}
& \multicolumn{2}{c}{Math500} 
& \multicolumn{2}{c}{HumanEval} 
& \multicolumn{2}{c}{MT-Bench} \\
\cmidrule(){3-8}
& 
& Speedup & $\tau$
& Speedup & $\tau$
& Speedup & $\tau$ \\
\midrule
b16 & b16 & 4.64x & 6.33 & 3.96x & 5.29 & 2.23x & 3.50 \\
b16 & b8  & 3.87x & 5.09 & 3.39x & 4.44 & 2.12x & 3.18 \\
\midrule
b8  & b16 & 3.78x & 5.02 & 3.24x & 4.28 & 2.09x & 3.09 \\
b8  & b8  & 3.97x & 5.21 & 3.53x & 4.61 & 2.22x & 3.29 \\
\bottomrule
\end{tabular}
}
\end{table}
Block size is a critical design choice for the DFlash draft model. An equally important question is whether a pretrained DFlash model can generalize from its training-time block size to different block sizes during inference. To study this, we train two draft models with block sizes 8 and 16 on the same data and evaluate their inference-time scaling behavior, as shown in \autoref{tab:ablation_block_size}.

When training and inference block sizes match (8$\rightarrow$8 and 16$\rightarrow$16), the block-size-16 model achieves substantially higher acceptance lengths on math and coding tasks. Acceptance histograms on Math500 reveal that the block-8 model frequently fully accepts entire blocks (35.7\%), suggesting that block size 8 is often underutilized. In contrast, the block-16 model exhibits a more spread-out acceptance distribution with higher average acceptance length, indicating more effective use of larger blocks.

We further examine cross-block-size generalization at inference time and observe a clear asymmetry. A model trained with a larger block size generalizes well to smaller inference-time block sizes: using block size 8 with a model trained at block size 16 yields acceptance lengths close to those of a model trained and evaluated at block size 8. However, the reverse does not hold.

\textbf{Overall, DFlash models trained with larger block sizes generalize well to smaller inference-time block sizes.} This property enables dynamic block-size scheduling during inference to improve end-to-end efficiency. In practical serving scenarios, large blocks can increase verification cost under compute-bound settings (\eg, large batch sizes); reducing the block size in such cases can therefore yield better overall speedup. We leave adaptive block-size scheduling to future work.
