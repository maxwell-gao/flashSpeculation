@misc{sadhukhan2025magicdecbreakinglatencythroughputtradeoff,
      title={MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding}, 
      author={Ranajoy Sadhukhan and Jian Chen and Zhuoming Chen and Vashisth Tiwari and Ruihang Lai and Jinyuan Shi and Ian En-Hsu Yen and Avner May and Tianqi Chen and Beidi Chen},
      year={2025},
      eprint={2408.11049},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.11049}, 
}

@misc{an2025pardacceleratingllminference,
      title={PARD: Accelerating LLM Inference with Low-Cost PARallel Draft Model Adaptation}, 
      author={Zihao An and Huajun Bai and Ziqiong Liu and Dong Li and Emad Barsoum},
      year={2025},
      eprint={2504.18583},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2504.18583}, 
}

@misc{li2025eaglespeculativesamplingrequires,
      title={EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty}, 
      author={Yuhui Li and Fangyun Wei and Chao Zhang and Hongyang Zhang},
      year={2025},
      eprint={2401.15077},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.15077}, 
}

@misc{li2024eagle2fasterinferencelanguage,
      title={EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees}, 
      author={Yuhui Li and Fangyun Wei and Chao Zhang and Hongyang Zhang},
      year={2024},
      eprint={2406.16858},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.16858}, 
}

@misc{li2025eagle3scalinginferenceacceleration,
      title={EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test}, 
      author={Yuhui Li and Fangyun Wei and Chao Zhang and Hongyang Zhang},
      year={2025},
      eprint={2503.01840},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.01840}, 
}

@misc{cai2024medusasimplellminference,
      title={Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads}, 
      author={Tianle Cai and Yuhong Li and Zhengyang Geng and Hongwu Peng and Jason D. Lee and Deming Chen and Tri Dao},
      year={2024},
      eprint={2401.10774},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.10774}, 
}

@misc{leviathan2023fastinferencetransformersspeculative,
      title={Fast Inference from Transformers via Speculative Decoding}, 
      author={Yaniv Leviathan and Matan Kalman and Yossi Matias},
      year={2023},
      eprint={2211.17192},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.17192}, 
}

@misc{wu2025fastdllmv2efficientblockdiffusion,
      title={Fast-dLLM v2: Efficient Block-Diffusion LLM}, 
      author={Chengyue Wu and Hao Zhang and Shuchen Xue and Shizhe Diao and Yonggan Fu and Zhijian Liu and Pavlo Molchanov and Ping Luo and Song Han and Enze Xie},
      year={2025},
      eprint={2509.26328},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2509.26328}, 
}

@misc{cheng2025sdarsynergisticdiffusionautoregressionparadigm,
      title={SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation}, 
      author={Shuang Cheng and Yihan Bian and Dawei Liu and Linfeng Zhang and Qian Yao and Zhongbo Tian and Wenhai Wang and Qipeng Guo and Kai Chen and Biqing Qi and Bowen Zhou},
      year={2025},
      eprint={2510.06303},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2510.06303}, 
}

@misc{arriola2025blockdiffusioninterpolatingautoregressive,
      title={Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models}, 
      author={Marianne Arriola and Aaron Gokaslan and Justin T. Chiu and Zhihan Yang and Zhixuan Qi and Jiaqi Han and Subham Sekhar Sahoo and Volodymyr Kuleshov},
      year={2025},
      eprint={2503.09573},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.09573}, 
}

@misc{nie2025largelanguagediffusionmodels,
      title={Large Language Diffusion Models}, 
      author={Shen Nie and Fengqi Zhu and Zebin You and Xiaolu Zhang and Jingyang Ou and Jun Hu and Jun Zhou and Yankai Lin and Ji-Rong Wen and Chongxuan Li},
      year={2025},
      eprint={2502.09992},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.09992}, 
}

@misc{liu2025tidarthinkdiffusiontalk,
      title={TiDAR: Think in Diffusion, Talk in Autoregression}, 
      author={Jingyu Liu and Xin Dong and Zhifan Ye and Rishabh Mehta and Yonggan Fu and Vartika Singh and Jan Kautz and Ce Zhang and Pavlo Molchanov},
      year={2025},
      eprint={2511.08923},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2511.08923}, 
}

@misc{samragh2025llmknowsfutureuncovering,
      title={Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential}, 
      author={Mohammad Samragh and Arnav Kundu and David Harrison and Kumari Nishu and Devang Naik and Minsik Cho and Mehrdad Farajtabar},
      year={2025},
      eprint={2507.11851},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.11851}, 
}

@misc{li2025diffuspecunlockingdiffusionlanguage,
      title={DiffuSpec: Unlocking Diffusion Language Models for Speculative Decoding}, 
      author={Guanghao Li and Zhihui Fu and Min Fang and Qibin Zhao and Ming Tang and Chun Yuan and Jun Wang},
      year={2025},
      eprint={2510.02358},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2510.02358}, 
}

@misc{sandler2025specdiff2scalingdiffusiondrafter,
      title={SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding}, 
      author={Jameson Sandler and Jacob K. Christopher and Thomas Hartvigsen and Ferdinando Fioretto},
      year={2025},
      eprint={2511.00606},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2511.00606}, 
}

@article{Guo_2025,
   title={DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning},
   volume={645},
   ISSN={1476-4687},
   url={http://dx.doi.org/10.1038/s41586-025-09422-z},
   DOI={10.1038/s41586-025-09422-z},
   number={8081},
   journal={Nature},
   publisher={Springer Science and Business Media LLC},
   author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Zhang, Ruoyu and Ma, Shirong and Bi, Xiao and others},
   year={2025},
   month=sep, pages={633â€“638} }


@misc{comanici2025gemini25pushingfrontier,
      title={Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities}, 
      author={Gheorghe Comanici and Eric Bieber and Mike Schaekermann and Ice Pasupat and Noveen Sachdeva and Inderjit Dhillon and Marcel Blistein and Ori Ram and Dan Zhang and Evan Rosen and others},
      year={2025},
      eprint={2507.06261},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.06261}, 
}


@misc{openai2024openaio1card,
      title={OpenAI o1 System Card}, 
      author={OpenAI and Aaron Jaech and Adam Kalai and Adam Lerer and Adam Richardson and Ahmed El-Kishky and Aiden Low and Alec Helyar and Aleksander Madry and Alex Beutel and others},
      year={2024},
      eprint={2412.16720},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.16720}, 
}

@misc{yang2025qwen3technicalreport,
      title={Qwen3 Technical Report}, 
      author={An Yang and Anfeng Li and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Gao and Chengen Huang and Chenxu Lv and others},
      year={2025},
      eprint={2505.09388},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.09388}, 
}

@misc{qian2026d3llmultrafastdiffusionllm,
      title={d3LLM: Ultra-Fast Diffusion LLM using Pseudo-Trajectory Distillation}, 
      author={Yu-Yang Qian and Junda Su and Lanxiang Hu and Peiyuan Zhang and Zhijie Deng and Peng Zhao and Hao Zhang},
      year={2026},
      eprint={2601.07568},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2601.07568}, 
}

@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and others},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{hu2021loralowrankadaptationlarge,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685}, 
}

@misc{zheng2024sglangefficientexecutionstructured,
      title={SGLang: Efficient Execution of Structured Language Model Programs}, 
      author={Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Chuyue Sun and Jeff Huang and Cody Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph E. Gonzalez and Clark Barrett and Ying Sheng},
      year={2024},
      eprint={2312.07104},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2312.07104}, 
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{lightman2023let,
  title={Let's Verify Step by Step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}

@misc{AIME,
  author={MAA},
  title = {{American Invitational Mathematics Examination - AIME}},
  year = {2025},
  url = {https://maa.org/math-competitions/american-invitational-mathematics-examination-aime}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{jain2024livecodebench,
  title={Livecodebench: Holistic and contamination free evaluation of large language models for code},
  author={Jain, Naman and Han, King and Gu, Alex and Li, Wen-Ding and Yan, Fanjia and Zhang, Tianjun and Wang, Sida and Solar-Lezama, Armando and Sen, Koushik and Stoica, Ion},
  journal={arXiv preprint arXiv:2403.07974},
  year={2024}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

@software{NemotronPostTrainingDatasetV2,
      author = {Nathawani, Dhruv and Ding, Shuoyang and Lavrukhin, Vitaly and Gitman, Igor and Majumdar, Somshubra and Bakhturina, Evelina and Ginsburg, Boris and Polak Scowcroft, Jane},
      title = {{Nemotron-Post-Training-Dataset-v2}},
      version = {2.0},
      publisher = {{NVIDIA}},
      year = {2025}, month = aug,
      url = {https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v2}
}

@misc{codealpaca,
  author = {Sahil Chaudhary},
  title = {Code Alpaca: An Instruction-following LLaMA model for code generation},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/sahil280114/codealpaca}},
}

@software{AngelSlim2025,
    title={{AngelSlim}},
    author={Tencent},
    year={2025},
    month={6},
    url={https://github.com/Tencent/AngelSlim},
}

@inproceedings{wolf-etal-2020-transformers,
    title ={{Transformers: State-of-the-Art Natural Language Processing}},
    author={Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      others},
  booktitle={Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2020}
}

@misc{specforge2025,
  title={SpecForge: Train speculative decoding models effortlessly},
  author={Shenggui Li and Yikai Zhu and Chao Wang and Fan Yin and Shuai Shi and Yubo Wang and Yi Zhang and Yingyi Huang and Haoshuai Zheng and Yineng Zhang},
  year={2025},
  publisher={GitHub},
  howpublished={\url{https://github.com/sgl-project/specforge}},
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@misc{dong2024flexattentionprogrammingmodel,
      title={Flex Attention: A Programming Model for Generating Optimized Attention Kernels}, 
      author={Juechu Dong and Boyuan Feng and Driss Guessous and Yanbo Liang and Horace He},
      year={2024},
      eprint={2412.05496},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.05496}, 
}

@misc{ding2023enhancingchatlanguagemodels,
      title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations}, 
      author={Ning Ding and Yulin Chen and Bokai Xu and Yujia Qin and Zhi Zheng and Shengding Hu and Zhiyuan Liu and Maosong Sun and Bowen Zhou},
      year={2023},
      eprint={2305.14233},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.14233}, 
}