@misc{dong2024surveyincontextlearning,
 author = {Qingxiu Dong and Lei Li and Damai Dai and Ce Zheng and Jingyuan Ma and Rui Li and Heming Xia and Jingjing Xu and Zhiyong Wu and Tianyu Liu and Baobao Chang and Xu Sun and Lei Li and Zhifang Sui},
 journal = {ArXiv preprint},
 title = {A Survey on In-context Learning},
 url = {https://arxiv.org/abs/2301.00234},
 volume = {abs/2301.00234},
 year = {2023}
}

@inproceedings{wei_chain--thought_2023,
 author = {Jason Wei and
Xuezhi Wang and
Dale Schuurmans and
Maarten Bosma and
Brian Ichter and
Fei Xia and
Ed H. Chi and
Quoc V. Le and
Denny Zhou},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/Wei0SBIXCLZ22.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
 year = {2022}
}

@inproceedings{wang_self-consistency_2023,
 author = {Xuezhi Wang and
Jason Wei and
Dale Schuurmans and
Quoc V. Le and
Ed H. Chi and
Sharan Narang and
Aakanksha Chowdhery and
Denny Zhou},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/0002WSLCNCZ23.bib},
 booktitle = {The Eleventh International Conference on Learning Representations,
{ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
 publisher = {OpenReview.net},
 timestamp = {Fri, 30 Jun 2023 01:00:00 +0200},
 title = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
 url = {https://openreview.net/pdf?id=1PL1NIMMrw},
 year = {2023}
}

@misc{kang_scalable_2025,
 author = {Kang, Zhewei and Zhao, Xuandong and Song, Dawn},
 journal = {ArXiv preprint},
 title = {Scalable Best-of-N Selection for Large Language Models via Self-Certainty},
 url = {https://arxiv.org/abs/2502.18581},
 volume = {abs/2502.18581},
 year = {2025}
}

@misc{liu_can_2025,
 author = {Liu, Runze and Gao, Junqi and Zhao, Jian and Zhang, Kaiyan and Li, Xiu and Qi, Biqing and Ouyang, Wanli and Zhou, Bowen},
 journal = {ArXiv preprint},
 title = {Can 1B {LLM} Surpass 405B {LLM}? Rethinking Compute-Optimal Test-Time Scaling},
 url = {https://arxiv.org/abs/2502.06703},
 volume = {abs/2502.06703},
 year = {2025}
}

@misc{snell_scaling_2024,
 author = {Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
 journal = {ArXiv preprint},
 title = {Scaling {LLM} Test-Time Compute Optimally can be More Effective than Scaling Model Parameters},
 url = {https://arxiv.org/abs/2408.03314},
 volume = {abs/2408.03314},
 year = {2024}
}

@inproceedings{li_functional_2024,
 author = {Shanda Li and
Chong You and
Guru Guruganesh and
Joshua Ainslie and
Santiago Onta{\~{n}}{\'{o}}n and
Manzil Zaheer and
Sumit Sanghai and
Yiming Yang and
Sanjiv Kumar and
Srinadh Bhojanapalli},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/LiYGAOZSYKB24.bib},
 booktitle = {The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024},
 publisher = {OpenReview.net},
 timestamp = {Wed, 07 Aug 2024 01:00:00 +0200},
 title = {Functional Interpolation for Relative Positions improves Long Context
Transformers},
 url = {https://openreview.net/forum?id=rR03qFesqk},
 year = {2024}
}

@inproceedings{wu_longgenbench_2024,
 abstract = {Current benchmarks like ``\${\textbackslash}textit\{Needle-in-a-Haystack\}\$'' (\${\textbackslash}textit\{{NIAH}\}\$), \${\textbackslash}textit\{Ruler\}\$, and \${\textbackslash}textit\{Needlebench\}\$ focus on models' ability to understand long-context input sequences but fail to capture a critical dimension: the generation of high-quality long-form text. Applications such as design proposals, technical documentation, and creative writing rely on coherent, instruction-following outputs over extended sequences—a challenge that existing benchmarks do not adequately address. To fill this gap, we introduce \${\textbackslash}textit\{{LongGenBench}\}\$, a novel benchmark designed to rigorously evaluate large language models' ({LLMs}) ability to generate long text while adhering to complex instructions. Through tasks requiring specific events or constraints within generated text, \${\textbackslash}textit\{{LongGenBench}\}\$ evaluates model performance across four distinct scenarios, three instruction types, and two generation-lengths (16K and 32K tokens). Our evaluation of ten state-of-the-art {LLMs} reveals that, despite strong results on \${\textbackslash}textit\{Ruler\}\$, all models struggled with long text generation on \${\textbackslash}textit\{{LongGenBench}\}\$, particularly as text length increased. This suggests that current {LLMs} are not yet equipped to meet the demands of real-world, long-form text generation. We open-source \${\textbackslash}textit\{{LongGenBench}\}\$ to promote comprehensive evaluation and improvement in this critical area, with code and data available at \$\{anonymousurl\}\$.},
 author = {Wu, Yuhao and Hee, Ming Shan and Hu, Zhiqiang and Lee, Roy Ka-Wei},
 date = {2024-10-04},
 eventtitle = {The Thirteenth International Conference on Learning Representations},
 file = {Full Text PDF:/Users/brachit/Zotero/storage/2ZPM4GJ5/Wu et al. - 2024 - LongGenBench Benchmarking Long-Form Generation in Long Context LLMs.pdf:application/pdf},
 langid = {english},
 shorttitle = {{LongGenBench}},
 title = {{LongGenBench}: Benchmarking Long-Form Generation in Long Context {LLMs}},
 url = {https://openreview.net/forum?id=3A71qNKWAS},
 urldate = {2025-05-20},
 year = {2024}
}

@misc{yang_rope_2025,
 author = {Yang, Bowen and Venkitesh, Bharat and Talupuru, Dwarak and Lin, Hangyu and Cairuz, David and Blunsom, Phil and Locatelli, Acyr},
 journal = {ArXiv preprint},
 title = {Rope to Nope and Back Again: A New Hybrid Attention Strategy},
 url = {https://arxiv.org/abs/2501.18795},
 volume = {abs/2501.18795},
 year = {2025}
}

@misc{nakanishi_scalable-softmax_2025,
 author = {Nakanishi, Ken M.},
 journal = {ArXiv preprint},
 title = {Scalable-Softmax Is Superior for Attention},
 url = {https://arxiv.org/abs/2501.19399},
 volume = {abs/2501.19399},
 year = {2025}
}

@inproceedings{hardt_test-time_2023,
 author = {Moritz Hardt and
Yu Sun},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/Hardt024.bib},
 booktitle = {The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024},
 publisher = {OpenReview.net},
 timestamp = {Wed, 07 Aug 2024 01:00:00 +0200},
 title = {Test-Time Training on Nearest Neighbors for Large Language Models},
 url = {https://openreview.net/forum?id=CNL2bku4ra},
 year = {2024}
}

@inproceedings{kazemnejad_impact_2023,
 author = {Amirhossein Kazemnejad and
Inkit Padhi and
Karthikeyan Natesan Ramamurthy and
Payel Das and
Siva Reddy},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/KazemnejadPRDR23.bib},
 booktitle = {Advances in Neural Information Processing Systems 36: Annual Conference
on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023},
 editor = {Alice Oh and
Tristan Naumann and
Amir Globerson and
Kate Saenko and
Moritz Hardt and
Sergey Levine},
 timestamp = {Fri, 01 Mar 2024 00:00:00 +0100},
 title = {The Impact of Positional Encoding on Length Generalization in Transformers},
 url = {http://papers.nips.cc/paper\_files/paper/2023/hash/4e85362c02172c0c6567ce593122d31c-Abstract-Conference.html},
 year = {2023}
}

@inproceedings{press_train_2022,
 author = {Ofir Press and
Noah A. Smith and
Mike Lewis},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/PressSL22.bib},
 booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
 publisher = {OpenReview.net},
 timestamp = {Tue, 27 Dec 2022 00:00:00 +0100},
 title = {Train Short, Test Long: Attention with Linear Biases Enables Input
Length Extrapolation},
 url = {https://openreview.net/forum?id=R8sQPpGCv0},
 year = {2022}
}

@misc{su_roformer_2023,
 author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
 journal = {ArXiv preprint},
 title = {{RoFormer}: Enhanced Transformer with Rotary Position Embedding},
 url = {https://arxiv.org/abs/2104.09864},
 volume = {abs/2104.09864},
 year = {2021}
}

@misc{gupta_gmat_2020,
 author = {Gupta, Ankit and Berant, Jonathan},
 journal = {ArXiv preprint},
 title = {{GMAT}: Global Memory Augmentation for Transformers},
 url = {https://arxiv.org/abs/2006.03274},
 volume = {abs/2006.03274},
 year = {2020}
}

@inproceedings{ainslie_etc_2020,
 address = {Online},
 author = {Ainslie, Joshua  and
Ontanon, Santiago  and
Alberti, Chris  and
Cvicek, Vaclav  and
Fisher, Zachary  and
Pham, Philip  and
Ravula, Anirudh  and
Sanghai, Sumit  and
Wang, Qifan  and
Yang, Li},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 doi = {10.18653/v1/2020.emnlp-main.19},
 editor = {Webber, Bonnie  and
Cohn, Trevor  and
He, Yulan  and
Liu, Yang},
 pages = {268--284},
 publisher = {Association for Computational Linguistics},
 title = {{ETC}: Encoding Long and Structured Inputs in Transformers},
 url = {https://aclanthology.org/2020.emnlp-main.19},
 year = {2020}
}

@inproceedings{liu_scaling_2024,
 author = {Xiaoran Liu and
Hang Yan and
Chenxin An and
Xipeng Qiu and
Dahua Lin},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/Liu0AQL24.bib},
 booktitle = {The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024},
 publisher = {OpenReview.net},
 timestamp = {Wed, 07 Aug 2024 01:00:00 +0200},
 title = {Scaling Laws of RoPE-based Extrapolation},
 url = {https://openreview.net/forum?id=JO7k0SJ5V6},
 year = {2024}
}

@misc{ye_differential_2025,
 author = {Ye, Tianzhu and Dong, Li and Xia, Yuqing and Sun, Yutao and Zhu, Yi and Huang, Gao and Wei, Furu},
 journal = {ArXiv preprint},
 title = {Differential Transformer},
 url = {https://arxiv.org/abs/2410.05258},
 volume = {abs/2410.05258},
 year = {2024}
}

@misc{wang_length_2024,
 author = {Wang, Jie and Ji, Tao and Wu, Yuanbin and Yan, Hang and Gui, Tao and Zhang, Qi and Huang, Xuanjing and Wang, Xiaoling},
 journal = {ArXiv preprint},
 title = {Length Generalization of Causal Transformers without Position Encoding},
 url = {https://arxiv.org/abs/2404.12224},
 volume = {abs/2404.12224},
 year = {2024}
}

@inproceedings{anil_exploring_2022,
 author = {Cem Anil and
Yuhuai Wu and
Anders Andreassen and
Aitor Lewkowycz and
Vedant Misra and
Vinay V. Ramasesh and
Ambrose Slone and
Guy Gur{-}Ari and
Ethan Dyer and
Behnam Neyshabur},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/AnilWALMRSGDN22.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {Exploring Length Generalization in Large Language Models},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/fb7451e43f9c1c35b774bcfad7a5714b-Abstract-Conference.html},
 year = {2022}
}

@inproceedings{chowdhury_monotonic_2023,
 author = {Jishnu Ray Chowdhury and
Cornelia Caragea},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/ChowdhuryC23a.bib},
 booktitle = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
2023, Honolulu, Hawaii, {USA}},
 editor = {Andreas Krause and
Emma Brunskill and
Kyunghyun Cho and
Barbara Engelhardt and
Sivan Sabato and
Jonathan Scarlett},
 pages = {28792--28808},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Mon, 28 Aug 2023 01:00:00 +0200},
 title = {Monotonic Location Attention for Length Generalization},
 url = {https://proceedings.mlr.press/v202/ray-chowdhury23b.html},
 volume = {202},
 year = {2023}
}

@inproceedings{chi_dissecting_2023,
 address = {Toronto, Canada},
 author = {Chi, Ta-Chung  and
Fan, Ting-Han  and
Rudnicky, Alexander  and
Ramadge, Peter},
 booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2023.acl-long.756},
 editor = {Rogers, Anna  and
Boyd-Graber, Jordan  and
Okazaki, Naoaki},
 pages = {13522--13537},
 publisher = {Association for Computational Linguistics},
 title = {Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis},
 url = {https://aclanthology.org/2023.acl-long.756},
 year = {2023}
}

@inproceedings{chi_kerple_2022,
 author = {Ta{-}Chung Chi and
Ting{-}Han Fan and
Peter J. Ramadge and
Alexander Rudnicky},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/ChiFRR22.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {{KERPLE:} Kernelized Relative Positional Embedding for Length Extrapolation},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/37a413841a614b5414b333585e7613b8-Abstract-Conference.html},
 year = {2022}
}

@inproceedings{choromanski_learning_2024,
 author = {Krzysztof Choromanski and
Shanda Li and
Valerii Likhosherstov and
Kumar Avinava Dubey and
Shengjie Luo and
Di He and
Yiming Yang and
Tam{\'{a}}s Sarl{\'{o}}s and
Thomas Weingarten and
Adrian Weller},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/aistats/ChoromanskiLLDL24.bib},
 booktitle = {International Conference on Artificial Intelligence and Statistics,
2-4 May 2024, Palau de Congressos, Valencia, Spain},
 editor = {Sanjoy Dasgupta and
Stephan Mandt and
Yingzhen Li},
 pages = {2278--2286},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Mon, 13 May 2024 01:00:00 +0200},
 title = {Learning a Fourier Transform for Linear Relative Positional Encodings
in Transformers},
 url = {https://proceedings.mlr.press/v238/choromanski24a.html},
 volume = {238},
 year = {2024}
}

@inproceedings{shaw_self-attention_2018,
 address = {New Orleans, Louisiana},
 author = {Shaw, Peter  and
Uszkoreit, Jakob  and
Vaswani, Ashish},
 booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
 doi = {10.18653/v1/N18-2074},
 editor = {Walker, Marilyn  and
Ji, Heng  and
Stent, Amanda},
 pages = {464--468},
 publisher = {Association for Computational Linguistics},
 title = {Self-Attention with Relative Position Representations},
 url = {https://aclanthology.org/N18-2074},
 year = {2018}
}

@misc{chen_extending_2023,
 author = {Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
 journal = {ArXiv preprint},
 title = {Extending Context Window of Large Language Models via Positional Interpolation},
 url = {https://arxiv.org/abs/2306.15595},
 volume = {abs/2306.15595},
 year = {2023}
}

@online{noauthor_1m_nodate,
 file = {1M Context Length Pre-training with LC-RoPE Length Generalization | Llama Model Working Group | Group | Workplace:/Users/brachit/Zotero/storage/PDBA3A3G/540245308700790.html:text/html;PDF:/Users/brachit/Zotero/storage/C6FF2PMX/1M Context Length Pre-training with LC-RoPE Length Generalization  Llama Model Working Group  Grou.pdf:application/pdf},
 title = {1M Context Length Pre-training with {LC}-{RoPE} Length Generalization {\textbar} Llama Model Working Group {\textbar} Group {\textbar} Workplace},
 url = {https://fb.workplace.com/groups/283103424414981/permalink/540245308700790/},
 urldate = {2025-05-27}
}

@online{noauthor_10m_nodate,
 file = {10M+ Multimodal Context | Llama Model Working Group | Group | Workplace:/Users/brachit/Zotero/storage/FRVATMBZ/665500699508583.html:text/html},
 title = {10M+ Multimodal Context {\textbar} Llama Model Working Group {\textbar} Group {\textbar} Workplace},
 url = {https://fb.workplace.com/groups/283103424414981/permalink/665500699508583/},
 urldate = {2025-05-27}
}

@online{noauthor_llama_nodate,
 abstract = {Llama 4 Long Context and the {iRoPE} Architecture {TL};{DR} Llama 4 Flash 17B pretraining has achieved 100\% accuracy on the public benchmark Ruler's Needle-in-a-Haystack with a 1M context length. It has also attained {\textgreater} 99\% accuracy at a 1M context length and {\textgreater} 90\% accuracy at a 2M context length on the...},
 file = {Snapshot:/Users/brachit/Zotero/storage/39NG5EL9/edit.html:text/html},
 langid = {english},
 title = {Llama 4 Long Context and the {iRoPE} architecture},
 titleaddon = {Google Docs},
 url = {https://docs.google.com/document/d/1pV8Slv3dQyGcZSkQjp74H3ijM8w3x3vvoySoeMFXE50/edit?tab=t.0&usp=embed_facebook},
 urldate = {2025-05-27}
}

@online{noauthor_position_nodate,
 abstract = {Position Encoding Recommendation Enhancing {RoPE} for Improved Length Generalization See updated contents in      We recommend {LC}-{RoPE} ({PR}) for position encoding, which is a simple, easy-to-interpret, effective, and low-risk improvement of {RoPE} without new learnable parameters. Compared with {RoPE},...},
 file = {PDF:/Users/brachit/Zotero/storage/655RAECE/Position Encoding Recommendation.pdf:application/pdf;Snapshot:/Users/brachit/Zotero/storage/MIALLBYY/edit.html:text/html},
 langid = {english},
 title = {Position Encoding Recommendation},
 titleaddon = {Google Docs},
 url = {https://docs.google.com/document/d/1P1pCyaIrlteYnxPrafWQSX4HBX_Xa3WFW-_Ch0j9yzk/edit?usp=embed_facebook},
 urldate = {2025-05-27}
}

@online{noauthor_llama_nodate-1,
 file = {Llama 4 Long Context and the iRoPE Architecture | Llama Model Working Group | Group | Workplace:/Users/brachit/Zotero/storage/3MMQR5H4/608422521883068.html:text/html;PDF:/Users/brachit/Zotero/storage/6873TS7C/Llama 4 Long Context and the iRoPE Architecture  Llama Model Working Group  Group  Workplace.pdf:application/pdf},
 title = {Llama 4 Long Context and the {iRoPE} Architecture {\textbar} Llama Model Working Group {\textbar} Group {\textbar} Workplace},
 url = {https://fb.workplace.com/groups/283103424414981/permalink/608422521883068/},
 urldate = {2025-05-27}
}

@misc{wan_qwenlong-l1_2025,
 author = {Wan, Fanqi and Shen, Weizhou and Liao, Shengyi and Shi, Yingcheng and Li, Chenliang and Yang, Ziyi and Zhang, Ji and Huang, Fei and Zhou, Jingren and Yan, Ming},
 journal = {ArXiv preprint},
 title = {{QwenLong}-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning},
 url = {https://arxiv.org/abs/2505.17667},
 volume = {abs/2505.17667},
 year = {2025}
}

@misc{tan_scaling_2025,
 author = {Tan, Shawn and Yang, Songlin and Courville, Aaron and Panda, Rameswar and Shen, Yikang},
 journal = {ArXiv preprint},
 title = {Scaling Stick-Breaking Attention: An Efficient Implementation and In-depth Study},
 url = {https://arxiv.org/abs/2410.17980},
 volume = {abs/2410.17980},
 year = {2024}
}

@misc{hou_universal_2024,
 author = {Hou, Kaiying and Brandfonbrener, David and Kakade, Sham and Jelassi, Samy and Malach, Eran},
 journal = {ArXiv preprint},
 title = {Universal Length Generalization with Turing Programs},
 url = {https://arxiv.org/abs/2407.03310},
 volume = {abs/2407.03310},
 year = {2024}
}

@misc{gao_how_2025,
 author = {Gao, Tianyu and Wettig, Alexander and Yen, Howard and Chen, Danqi},
 journal = {ArXiv preprint},
 title = {How to Train Long-Context Language Models (Effectively)},
 url = {https://arxiv.org/abs/2410.02660},
 volume = {abs/2410.02660},
 year = {2024}
}

@inproceedings{peng_yarn_2023,
 author = {Bowen Peng and
Jeffrey Quesnelle and
Honglu Fan and
Enrico Shippole},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/PengQFS24.bib},
 booktitle = {The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024},
 publisher = {OpenReview.net},
 timestamp = {Wed, 07 Aug 2024 01:00:00 +0200},
 title = {YaRN: Efficient Context Window Extension of Large Language Models},
 url = {https://openreview.net/forum?id=wHBfxhZu1u},
 year = {2024}
}

@misc{shi_explaining_2025,
 author = {Shi, Jingzhe and Ma, Qinwei and Liu, Hongyi and Zhao, Hang and Hwang, Jeng-Neng and Belongie, Serge and Li, Lei},
 journal = {ArXiv preprint},
 title = {Explaining Context Length Scaling and Bounds for Language Models},
 url = {https://arxiv.org/abs/2502.01481},
 volume = {abs/2502.01481},
 year = {2025}
}

@misc{wei_larger_2023,
 author = {Wei, Jerry and Wei, Jason and Tay, Yi and Tran, Dustin and Webson, Albert and Lu, Yifeng and Chen, Xinyun and Liu, Hanxiao and Huang, Da and Zhou, Denny and Ma, Tengyu},
 journal = {ArXiv preprint},
 title = {Larger language models do in-context learning differently},
 url = {https://arxiv.org/abs/2303.03846},
 volume = {abs/2303.03846},
 year = {2023}
}

@misc{team_gemini_2024,
 author = {Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and Mariooryad, Soroosh and Ding, Yifan and Geng, Xinyang and Alcober, Fred and Frostig, Roy and Omernick, Mark and Walker, Lexi and Paduraru, Cosmin and Sorokin, Christina and Tacchetti, Andrea and Gaffney, Colin and Daruki, Samira and Sercinoglu, Olcan and Gleicher, Zach and Love, Juliette and Voigtlaender, Paul and Jain, Rohan and Surita, Gabriela and Mohamed, Kareem and Blevins, Rory and Ahn, Junwhan and Zhu, Tao and Kawintiranon, Kornraphop and Firat, Orhan and Gu, Yiming and Zhang, Yujing and Rahtz, Matthew and Faruqui, Manaal and Clay, Natalie and Gilmer, Justin and Co-Reyes, J. D. and Penchev, Ivo and Zhu, Rui and Morioka, Nobuyuki and Hui, Kevin and Haridasan, Krishna and Campos, Victor and Mahdieh, Mahdis and Guo, Mandy and Hassan, Samer and Kilgour, Kevin and Vezer, Arpi and Cheng, Heng-Tze and Liedekerke, Raoul de and Goyal, Siddharth and Barham, Paul and Strouse, D. J. and Noury, Seb and Adler, Jonas and Sundararajan, Mukund and Vikram, Sharad and Lepikhin, Dmitry and Paganini, Michela and Garcia, Xavier and Yang, Fan and Valter, Dasha and Trebacz, Maja and Vodrahalli, Kiran and Asawaroengchai, Chulayuth and Ring, Roman and Kalb, Norbert and Soares, Livio Baldini and Brahma, Siddhartha and Steiner, David and Yu, Tianhe and Mentzer, Fabian and He, Antoine and Gonzalez, Lucas and Xu, Bibo and Kaufman, Raphael Lopez and Shafey, Laurent El and Oh, Junhyuk and Hennigan, Tom and Driessche, George van den and Odoom, Seth and Lucic, Mario and Roelofs, Becca and Lall, Sid and Marathe, Amit and Chan, Betty and Ontanon, Santiago and He, Luheng and Teplyashin, Denis and Lai, Jonathan and Crone, Phil and Damoc, Bogdan and Ho, Lewis and Riedel, Sebastian and Lenc, Karel and Yeh, Chih-Kuan and Chowdhery, Aakanksha and Xu, Yang and Kazemi, Mehran and Amid, Ehsan and Petrushkina, Anastasia and Swersky, Kevin and Khodaei, Ali and Chen, Gowoon and Larkin, Chris and Pinto, Mario and Yan, Geng and Badia, Adria Puigdomenech and Patil, Piyush and Hansen, Steven and Orr, Dave and Arnold, Sebastien M. R. and Grimstad, Jordan and Dai, Andrew and Douglas, Sholto and Sinha, Rishika and Yadav, Vikas and Chen, Xi and Gribovskaya, Elena and Austin, Jacob and Zhao, Jeffrey and Patel, Kaushal and Komarek, Paul and Austin, Sophia and Borgeaud, Sebastian and Friso, Linda and Goyal, Abhimanyu and Caine, Ben and Cao, Kris and Chung, Da-Woon and Lamm, Matthew and Barth-Maron, Gabe and Kagohara, Thais and Olszewska, Kate and Chen, Mia and Shivakumar, Kaushik and Agarwal, Rishabh and Godhia, Harshal and Rajwar, Ravi and Snaider, Javier and Dotiwalla, Xerxes and Liu, Yuan and Barua, Aditya and Ungureanu, Victor and Zhang, Yuan and Batsaikhan, Bat-Orgil and Wirth, Mateo and Qin, James and Danihelka, Ivo and Doshi, Tulsee and Chadwick, Martin and Chen, Jilin and Jain, Sanil and Le, Quoc and Kar, Arjun and Gurumurthy, Madhu and Li, Cheng and Sang, Ruoxin and Liu, Fangyu and Lamprou, Lampros and Munoz, Rich and Lintz, Nathan and Mehta, Harsh and Howard, Heidi and Reynolds, Malcolm and Aroyo, Lora and Wang, Quan and Blanco, Lorenzo and Cassirer, Albin and Griffith, Jordan and Das, Dipanjan and Lee, Stephan and Sygnowski, Jakub and Fisher, Zach and Besley, James and Powell, Richard and Ahmed, Zafarali and Paulus, Dominik and Reitter, David and Borsos, Zalan and Joshi, Rishabh and Pope, Aedan and Hand, Steven and Selo, Vittorio and Jain, Vihan and Sethi, Nikhil and Goel, Megha and Makino, Takaki and May, Rhys and Yang, Zhen and Schalkwyk, Johan and Butterfield, Christina and Hauth, Anja and Goldin, Alex and Hawkins, Will and Senter, Evan and Brin, Sergey and Woodman, Oliver and Ritter, Marvin and Noland, Eric and Giang, Minh and Bolina, Vijay and Lee, Lisa and Blyth, Tim and Mackinnon, Ian and Reid, Machel and Sarvana, Obaid and Silver, David and Chen, Alexander and Wang, Lily and Maggiore, Loren and Chang, Oscar and Attaluri, Nithya and Thornton, Gregory and Chiu, Chung-Cheng and Bunyan, Oskar and Levine, Nir and Chung, Timothy and Eltyshev, Evgenii and Si, Xiance and Lillicrap, Timothy and Brady, Demetra and Aggarwal, Vaibhav and Wu, Boxi and Xu, Yuanzhong and {McIlroy}, Ross and Badola, Kartikeya and Sandhu, Paramjit and Moreira, Erica and Stokowiec, Wojciech and Hemsley, Ross and Li, Dong and Tudor, Alex and Shyam, Pranav and Rahimtoroghi, Elahe and Haykal, Salem and Sprechmann, Pablo and Zhou, Xiang and Mincu, Diana and Li, Yujia and Addanki, Ravi and Krishna, Kalpesh and Wu, Xiao and Frechette, Alexandre and Eyal, Matan and Dafoe, Allan and Lacey, Dave and Whang, Jay and Avrahami, Thi and Zhang, Ye and Taropa, Emanuel and Lin, Hanzhao and Toyama, Daniel and Rutherford, Eliza and Sano, Motoki and Choe, {HyunJeong} and Tomala, Alex and Safranek-Shrader, Chalence and Kassner, Nora and Pajarskas, Mantas and Harvey, Matt and Sechrist, Sean and Fortunato, Meire and Lyu, Christina and Elsayed, Gamaleldin and Kuang, Chenkai and Lottes, James and Chu, Eric and Jia, Chao and Chen, Chih-Wei and Humphreys, Peter and Baumli, Kate and Tao, Connie and Samuel, Rajkumar and Santos, Cicero Nogueira dos and Andreassen, Anders and Rakićević, Nemanja and Grewe, Dominik and Kumar, Aviral and Winkler, Stephanie and Caton, Jonathan and Brock, Andrew and Dalmia, Sid and Sheahan, Hannah and Barr, Iain and Miao, Yingjie and Natsev, Paul and Devlin, Jacob and Behbahani, Feryal and Prost, Flavien and Sun, Yanhua and Myaskovsky, Artiom and Pillai, Thanumalayan Sankaranarayana and Hurt, Dan and Lazaridou, Angeliki and Xiong, Xi and Zheng, Ce and Pardo, Fabio and Li, Xiaowei and Horgan, Dan and Stanton, Joe and Ambar, Moran and Xia, Fei and Lince, Alejandro and Wang, Mingqiu and Mustafa, Basil and Webson, Albert and Lee, Hyo and Anil, Rohan and Wicke, Martin and Dozat, Timothy and Sinha, Abhishek and Piqueras, Enrique and Dabir, Elahe and Upadhyay, Shyam and Boral, Anudhyan and Hendricks, Lisa Anne and Fry, Corey and Djolonga, Josip and Su, Yi and Walker, Jake and Labanowski, Jane and Huang, Ronny and Misra, Vedant and Chen, Jeremy and Skerry-Ryan, R. J. and Singh, Avi and Rijhwani, Shruti and Yu, Dian and Castro-Ros, Alex and Changpinyo, Beer and Datta, Romina and Bagri, Sumit and Hrafnkelsson, Arnar Mar and Maggioni, Marcello and Zheng, Daniel and Sulsky, Yury and Hou, Shaobo and Paine, Tom Le and Yang, Antoine and Riesa, Jason and Rogozinska, Dominika and Marcus, Dror and Badawy, Dalia El and Zhang, Qiao and Wang, Luyu and Miller, Helen and Greer, Jeremy and Sjos, Lars Lowe and Nova, Azade and Zen, Heiga and Chaabouni, Rahma and Rosca, Mihaela and Jiang, Jiepu and Chen, Charlie and Liu, Ruibo and Sainath, Tara and Krikun, Maxim and Polozov, Alex and Lespiau, Jean-Baptiste and Newlan, Josh and Cankara, Zeyncep and Kwak, Soo and Xu, Yunhan and Chen, Phil and Coenen, Andy and Meyer, Clemens and Tsihlas, Katerina and Ma, Ada and Gottweis, Juraj and Xing, Jinwei and Gu, Chenjie and Miao, Jin and Frank, Christian and Cankara, Zeynep and Ganapathy, Sanjay and Dasgupta, Ishita and Hughes-Fitt, Steph and Chen, Heng and Reid, David and Rong, Keran and Fan, Hongmin and Amersfoort, Joost van and Zhuang, Vincent and Cohen, Aaron and Gu, Shixiang Shane and Mohananey, Anhad and Ilic, Anastasija and Tobin, Taylor and Wieting, John and Bortsova, Anna and Thacker, Phoebe and Wang, Emma and Caveness, Emily and Chiu, Justin and Sezener, Eren and Kaskasoli, Alex and Baker, Steven and Millican, Katie and Elhawaty, Mohamed and Aisopos, Kostas and Lebsack, Carl and Byrd, Nathan and Dai, Hanjun and Jia, Wenhao and Wiethoff, Matthew and Davoodi, Elnaz and Weston, Albert and Yagati, Lakshman and Ahuja, Arun and Gao, Isabel and Pundak, Golan and Zhang, Susan and Azzam, Michael and Sim, Khe Chai and Caelles, Sergi and Keeling, James and Sharma, Abhanshu and Swing, Andy and Li, {YaGuang} and Liu, Chenxi and Bostock, Carrie Grimes and Bansal, Yamini and Nado, Zachary and Anand, Ankesh and Lipschultz, Josh and Karmarkar, Abhijit and Proleev, Lev and Ittycheriah, Abe and Yeganeh, Soheil Hassas and Polovets, George and Faust, Aleksandra and Sun, Jiao and Rrustemi, Alban and Li, Pen and Shivanna, Rakesh and Liu, Jeremiah and Welty, Chris and Lebron, Federico and Baddepudi, Anirudh and Krause, Sebastian and Parisotto, Emilio and Soricut, Radu and Xu, Zheng and Bloxwich, Dawn and Johnson, Melvin and Neyshabur, Behnam and Mao-Jones, Justin and Wang, Renshen and Ramasesh, Vinay and Abbas, Zaheer and Guez, Arthur and Segal, Constant and Nguyen, Duc Dung and Svensson, James and Hou, Le and York, Sarah and Milan, Kieran and Bridgers, Sophie and Gworek, Wiktor and Tagliasacchi, Marco and Lee-Thorp, James and Chang, Michael and Guseynov, Alexey and Hartman, Ale Jakse and Kwong, Michael and Zhao, Ruizhe and Kashem, Sheleem and Cole, Elizabeth and Miech, Antoine and Tanburn, Richard and Phuong, Mary and Pavetic, Filip and Cevey, Sebastien and Comanescu, Ramona and Ives, Richard and Yang, Sherry and Du, Cosmo and Li, Bo and Zhang, Zizhao and Iinuma, Mariko and Hu, Clara Huiyi and Roy, Aurko and Bijwadia, Shaan and Zhu, Zhenkai and Martins, Danilo and Saputro, Rachel and Gergely, Anita and Zheng, Steven and Jia, Dawei and Antonoglou, Ioannis and Sadovsky, Adam and Gu, Shane and Bi, Yingying and Andreev, Alek and Samangooei, Sina and Khan, Mina and Kocisky, Tomas and Filos, Angelos and Kumar, Chintu and Bishop, Colton and Yu, Adams and Hodkinson, Sarah and Mittal, Sid and Shah, Premal and Moufarek, Alexandre and Cheng, Yong and Bloniarz, Adam and Lee, Jaehoon and Pejman, Pedram and Michel, Paul and Spencer, Stephen and Feinberg, Vladimir and Xiong, Xuehan and Savinov, Nikolay and Smith, Charlotte and Shakeri, Siamak and Tran, Dustin and Chesus, Mary and Bohnet, Bernd and Tucker, George and Glehn, Tamara von and Muir, Carrie and Mao, Yiran and Kazawa, Hideto and Slone, Ambrose and Soparkar, Kedar and Shrivastava, Disha and Cobon-Kerr, James and Sharman, Michael and Pavagadhi, Jay and Araya, Carlos and Misiunas, Karolis and Ghelani, Nimesh and Laskin, Michael and Barker, David and Li, Qiujia and Briukhov, Anton and Houlsby, Neil and Glaese, Mia and Lakshminarayanan, Balaji and Schucher, Nathan and Tang, Yunhao and Collins, Eli and Lim, Hyeontaek and Feng, Fangxiaoyu and Recasens, Adria and Lai, Guangda and Magni, Alberto and Cao, Nicola De and Siddhant, Aditya and Ashwood, Zoe and Orbay, Jordi and Dehghani, Mostafa and Brennan, Jenny and He, Yifan and Xu, Kelvin and Gao, Yang and Saroufim, Carl and Molloy, James and Wu, Xinyi and Arnold, Seb and Chang, Solomon and Schrittwieser, Julian and Buchatskaya, Elena and Radpour, Soroush and Polacek, Martin and Giordano, Skye and Bapna, Ankur and Tokumine, Simon and Hellendoorn, Vincent and Sottiaux, Thibault and Cogan, Sarah and Severyn, Aliaksei and Saleh, Mohammad and Thakoor, Shantanu and Shefey, Laurent and Qiao, Siyuan and Gaba, Meenu and Chang, Shuo-yiin and Swanson, Craig and Zhang, Biao and Lee, Benjamin and Rubenstein, Paul Kishan and Song, Gan and Kwiatkowski, Tom and Koop, Anna and Kannan, Ajay and Kao, David and Schuh, Parker and Stjerngren, Axel and Ghiasi, Golnaz and Gibson, Gena and Vilnis, Luke and Yuan, Ye and Ferreira, Felipe Tiengo and Kamath, Aishwarya and Klimenko, Ted and Franko, Ken and Xiao, Kefan and Bhattacharya, Indro and Patel, Miteyan and Wang, Rui and Morris, Alex and Strudel, Robin and Sharma, Vivek and Choy, Peter and Hashemi, Sayed Hadi and Landon, Jessica and Finkelstein, Mara and Jhakra, Priya and Frye, Justin and Barnes, Megan and Mauger, Matthew and Daun, Dennis and Baatarsukh, Khuslen and Tung, Matthew and Farhan, Wael and Michalewski, Henryk and Viola, Fabio and Quitry, Felix de Chaumont and Lan, Charline Le and Hudson, Tom and Wang, Qingze and Fischer, Felix and Zheng, Ivy and White, Elspeth and Dragan, Anca and Alayrac, Jean-baptiste and Ni, Eric and Pritzel, Alexander and Iwanicki, Adam and Isard, Michael and Bulanova, Anna and Zilka, Lukas and Dyer, Ethan and Sachan, Devendra and Srinivasan, Srivatsan and Muckenhirn, Hannah and Cai, Honglong and Mandhane, Amol and Tariq, Mukarram and Rae, Jack W. and Wang, Gary and Ayoub, Kareem and {FitzGerald}, Nicholas and Zhao, Yao and Han, Woohyun and Alberti, Chris and Garrette, Dan and Krishnakumar, Kashyap and Gimenez, Mai and Levskaya, Anselm and Sohn, Daniel and Matak, Josip and Iturrate, Inaki and Chang, Michael B. and Xiang, Jackie and Cao, Yuan and Ranka, Nishant and Brown, Geoff and Hutter, Adrian and Mirrokni, Vahab and Chen, Nanxin and Yao, Kaisheng and Egyed, Zoltan and Galilee, Francois and Liechty, Tyler and Kallakuri, Praveen and Palmer, Evan and Ghemawat, Sanjay and Liu, Jasmine and Tao, David and Thornton, Chloe and Green, Tim and Jasarevic, Mimi and Lin, Sharon and Cotruta, Victor and Tan, Yi-Xuan and Fiedel, Noah and Yu, Hongkun and Chi, Ed and Neitz, Alexander and Heitkaemper, Jens and Sinha, Anu and Zhou, Denny and Sun, Yi and Kaed, Charbel and Hulse, Brice and Mishra, Swaroop and Georgaki, Maria and Kudugunta, Sneha and Farabet, Clement and Shafran, Izhak and Vlasic, Daniel and Tsitsulin, Anton and Ananthanarayanan, Rajagopal and Carin, Alen and Su, Guolong and Sun, Pei and V, Shashank and Carvajal, Gabriel and Broder, Josef and Comsa, Iulia and Repina, Alena and Wong, William and Chen, Warren Weilun and Hawkins, Peter and Filonov, Egor and Loher, Lucia and Hirnschall, Christoph and Wang, Weiyi and Ye, Jingchen and Burns, Andrea and Cate, Hardie and Wright, Diana Gage and Piccinini, Federico and Zhang, Lei and Lin, Chu-Cheng and Gog, Ionel and Kulizhskaya, Yana and Sreevatsa, Ashwin and Song, Shuang and Cobo, Luis C. and Iyer, Anand and Tekur, Chetan and Garrido, Guillermo and Xiao, Zhuyun and Kemp, Rupert and Zheng, Huaixiu Steven and Li, Hui and Agarwal, Ananth and Ngani, Christel and Goshvadi, Kati and Santamaria-Fernandez, Rebeca and Fica, Wojciech and Chen, Xinyun and Gorgolewski, Chris and Sun, Sean and Garg, Roopal and Ye, Xinyu and Eslami, S. M. Ali and Hua, Nan and Simon, Jon and Joshi, Pratik and Kim, Yelin and Tenney, Ian and Potluri, Sahitya and Thiet, Lam Nguyen and Yuan, Quan and Luisier, Florian and Chronopoulou, Alexandra and Scellato, Salvatore and Srinivasan, Praveen and Chen, Minmin and Koverkathu, Vinod and Dalibard, Valentin and Xu, Yaming and Saeta, Brennan and Anderson, Keith and Sellam, Thibault and Fernando, Nick and Huot, Fantine and Jung, Junehyuk and Varadarajan, Mani and Quinn, Michael and Raul, Amit and Le, Maigo and Habalov, Ruslan and Clark, Jon and Jalan, Komal and Bullard, Kalesha and Singhal, Achintya and Luong, Thang and Wang, Boyu and Rajayogam, Sujeevan and Eisenschlos, Julian and Jia, Johnson and Finchelstein, Daniel and Yakubovich, Alex and Balle, Daniel and Fink, Michael and Agarwal, Sameer and Li, Jing and Dvijotham, Dj and Pal, Shalini and Kang, Kai and Konzelmann, Jaclyn and Beattie, Jennifer and Dousse, Olivier and Wu, Diane and Crocker, Remi and Elkind, Chen and Jonnalagadda, Siddhartha Reddy and Lee, Jong and Holtmann-Rice, Dan and Kallarackal, Krystal and Liu, Rosanne and Vnukov, Denis and Vats, Neera and Invernizzi, Luca and Jafari, Mohsen and Zhou, Huanjie and Taylor, Lilly and Prendki, Jennifer and Wu, Marcus and Eccles, Tom and Liu, Tianqi and Kopparapu, Kavya and Beaufays, Francoise and Angermueller, Christof and Marzoca, Andreea and Sarcar, Shourya and Dib, Hilal and Stanway, Jeff and Perbet, Frank and Trdin, Nejc and Sterneck, Rachel and Khorlin, Andrey and Li, Dinghua and Wu, Xihui and Goenka, Sonam and Madras, David and Goldshtein, Sasha and Gierke, Willi and Zhou, Tong and Liu, Yaxin and Liang, Yannie and White, Anais and Li, Yunjie and Singh, Shreya and Bahargam, Sanaz and Epstein, Mark and Basu, Sujoy and Lao, Li and Ozturel, Adnan and Crous, Carl and Zhai, Alex and Lu, Han and Tung, Zora and Gaur, Neeraj and Walton, Alanna and Dixon, Lucas and Zhang, Ming and Globerson, Amir and Uy, Grant and Bolt, Andrew and Wiles, Olivia and Nasr, Milad and Shumailov, Ilia and Selvi, Marco and Piccinno, Francesco and Aguilar, Ricardo and {McCarthy}, Sara and Khalman, Misha and Shukla, Mrinal and Galic, Vlado and Carpenter, John and Villela, Kevin and Zhang, Haibin and Richardson, Harry and Martens, James and Bosnjak, Matko and Belle, Shreyas Rammohan and Seibert, Jeff and Alnahlawi, Mahmoud and {McWilliams}, Brian and Singh, Sankalp and Louis, Annie and Ding, Wen and Popovici, Dan and Simicich, Lenin and Knight, Laura and Mehta, Pulkit and Gupta, Nishesh and Shi, Chongyang and Fatehi, Saaber and Mitrovic, Jovana and Grills, Alex and Pagadora, Joseph and Munkhdalai, Tsendsuren and Petrova, Dessie and Eisenbud, Danielle and Zhang, Zhishuai and Yates, Damion and Mittal, Bhavishya and Tripuraneni, Nilesh and Assael, Yannis and Brovelli, Thomas and Jain, Prateek and Velimirovic, Mihajlo and Akbulut, Canfer and Mu, Jiaqi and Macherey, Wolfgang and Kumar, Ravin and Xu, Jun and Qureshi, Haroon and Comanici, Gheorghe and Wiesner, Jeremy and Gong, Zhitao and Ruddock, Anton and Bauer, Matthias and Felt, Nick and {GP}, Anirudh and Arnab, Anurag and Zelle, Dustin and Rothfuss, Jonas and Rosgen, Bill and Shenoy, Ashish and Seybold, Bryan and Li, Xinjian and Mudigonda, Jayaram and Erdogan, Goker and Xia, Jiawei and Simsa, Jiri and Michi, Andrea and Yao, Yi and Yew, Christopher and Kan, Steven and Caswell, Isaac and Radebaugh, Carey and Elisseeff, Andre and Valenzuela, Pedro and {McKinney}, Kay and Paterson, Kim and Cui, Albert and Latorre-Chimoto, Eri and Kim, Solomon and Zeng, William and Durden, Ken and Ponnapalli, Priya and Sosea, Tiberiu and Choquette-Choo, Christopher A. and Manyika, James and Robenek, Brona and Vashisht, Harsha and Pereira, Sebastien and Lam, Hoi and Velic, Marko and Owusu-Afriyie, Denese and Lee, Katherine and Bolukbasi, Tolga and Parrish, Alicia and Lu, Shawn and Park, Jane and Venkatraman, Balaji and Talbert, Alice and Rosique, Lambert and Cheng, Yuchung and Sozanschi, Andrei and Paszke, Adam and Kumar, Praveen and Austin, Jessica and Li, Lu and Salama, Khalid and Perz, Bartek and Kim, Wooyeol and Dukkipati, Nandita and Baryshnikov, Anthony and Kaplanis, Christos and Sheng, {XiangHai} and Chervonyi, Yuri and Unlu, Caglar and Casas, Diego de Las and Askham, Harry and Tunyasuvunakool, Kathryn and Gimeno, Felix and Poder, Siim and Kwak, Chester and Miecnikowski, Matt and Mirrokni, Vahab and Dimitriev, Alek and Parisi, Aaron and Liu, Dangyi and Tsai, Tomy and Shevlane, Toby and Kouridi, Christina and Garmon, Drew and Goedeckemeyer, Adrian and Brown, Adam R. and Vijayakumar, Anitha and Elqursh, Ali and Jazayeri, Sadegh and Huang, Jin and Carthy, Sara Mc and Hoover, Jay and Kim, Lucy and Kumar, Sandeep and Chen, Wei and Biles, Courtney and Bingham, Garrett and Rosen, Evan and Wang, Lisa and Tan, Qijun and Engel, David and Pongetti, Francesco and Cesare, Dario de and Hwang, Dongseong and Yu, Lily and Pullman, Jennifer and Narayanan, Srini and Levin, Kyle and Gopal, Siddharth and Li, Megan and Aharoni, Asaf and Trinh, Trieu and Lo, Jessica and Casagrande, Norman and Vij, Roopali and Matthey, Loic and Ramadhana, Bramandia and Matthews, Austin and Carey, C. J. and Johnson, Matthew and Goranova, Kremena and Shah, Rohin and Ashraf, Shereen and Dasgupta, Kingshuk and Larsen, Rasmus and Wang, Yicheng and Vuyyuru, Manish Reddy and Jiang, Chong and Ijazi, Joana and Osawa, Kazuki and Smith, Celine and Boppana, Ramya Sree and Bilal, Taylan and Koizumi, Yuma and Xu, Ying and Altun, Yasemin and Shabat, Nir and Bariach, Ben and Korchemniy, Alex and Choo, Kiam and Ronneberger, Olaf and Iwuanyanwu, Chimezie and Zhao, Shubin and Soergel, David and Hsieh, Cho-Jui and Cai, Irene and Iqbal, Shariq and Sundermeyer, Martin and Chen, Zhe and Bursztein, Elie and Malaviya, Chaitanya and Biadsy, Fadi and Shroff, Prakash and Dhillon, Inderjit and Latkar, Tejasi and Dyer, Chris and Forbes, Hannah and Nicosia, Massimo and Nikolaev, Vitaly and Greene, Somer and Georgiev, Marin and Wang, Pidong and Martin, Nina and Sedghi, Hanie and Zhang, John and Banzal, Praseem and Fritz, Doug and Rao, Vikram and Wang, Xuezhi and Zhang, Jiageng and Patraucean, Viorica and Du, Dayou and Mordatch, Igor and Jurin, Ivan and Liu, Lewis and Dubey, Ayush and Mohan, Abhi and Nowakowski, Janek and Ion, Vlad-Doru and Wei, Nan and Tojo, Reiko and Raad, Maria Abi and Hudson, Drew A. and Keshava, Vaishakh and Agrawal, Shubham and Ramirez, Kevin and Wu, Zhichun and Nguyen, Hoang and Liu, Ji and Sewak, Madhavi and Petrini, Bryce and Choi, {DongHyun} and Philips, Ivan and Wang, Ziyue and Bica, Ioana and Garg, Ankush and Wilkiewicz, Jarek and Agrawal, Priyanka and Li, Xiaowei and Guo, Danhao and Xue, Emily and Shaik, Naseer and Leach, Andrew and Khan, Sadh {MNM} and Wiesinger, Julia and Jerome, Sammy and Chakladar, Abhishek and Wang, Alek Wenjiao and Ornduff, Tina and Abu, Folake and Ghaffarkhah, Alireza and Wainwright, Marcus and Cortes, Mario and Liu, Frederick and Maynez, Joshua and Terzis, Andreas and Samangouei, Pouya and Mansour, Riham and Kępa, Tomasz and Aubet, François-Xavier and Algymr, Anton and Banica, Dan and Weisz, Agoston and Orban, Andras and Senges, Alexandre and Andrejczuk, Ewa and Geller, Mark and Santo, Niccolo Dal and Anklin, Valentin and Merey, Majd Al and Baeuml, Martin and Strohman, Trevor and Bai, Junwen and Petrov, Slav and Wu, Yonghui and Hassabis, Demis and Kavukcuoglu, Koray and Dean, Jeff and Vinyals, Oriol},
 journal = {ArXiv preprint},
 title = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
 url = {https://arxiv.org/abs/2403.05530},
 volume = {abs/2403.05530},
 year = {2024}
}

@misc{muennighoff_s1_2025,
 author = {Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Candès, Emmanuel and Hashimoto, Tatsunori},
 journal = {ArXiv preprint},
 title = {s1: Simple test-time scaling},
 url = {https://arxiv.org/abs/2501.19393},
 volume = {abs/2501.19393},
 year = {2025}
}

@misc{wang_thoughts_2025,
 author = {Wang, Yue and Liu, Qiuzhi and Xu, Jiahao and Liang, Tian and Chen, Xingyu and He, Zhiwei and Song, Linfeng and Yu, Dian and Li, Juntao and Zhang, Zhuosheng and Wang, Rui and Tu, Zhaopeng and Mi, Haitao and Yu, Dong},
 journal = {ArXiv preprint},
 title = {Thoughts Are All Over the Place: On the Underthinking of o1-Like {LLMs}},
 url = {https://arxiv.org/abs/2501.18585},
 volume = {abs/2501.18585},
 year = {2025}
}

@misc{chen_not_2025,
 author = {Chen, Xingyu and Xu, Jiahao and Liang, Tian and He, Zhiwei and Pang, Jianhui and Yu, Dian and Song, Linfeng and Liu, Qiuzhi and Zhou, Mengfei and Zhang, Zhuosheng and Wang, Rui and Tu, Zhaopeng and Mi, Haitao and Yu, Dong},
 journal = {ArXiv preprint},
 title = {Do {NOT} Think That Much for 2+3=? On the Overthinking of o1-Like {LLMs}},
 url = {https://arxiv.org/abs/2412.21187},
 volume = {abs/2412.21187},
 year = {2024}
}

@inproceedings{sun_test-time_2020,
 author = {Yu Sun and
Xiaolong Wang and
Zhuang Liu and
John Miller and
Alexei A. Efros and
Moritz Hardt},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/SunWLMEH20.bib},
 booktitle = {Proceedings of the 37th International Conference on Machine Learning,
{ICML} 2020, 13-18 July 2020, Virtual Event},
 pages = {9229--9248},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Tue, 15 Dec 2020 00:00:00 +0100},
 title = {Test-Time Training with Self-Supervision for Generalization under
Distribution Shifts},
 url = {http://proceedings.mlr.press/v119/sun20b.html},
 volume = {119},
 year = {2020}
}

@inproceedings{gandelsman_test-time_2022,
 author = {Yossi Gandelsman and
Yu Sun and
Xinlei Chen and
Alexei A. Efros},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/GandelsmanSCE22.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {Test-Time Training with Masked Autoencoders},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/bcdec1c2d60f94a93b6e36f937aa0530-Abstract-Conference.html},
 year = {2022}
}

@misc{sun_learning_2025,
 author = {Sun, Yu and Li, Xinhao and Dalal, Karan and Xu, Jiarui and Vikram, Arjun and Zhang, Genghan and Dubois, Yann and Chen, Xinlei and Wang, Xiaolong and Koyejo, Sanmi and Hashimoto, Tatsunori and Guestrin, Carlos},
 journal = {ArXiv preprint},
 title = {Learning to (Learn at Test Time): {RNNs} with Expressive Hidden States},
 url = {https://arxiv.org/abs/2407.04620},
 volume = {abs/2407.04620},
 year = {2024}
}

@misc{akyurek_surprising_2025,
 author = {Akyürek, Ekin and Damani, Mehul and Zweiger, Adam and Qiu, Linlu and Guo, Han and Pari, Jyothish and Kim, Yoon and Andreas, Jacob},
 journal = {ArXiv preprint},
 title = {The Surprising Effectiveness of Test-Time Training for Few-Shot Learning},
 url = {https://arxiv.org/abs/2411.07279},
 volume = {abs/2411.07279},
 year = {2024}
}

@misc{dalal_one-minute_2025,
 author = {Dalal, Karan and Koceja, Daniel and Hussein, Gashon and Xu, Jiarui and Zhao, Yue and Song, Youjin and Han, Shihao and Cheung, Ka Chun and Kautz, Jan and Guestrin, Carlos and Hashimoto, Tatsunori and Koyejo, Sanmi and Choi, Yejin and Sun, Yu and Wang, Xiaolong},
 journal = {ArXiv preprint},
 title = {One-Minute Video Generation with Test-Time Training},
 url = {https://arxiv.org/abs/2504.05298},
 volume = {abs/2504.05298},
 year = {2025}
}

@misc{du_context_2024,
 author = {Du, Kevin and Snæbjarnarson, Vésteinn and Stoehr, Niklas and White, Jennifer C. and Schein, Aaron and Cotterell, Ryan},
 journal = {ArXiv preprint},
 title = {Context versus Prior Knowledge in Language Models},
 url = {https://arxiv.org/abs/2404.04633},
 volume = {abs/2404.04633},
 year = {2024}
}

@inproceedings{zheng_minif2f_2022,
 author = {Kunhao Zheng and
Jesse Michael Han and
Stanislas Polu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/ZhengHP22.bib},
 booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
 publisher = {OpenReview.net},
 timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
 title = {miniF2F: a cross-system benchmark for formal Olympiad-level mathematics},
 url = {https://openreview.net/forum?id=9ZPegFuFTFv},
 year = {2022}
}

@misc{wang_reinforcement_2025,
 author = {Wang, Yiping and Yang, Qing and Zeng, Zhiyuan and Ren, Liliang and Liu, Liyuan and Peng, Baolin and Cheng, Hao and He, Xuehai and Wang, Kuan and Gao, Jianfeng and Chen, Weizhu and Wang, Shuohang and Du, Simon Shaolei and Shen, Yelong},
 journal = {ArXiv preprint},
 title = {Reinforcement Learning for Reasoning in Large Language Models with One Training Example},
 url = {https://arxiv.org/abs/2504.20571},
 volume = {abs/2504.20571},
 year = {2025}
}

@misc{liu_prorl_2025,
 author = {Liu, Mingjie and Diao, Shizhe and Lu, Ximing and Hu, Jian and Dong, Xin and Choi, Yejin and Kautz, Jan and Dong, Yi},
 journal = {ArXiv preprint},
 title = {{ProRL}: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models},
 url = {https://arxiv.org/abs/2505.24864},
 volume = {abs/2505.24864},
 year = {2025}
}

@misc{he_quantifying_2019,
 author = {He, Tianxing and Zhang, Jingzhao and Zhou, Zhiming and Glass, James},
 journal = {ArXiv preprint},
 title = {Quantifying Exposure Bias for Neural Language Generation},
 url = {https://arxiv.org/abs/1905.10617},
 volume = {abs/1905.10617},
 year = {2019}
}

@misc{tan_connecting_2019,
 author = {Tan, Bowen and Hu, Zhiting and Yang, Zichao and Salakhutdinov, Ruslan and Xing, Eric},
 journal = {ArXiv preprint},
 title = {Connecting the Dots Between {MLE} and {RL} for Sequence Prediction},
 url = {https://arxiv.org/abs/1811.09740},
 volume = {abs/1811.09740},
 year = {2018}
}

@misc{aminian_theoretical_2025,
 author = {Aminian, Gholamali and Asadi, Amir R. and Shenfeld, Idan and Mroueh, Youssef},
 journal = {ArXiv preprint},
 title = {Theoretical Analysis of {KL}-regularized {RLHF} with Multiple Reference Models},
 url = {https://arxiv.org/abs/2502.01203},
 volume = {abs/2502.01203},
 year = {2025}
}

@article{zhao_sharp_2024,
 abstract = {*Reverse-Kullback-Leibler* regularization has emerged to be a predominant technique used to enhance policy optimization in reinforcement learning ({RL}) and reinforcement learning from human feedback ({RLHF}), which forces the learned policy to stay close to a reference policy. While the effectiveness and necessity of {KL}-regularization has been empirically demonstrated in various practical scenarios, current theoretical analysis of {KL}-regularized {RLHF} still obtain the same \${\textbackslash}mathcal\{O\}(1 / {\textbackslash}epsilon{\textasciicircum}2)\$ sample complexity as problems without {KL}-regularization. To understand the fundamental distinction between policy learning objectives with {KL}-regularization and ones without {KL}-regularization, we are the first to theoretically demonstrate the power of {KL}-regularization by providing a sharp analysis for {KL}-regularized contextual bandits and {RLHF}, revealing an \${\textbackslash}mathcal\{O\}(1 / {\textbackslash}epsilon)\$ sample complexity when \${\textbackslash}epsilon\$ is sufficiently small. We further explore the role of data coverage in contextual bandits and {RLHF}. While the coverage assumption is commonly employed in offline {RLHF} to link the samples from the reference policy to the optimal policy, often at the cost of a multiplicative dependence on the coverage coefficient, its impact on the sample complexity of online {RLHF} remains unclear. Previous theoretical analyses of online {RLHF} typically require explicit exploration and additional structural assumptions on the reward function class. In contrast, we show that with sufficient coverage from the reference policy, a simple two-stage mixed sampling strategy can achieve a sample complexity with only an additive dependence on the coverage coefficient. Our results provide a comprehensive understanding of the roles of {KL}-regularization and data coverage in {RLHF}, shedding light on the design of more efficient {RLHF} algorithms.},
 author = {Zhao, Heyang and Ye, Chenlu and Gu, Quanquan and Zhang, Tong},
 date = {2024-10-04},
 file = {Full Text PDF:/Users/brachit/Zotero/storage/M4D2I2N2/Zhao et al. - 2024 - Sharp Analysis for KL-Regularized Contextual Bandits and RLHF.pdf:application/pdf},
 langid = {english},
 title = {Sharp Analysis for {KL}-Regularized Contextual Bandits and {RLHF}},
 url = {https://openreview.net/forum?id=5ZkuWAbxzT&utm_source=chatgpt.com},
 urldate = {2025-06-04},
 year = {2024}
}

@article{zhan_policy_2023,
 abstract = {Policy optimization, which ﬁnds the desired policy by maximizing value functions via optimization techniques, lies at the heart of reinforcement learning ({RL}). In addition to value maximization, other practical considerations arise as well, including the need of encouraging exploration, and that of ensuring certain structural properties of the learned policy due to safety, resource and operational constraints. These can often be accounted for via regularized {RL}, which augments the target value function with a structure-promoting regularizer.},
 author = {Zhan, Wenhao and Cen, Shicong and Huang, Baihe and Chen, Yuxin and Lee, Jason D. and Chi, Yuejie},
 date = {2023-06-30},
 doi = {10.1137/21M1456789},
 file = {PDF:/Users/brachit/Zotero/storage/86LPMFYN/Zhan et al. - 2023 - Policy Mirror Descent for Regularized Reinforcement Learning A Generalized Framework with Linear Co.pdf:application/pdf},
 issn = {1052-6234, 1095-7189},
 journaltitle = {{SIAM} Journal on Optimization},
 langid = {english},
 number = {2},
 pages = {1061--1091},
 shortjournal = {{SIAM} J. Optim.},
 shorttitle = {Policy Mirror Descent for Regularized Reinforcement Learning},
 title = {Policy Mirror Descent for Regularized Reinforcement Learning: A Generalized Framework with Linear Convergence},
 url = {https://epubs.siam.org/doi/10.1137/21M1456789},
 urldate = {2025-06-04},
 volume = {33},
 year = {2023}
}

@misc{dong_reinforcement_2025,
 author = {Dong, Qingxiu and Dong, Li and Tang, Yao and Ye, Tianzhu and Sun, Yutao and Sui, Zhifang and Wei, Furu},
 journal = {ArXiv preprint},
 title = {Reinforcement Pre-Training},
 url = {https://arxiv.org/abs/2506.08007},
 volume = {abs/2506.08007},
 year = {2025}
}

@misc{zweiger_self-adapting_2025,
 author = {Zweiger, Adam and Pari, Jyothish and Guo, Han and Akyürek, Ekin and Kim, Yoon and Agrawal, Pulkit},
 journal = {ArXiv preprint},
 title = {Self-Adapting Language Models},
 url = {https://arxiv.org/abs/2506.10943},
 volume = {abs/2506.10943},
 year = {2025}
}

@inproceedings{akyurek_what_2022,
 author = {Ekin Aky{\"{u}}rek and
Dale Schuurmans and
Jacob Andreas and
Tengyu Ma and
Denny Zhou},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/AkyurekSA0Z23.bib},
 booktitle = {The Eleventh International Conference on Learning Representations,
{ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
 publisher = {OpenReview.net},
 timestamp = {Fri, 30 Jun 2023 01:00:00 +0200},
 title = {What learning algorithm is in-context learning? Investigations with
linear models},
 url = {https://openreview.net/pdf?id=0g0X4H8yN4I},
 year = {2023}
}

@inproceedings{liu_few-shot_2022,
 author = {Haokun Liu and
Derek Tam and
Mohammed Muqeeth and
Jay Mohta and
Tenghao Huang and
Mohit Bansal and
Colin Raffel},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/LiuTMMHBR22.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than
In-Context Learning},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/0cde695b83bd186c1fd456302888454c-Abstract-Conference.html},
 year = {2022}
}

@misc{li_learning_2025,
 author = {Li, Yanyang and Lyu, Michael and Wang, Liwei},
 journal = {ArXiv preprint},
 title = {Learning to Reason from Feedback at Test-Time},
 url = {https://arxiv.org/abs/2502.15771},
 volume = {abs/2502.15771},
 year = {2025}
}

@inproceedings{liu_ttt_2021,
 author = {Yuejiang Liu and
Parth Kothari and
Bastien van Delft and
Baptiste Bellot{-}Gurlet and
Taylor Mordan and
Alexandre Alahi},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/LiuKDBMA21.bib},
 booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems 2021, NeurIPS 2021, December
6-14, 2021, virtual},
 editor = {Marc'Aurelio Ranzato and
Alina Beygelzimer and
Yann N. Dauphin and
Percy Liang and
Jennifer Wortman Vaughan},
 pages = {21808--21820},
 timestamp = {Tue, 03 May 2022 01:00:00 +0200},
 title = {{TTT++:} When Does Self-Supervised Test-Time Training Fail or Thrive?},
 url = {https://proceedings.neurips.cc/paper/2021/hash/b618c3210e934362ac261db280128c22-Abstract.html},
 year = {2021}
}

@inproceedings{bartler_mt3_2022,
 author = {Alexander Bartler and
Andre B{\"{u}}hler and
Felix Wiewel and
Mario D{\"{o}}bler and
Bin Yang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/aistats/BartlerBWD022.bib},
 booktitle = {International Conference on Artificial Intelligence and Statistics,
{AISTATS} 2022, 28-30 March 2022, Virtual Event},
 editor = {Gustau Camps{-}Valls and
Francisco J. R. Ruiz and
Isabel Valera},
 pages = {3080--3090},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Fri, 20 May 2022 01:00:00 +0200},
 title = {{MT3:} Meta Test-Time Training for Self-Supervised Test-Time Adaption},
 url = {https://proceedings.mlr.press/v151/bartler22a.html},
 volume = {151},
 year = {2022}
}

@inproceedings{zhao_pitfalls_2023,
 author = {Hao Zhao and
Yuejiang Liu and
Alexandre Alahi and
Tao Lin},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/ZhaoLAL23.bib},
 booktitle = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
2023, Honolulu, Hawaii, {USA}},
 editor = {Andreas Krause and
Emma Brunskill and
Kyunghyun Cho and
Barbara Engelhardt and
Sivan Sabato and
Jonathan Scarlett},
 pages = {42058--42080},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Mon, 28 Aug 2023 01:00:00 +0200},
 title = {On Pitfalls of Test-Time Adaptation},
 url = {https://proceedings.mlr.press/v202/zhao23d.html},
 volume = {202},
 year = {2023}
}

@inproceedings{wang_greater_2024,
 abstract = {Long text generation, such as novel writing and discourse-level translation with extremely long contexts, presents significant challenges to current language models. Existing methods mainly focus on extending the model's context window through strategies like length extrapolation. However, these approaches demand substantial hardware resources during the training and/or inference phases. Our proposed method, Temp-Lora, introduces an alternative concept. Instead of relying on the {KV} cache to store all context information, we embeds this information directly into a temporary Lora module. In the process of long text generation, this module is progressively trained with text generated previously. This approach not only efficiently preserves contextual knowledge but also prevents any permanent alteration to the model's parameters given that the module is discarded post-generation. Extensive experiments on the {PG}19 language modeling benchmark and the {GuoFeng} discourse-level translation benchmark validate the effectiveness of Temp-Lora. Our results show that: 1) Temp-Lora substantially enhances generation quality for long text, as indicated by a 13.2{\textbackslash}\% decrease in perplexity ({PPL}) on a subset of {PG}19, and a 29.3{\textbackslash}\% decrease in {PPL} along with a 113.2{\textbackslash}\% increase in {BLEU} score on a subset of {GuoFeng}, 2) Temp-Lora is compatible with and enhances most existing long text generation methods, and 3) Temp-Lora can greatly reduce computational costs by shortening the context window. For example, we can ensure a moderate improvement in generation quality (a decrease of 3.8{\textbackslash}\% in {PPL}) while enabling a 51.5{\textbackslash}\% memory usage reduction and a 60.0{\textbackslash}\% decrease in latency for inference.},
 author = {Wang, Yan and Ma, Dongyang and Cai, Deng},
 date = {2024-08-26},
 eventtitle = {First Conference on Language Modeling},
 file = {Full Text PDF:/Users/brachit/Zotero/storage/CDX64HRR/Wang et al. - 2024 - With Greater Text Comes Greater Necessity Inference-Time Training Helps Long Text Generation.pdf:application/pdf},
 langid = {english},
 shorttitle = {With Greater Text Comes Greater Necessity},
 title = {With Greater Text Comes Greater Necessity: Inference-Time Training Helps Long Text Generation},
 url = {https://openreview.net/forum?id=dj9x6JuiD5},
 urldate = {2025-07-01},
 year = {2024}
}

@inproceedings{hubotter_efficiently_2024,
 abstract = {Recent efforts in fine-tuning language models often rely on automatic data selection, commonly using Nearest Neighbors retrieval from large datasets. However, we theoretically show that this approach tends to select redundant data, limiting its effectiveness or even hurting performance. To address this, we introduce {SIFT}, a data selection algorithm designed to reduce uncertainty about the model's response given a prompt, which unifies ideas from retrieval and active learning. Whereas Nearest Neighbor retrieval typically fails in the presence of information duplication, {SIFT} accounts for information duplication and optimizes the overall information gain of the selected examples. We focus our evaluations on fine-tuning at test-time for prompt-specific language modeling on the Pile dataset, and show that {SIFT} consistently outperforms Nearest Neighbor retrieval, with minimal computational overhead. Moreover, we show that our uncertainty estimates can predict the performance gain of test-time fine-tuning, and use this to develop an adaptive algorithm that invests test-time compute proportional to realized performance gains. We provide the `activeft` (Active Fine-Tuning) library which can be used as a drop-in replacement for Nearest Neighbor retrieval.},
 author = {Hübotter, Jonas and Bongni, Sascha and Hakimi, Ido and Krause, Andreas},
 date = {2024-10-04},
 eventtitle = {The Thirteenth International Conference on Learning Representations},
 file = {Full Text PDF:/Users/brachit/Zotero/storage/ELCX4JZ8/Hübotter et al. - 2024 - Efficiently Learning at Test-Time Active Fine-Tuning of LLMs.pdf:application/pdf},
 langid = {english},
 shorttitle = {Efficiently Learning at Test-Time},
 title = {Efficiently Learning at Test-Time: Active Fine-Tuning of {LLMs}},
 url = {https://openreview.net/forum?id=NS1G1Uhny3},
 urldate = {2025-07-01},
 year = {2024}
}

@inproceedings{niu_test-time_2024,
 author = {Shuaicheng Niu and
Chunyan Miao and
Guohao Chen and
Pengcheng Wu and
Peilin Zhao},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/NiuMCWZ24.bib},
 booktitle = {Forty-first International Conference on Machine Learning, {ICML} 2024,
Vienna, Austria, July 21-27, 2024},
 publisher = {OpenReview.net},
 timestamp = {Mon, 02 Sep 2024 01:00:00 +0200},
 title = {Test-Time Model Adaptation with Only Forward Passes},
 url = {https://openreview.net/forum?id=qz1Vx1v9iK},
 year = {2024}
}

@inproceedings{hardt_test-time_2024,
 author = {Moritz Hardt and
Yu Sun},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/Hardt024.bib},
 booktitle = {The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024},
 publisher = {OpenReview.net},
 timestamp = {Wed, 07 Aug 2024 01:00:00 +0200},
 title = {Test-Time Training on Nearest Neighbors for Large Language Models},
 url = {https://openreview.net/forum?id=CNL2bku4ra},
 year = {2024}
}

@inproceedings{zheng_minif2f_2022-1,
 author = {Kunhao Zheng and
Jesse Michael Han and
Stanislas Polu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/ZhengHP22.bib},
 booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
 publisher = {OpenReview.net},
 timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
 title = {miniF2F: a cross-system benchmark for formal Olympiad-level mathematics},
 url = {https://openreview.net/forum?id=9ZPegFuFTFv},
 year = {2022}
}

@misc{lampinen_generalization_2025,
 author = {Lampinen, Andrew K. and Chaudhry, Arslan and Chan, Stephanie C. Y. and Wild, Cody and Wan, Diane and Ku, Alex and Bornschein, Jörg and Pascanu, Razvan and Shanahan, Murray and {McClelland}, James L.},
 journal = {ArXiv preprint},
 title = {On the generalization of language models from in-context learning and finetuning: a controlled study},
 url = {https://arxiv.org/abs/2505.00661},
 volume = {abs/2505.00661},
 year = {2025}
}

@inproceedings{wei_inverse_2023,
 address = {Singapore},
 author = {Wei, Jason  and
Kim, Najoung  and
Tay, Yi  and
Le, Quoc},
 booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2023.emnlp-main.963},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 pages = {15580--15591},
 publisher = {Association for Computational Linguistics},
 title = {Inverse Scaling Can Become {U}-Shaped},
 url = {https://aclanthology.org/2023.emnlp-main.963},
 year = {2023}
}

@misc{zuo_ttrl_2025,
 author = {Zuo, Yuxin and Zhang, Kaiyan and Sheng, Li and Qu, Shang and Cui, Ganqu and Zhu, Xuekai and Li, Haozhan and Zhang, Yuchen and Long, Xinwei and Hua, Ermo and Qi, Biqing and Sun, Youbang and Ma, Zhiyuan and Yuan, Lifan and Ding, Ning and Zhou, Bowen},
 journal = {ArXiv preprint},
 title = {{TTRL}: Test-Time Reinforcement Learning},
 url = {https://arxiv.org/abs/2504.16084},
 volume = {abs/2504.16084},
 year = {2025}
}

@inproceedings{wang_chain--thought_2024,
 author = {Xuezhi Wang and
Denny Zhou},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/0002Z24.bib},
 booktitle = {Advances in Neural Information Processing Systems 38: Annual Conference
on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver,
BC, Canada, December 10 - 15, 2024},
 editor = {Amir Globersons and
Lester Mackey and
Danielle Belgrave and
Angela Fan and
Ulrich Paquet and
Jakub M. Tomczak and
Cheng Zhang},
 timestamp = {Thu, 13 Feb 2025 00:00:00 +0100},
 title = {Chain-of-Thought Reasoning Without Prompting},
 url = {http://papers.nips.cc/paper\_files/paper/2024/hash/7a8e7fd295aa04eac4b470ae27f8785c-Abstract-Conference.html},
 year = {2024}
}

@inproceedings{li_chain_2024,
 author = {Zhiyuan Liu and
Hong Liu and
Denny Zhou and
Tengyu Ma},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/0001LZ024.bib},
 booktitle = {The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024},
 publisher = {OpenReview.net},
 timestamp = {Wed, 07 Aug 2024 01:00:00 +0200},
 title = {Chain of Thought Empowers Transformers to Solve Inherently Serial
Problems},
 url = {https://openreview.net/forum?id=3EWTEy9MTM},
 year = {2024}
}

@misc{krishna_fact_2025,
 author = {Krishna, Satyapriya and Krishna, Kalpesh and Mohananey, Anhad and Schwarcz, Steven and Stambler, Adam and Upadhyay, Shyam and Faruqui, Manaal},
 journal = {ArXiv preprint},
 title = {Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation},
 url = {https://arxiv.org/abs/2409.12941},
 volume = {abs/2409.12941},
 year = {2024}
}

@misc{peng_limitations_2024,
 author = {Peng, Binghui and Narayanan, Srini and Papadimitriou, Christos},
 journal = {ArXiv preprint},
 title = {On Limitations of the Transformer Architecture},
 url = {https://arxiv.org/abs/2402.08164},
 volume = {abs/2402.08164},
 year = {2024}
}

@misc{gelada_scaling_2025,
 author = {Gelada, Carles and Buckman, Jacob and Zhang, Sean and Bach, Txus},
 journal = {ArXiv preprint},
 title = {Scaling Context Requires Rethinking Attention},
 url = {https://arxiv.org/abs/2507.04239},
 volume = {abs/2507.04239},
 year = {2025}
}

@inproceedings{shaham_zeroscrolls_2023,
 address = {Singapore},
 author = {Shaham, Uri  and
Ivgi, Maor  and
Efrat, Avia  and
Berant, Jonathan  and
Levy, Omer},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
 doi = {10.18653/v1/2023.findings-emnlp.536},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 pages = {7977--7989},
 publisher = {Association for Computational Linguistics},
 title = {{Z}ero{SCROLLS}: A Zero-Shot Benchmark for Long Text Understanding},
 url = {https://aclanthology.org/2023.findings-emnlp.536},
 year = {2023}
}

@article{reid2024gemini,
 author = {Reid, M. and T. Team, Gemini and others},
 journal = {ArXiv preprint},
 title = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
 url = {https://arxiv.org/abs/2403.05530},
 volume = {abs/2403.05530},
 year = {2024}
}

@article{bai2023qwen,
 author = {Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and others},
 journal = {ArXiv preprint},
 title = {Qwen technical report},
 url = {https://arxiv.org/abs/2309.16609},
 volume = {abs/2309.16609},
 year = {2023}
}

@article{chen2023extending,
 author = {Chen, Shouyuan and Ding, Sherman and Wang, Chen and Li, Lulu and Zha, Dayi and Wang, Li},
 journal = {ArXiv preprint},
 title = {Extending context window of large language models via positional interpolation},
 url = {https://arxiv.org/abs/2306.15595},
 volume = {abs/2306.15595},
 year = {2023}
}

@article{beltagy2020longformer,
 author = {Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
 journal = {ArXiv preprint},
 title = {Longformer: The long-document transformer},
 url = {https://arxiv.org/abs/2004.05150},
 volume = {abs/2004.05150},
 year = {2020}
}

@inproceedings{zaheer2020bigbird,
 author = {Manzil Zaheer and
Guru Guruganesh and
Kumar Avinava Dubey and
Joshua Ainslie and
Chris Alberti and
Santiago Onta{\~{n}}{\'{o}}n and
Philip Pham and
Anirudh Ravula and
Qifan Wang and
Li Yang and
Amr Ahmed},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/ZaheerGDAAOPRWY20.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {Big Bird: Transformers for Longer Sequences},
 url = {https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html},
 year = {2020}
}

@article{bai2023longbench,
 author = {Bai, Yushi and Zheng, Xin and Lv, Tingyu and Cui, Yuran and Chen, Wei},
 journal = {ArXiv preprint},
 title = {Longbench: A bilingual, multitask benchmark for long context understanding},
 url = {https://arxiv.org/abs/2308.14508},
 volume = {abs/2308.14508},
 year = {2023}
}

@inproceedings{shaham2023zeroscrolls,
 address = {Singapore},
 author = {Shaham, Uri  and
Ivgi, Maor  and
Efrat, Avia  and
Berant, Jonathan  and
Levy, Omer},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
 doi = {10.18653/v1/2023.findings-emnlp.536},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 pages = {7977--7989},
 publisher = {Association for Computational Linguistics},
 title = {{Z}ero{SCROLLS}: A Zero-Shot Benchmark for Long Text Understanding},
 url = {https://aclanthology.org/2023.findings-emnlp.536},
 year = {2023}
}

@inproceedings{jimenez2024swebench,
 author = {Carlos E. Jimenez and
John Yang and
Alexander Wettig and
Shunyu Yao and
Kexin Pei and
Ofir Press and
Karthik R. Narasimhan},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/JimenezYWYPPN24.bib},
 booktitle = {The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024},
 publisher = {OpenReview.net},
 timestamp = {Mon, 29 Jul 2024 01:00:00 +0200},
 title = {SWE-bench: Can Language Models Resolve Real-world Github Issues?},
 url = {https://openreview.net/forum?id=VTF8yNQM66},
 year = {2024}
}

@article{liu2023lost,
 address = {Cambridge, MA},
 author = {Liu, Nelson F.  and
Lin, Kevin  and
Hewitt, John  and
Paranjape, Ashwin  and
Bevilacqua, Michele  and
Petroni, Fabio  and
Liang, Percy},
 doi = {10.1162/tacl_a_00638},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {157--173},
 publisher = {MIT Press},
 title = {Lost in the Middle: How Language Models Use Long Contexts},
 url = {https://aclanthology.org/2024.tacl-1.9},
 volume = {12},
 year = {2024}
}

@misc{kamradt2024needle,
 author = {Greg Kamradt},
 howpublished = {\url{https://blog.gregkamradt.com/llm-leaderboard-and-evals/pressure-testing-llms-with-a-needle-in-a-haystack}},
 note = {Accessed: 2025-09-04},
 title = {Pressure Testing LLMs: Needle In A Haystack},
 year = {2024}
}

@misc{anthropic2024,
 author = {{Anthropic}},
 note = {Technical Report},
 title = {Claude 3: Extended Context Windows for Enterprise Applications},
 year = {2024}
}

@misc{openai2024,
 author = {{OpenAI}},
 note = {Technical Report},
 title = {GPT-4: Technical Report},
 year = {2024}
}

@inproceedings{wei2023chain,
 author = {Jason Wei and
Xuezhi Wang and
Dale Schuurmans and
Maarten Bosma and
Brian Ichter and
Fei Xia and
Ed H. Chi and
Quoc V. Le and
Denny Zhou},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/Wei0SBIXCLZ22.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
 year = {2022}
}

@inproceedings{brown2024large,
 address = {Singapore},
 author = {Huang, Jiaxin  and
Gu, Shixiang  and
Hou, Le  and
Wu, Yuexin  and
Wang, Xuezhi  and
Yu, Hongkun  and
Han, Jiawei},
 booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2023.emnlp-main.67},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 pages = {1051--1068},
 publisher = {Association for Computational Linguistics},
 title = {Large Language Models Can Self-Improve},
 url = {https://aclanthology.org/2023.emnlp-main.67},
 year = {2023}
}

@article{park2023generative,
 author = {Park, Joon Sung and O'Brien, Joseph C and Cai, Carrie J and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
 journal = {ArXiv preprint},
 title = {Generative agents: Interactive simulacra of human behavior},
 url = {https://arxiv.org/abs/2304.03442},
 volume = {abs/2304.03442},
 year = {2023}
}

@inproceedings{lewis2020retrieval,
 author = {Patrick S. H. Lewis and
Ethan Perez and
Aleksandra Piktus and
Fabio Petroni and
Vladimir Karpukhin and
Naman Goyal and
Heinrich K{\"{u}}ttler and
Mike Lewis and
Wen{-}tau Yih and
Tim Rockt{\"{a}}schel and
Sebastian Riedel and
Douwe Kiela},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/LewisPPPKGKLYR020.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
 url = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
 year = {2020}
}

@article{katz2023natural,
 author = {Katz, Daniel Martin and Bommarito, Michael J and Gao, Shang and Arredondo, Pablo},
 journal = {ArXiv preprint},
 title = {Natural language processing in the legal domain},
 url = {https://arxiv.org/abs/2302.12039},
 volume = {abs/2302.12039},
 year = {2023}
}

@article{taylor2022galactica,
 author = {Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
 journal = {ArXiv preprint},
 title = {Galactica: A large language model for science},
 url = {https://arxiv.org/abs/2211.09085},
 volume = {abs/2211.09085},
 year = {2022}
}

@inproceedings{zhou2023webarena,
 author = {Shuyan Zhou and
Frank F. Xu and
Hao Zhu and
Xuhui Zhou and
Robert Lo and
Abishek Sridhar and
Xianyi Cheng and
Tianyue Ou and
Yonatan Bisk and
Daniel Fried and
Uri Alon and
Graham Neubig},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/ZhouX0ZLSCOBF0N24.bib},
 booktitle = {The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024},
 publisher = {OpenReview.net},
 timestamp = {Wed, 07 Aug 2024 01:00:00 +0200},
 title = {WebArena: {A} Realistic Web Environment for Building Autonomous Agents},
 url = {https://openreview.net/forum?id=oKn9c6ytLx},
 year = {2024}
}

@inproceedings{zhang2023repocoder,
 address = {Singapore},
 author = {Zhang, Fengji  and
Chen, Bei  and
Zhang, Yue  and
Keung, Jacky  and
Liu, Jin  and
Zan, Daoguang  and
Mao, Yi  and
Lou, Jian-Guang  and
Chen, Weizhu},
 booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2023.emnlp-main.151},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 pages = {2471--2484},
 publisher = {Association for Computational Linguistics},
 title = {{R}epo{C}oder: Repository-Level Code Completion Through Iterative Retrieval and Generation},
 url = {https://aclanthology.org/2023.emnlp-main.151},
 year = {2023}
}

@article{hsieh2024ruler,
 author = {Hsieh, Cheng-Ping and Sun, Simeng and Kriman, Samuel and Acharya, Shantanu and Rekesh, Dima and Jia, Fei and Zhang, Yang and Ginsburg, Boris},
 journal = {ArXiv preprint},
 title = {RULER: What's the Real Context Size of Your Long-Context Language Models?},
 url = {https://arxiv.org/abs/2404.06654},
 volume = {abs/2404.06654},
 year = {2024}
}

@article{levy2024same,
 author = {Levy, Mosh and Jacoby, Alon and Goldberg, Yoav},
 journal = {ArXiv preprint},
 title = {Same task, more tokens: the impact of input length on the reasoning performance of large language models},
 url = {https://arxiv.org/abs/2402.14848},
 volume = {abs/2402.14848},
 year = {2024}
}

@inproceedings{wang2023selfconsistency,
 author = {Xuezhi Wang and
Jason Wei and
Dale Schuurmans and
Quoc V. Le and
Ed H. Chi and
Sharan Narang and
Aakanksha Chowdhery and
Denny Zhou},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/0002WSLCNCZ23.bib},
 booktitle = {The Eleventh International Conference on Learning Representations,
{ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
 publisher = {OpenReview.net},
 timestamp = {Fri, 30 Jun 2023 01:00:00 +0200},
 title = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
 url = {https://openreview.net/pdf?id=1PL1NIMMrw},
 year = {2023}
}

@inproceedings{kryscinski2021booksum,
 address = {Abu Dhabi, United Arab Emirates},
 author = {Kryscinski, Wojciech  and
Rajani, Nazneen  and
Agarwal, Divyansh  and
Xiong, Caiming  and
Radev, Dragomir},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2022},
 doi = {10.18653/v1/2022.findings-emnlp.488},
 editor = {Goldberg, Yoav  and
Kozareva, Zornitsa  and
Zhang, Yue},
 pages = {6536--6558},
 publisher = {Association for Computational Linguistics},
 title = {{BOOKSUM}: A Collection of Datasets for Long-form Narrative Summarization},
 url = {https://aclanthology.org/2022.findings-emnlp.488},
 year = {2022}
}

@article{nakano2021webgpt,
 author = {Nakano, Reiichiro and others},
 journal = {ArXiv preprint},
 title = {WebGPT: Browser-assisted question-answering with human feedback},
 url = {https://arxiv.org/abs/2112.09332},
 volume = {abs/2112.09332},
 year = {2021}
}

@inproceedings{stiennon2020learning,
 author = {Nisan Stiennon and
Long Ouyang and
Jeffrey Wu and
Daniel M. Ziegler and
Ryan Lowe and
Chelsea Voss and
Alec Radford and
Dario Amodei and
Paul F. Christiano},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/StiennonO0ZLVRA20.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {Learning to summarize with human feedback},
 url = {https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html},
 year = {2020}
}

@article{zelikman2024quiet,
 author = {Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D},
 journal = {ArXiv preprint},
 title = {Quiet-star: Language models can teach themselves to think before speaking},
 url = {https://arxiv.org/abs/2403.09629},
 volume = {abs/2403.09629},
 year = {2024}
}

@inproceedings{dai2019transformerxl,
 address = {Florence, Italy},
 author = {Dai, Zihang  and
Yang, Zhilin  and
Yang, Yiming  and
Carbonell, Jaime  and
Le, Quoc  and
Salakhutdinov, Ruslan},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/P19-1285},
 editor = {Korhonen, Anna  and
Traum, David  and
M{\`a}rquez, Llu{\'\i}s},
 pages = {2978--2988},
 publisher = {Association for Computational Linguistics},
 title = {Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context},
 url = {https://aclanthology.org/P19-1285},
 year = {2019}
}

@inproceedings{borgeaud2022retro,
 author = {Sebastian Borgeaud and
Arthur Mensch and
Jordan Hoffmann and
Trevor Cai and
Eliza Rutherford and
Katie Millican and
George van den Driessche and
Jean{-}Baptiste Lespiau and
Bogdan Damoc and
Aidan Clark and
Diego de Las Casas and
Aurelia Guy and
Jacob Menick and
Roman Ring and
Tom Hennigan and
Saffron Huang and
Loren Maggiore and
Chris Jones and
Albin Cassirer and
Andy Brock and
Michela Paganini and
Geoffrey Irving and
Oriol Vinyals and
Simon Osindero and
Karen Simonyan and
Jack W. Rae and
Erich Elsen and
Laurent Sifre},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/BorgeaudMHCRM0L22.bib},
 booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
2022, Baltimore, Maryland, {USA}},
 editor = {Kamalika Chaudhuri and
Stefanie Jegelka and
Le Song and
Csaba Szepesv{\'{a}}ri and
Gang Niu and
Sivan Sabato},
 pages = {2206--2240},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Tue, 12 Jul 2022 01:00:00 +0200},
 title = {Improving Language Models by Retrieving from Trillions of Tokens},
 url = {https://proceedings.mlr.press/v162/borgeaud22a.html},
 volume = {162},
 year = {2022}
}

@article{izacard2022atlas,
 author = {Izacard, Gautier and others},
 journal = {ArXiv preprint},
 title = {Atlas: Few-shot learning with retrieval augmented language models},
 url = {https://arxiv.org/abs/2208.03299},
 volume = {abs/2208.03299},
 year = {2022}
}

@article{press2021alibi,
 author = {Press, Ofir and others},
 journal = {ICLR},
 title = {ALiBi: Attention with linear biases enables input length extrapolation},
 year = {2022}
}

@inproceedings{peng2023yarn,
 author = {Bowen Peng and
Jeffrey Quesnelle and
Honglu Fan and
Enrico Shippole},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/PengQFS24.bib},
 booktitle = {The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024},
 publisher = {OpenReview.net},
 timestamp = {Wed, 07 Aug 2024 01:00:00 +0200},
 title = {YaRN: Efficient Context Window Extension of Large Language Models},
 url = {https://openreview.net/forum?id=wHBfxhZu1u},
 year = {2024}
}

@article{snell2024scaling,
 author = {Snell, Charlie and others},
 journal = {ArXiv preprint},
 title = {Scaling LLM test-time compute optimally can be more effective than scaling model parameters},
 url = {https://arxiv.org/abs/2408.03314},
 volume = {abs/2408.03314},
 year = {2024}
}

@inproceedings{chalkidis2020legal,
 address = {Online},
 author = {Chalkidis, Ilias  and
Fergadiotis, Manos  and
Malakasiotis, Prodromos  and
Aletras, Nikolaos  and
Androutsopoulos, Ion},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
 doi = {10.18653/v1/2020.findings-emnlp.261},
 editor = {Cohn, Trevor  and
He, Yulan  and
Liu, Yang},
 pages = {2898--2904},
 publisher = {Association for Computational Linguistics},
 title = {{LEGAL}-{BERT}: The Muppets straight out of Law School},
 url = {https://aclanthology.org/2020.findings-emnlp.261},
 year = {2020}
}

@article{hendrycks2021cuad,
 author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},
 journal = {NeurIPS Datasets and Benchmarks Track},
 title = {CUAD: An expert-annotated NLP dataset for legal contract review},
 year = {2021}
}

@inproceedings{bommarito2022lexglue,
 address = {Dublin, Ireland},
 author = {Chalkidis, Ilias  and
Jana, Abhik  and
Hartung, Dirk  and
Bommarito, Michael  and
Androutsopoulos, Ion  and
Katz, Daniel  and
Aletras, Nikolaos},
 booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2022.acl-long.297},
 editor = {Muresan, Smaranda  and
Nakov, Preslav  and
Villavicencio, Aline},
 pages = {4310--4330},
 publisher = {Association for Computational Linguistics},
 title = {{L}ex{GLUE}: A Benchmark Dataset for Legal Language Understanding in {E}nglish},
 url = {https://aclanthology.org/2022.acl-long.297},
 year = {2022}
}

@inproceedings{ladhak2020exploring,
 address = {Online},
 author = {Ladhak, Faisal  and
Li, Bryan  and
Al-Onaizan, Yaser  and
McKeown, Kathleen},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/2020.acl-main.453},
 editor = {Jurafsky, Dan  and
Chai, Joyce  and
Schluter, Natalie  and
Tetreault, Joel},
 pages = {5043--5054},
 publisher = {Association for Computational Linguistics},
 title = {Exploring Content Selection in Summarization of Novel Chapters},
 url = {https://aclanthology.org/2020.acl-main.453},
 year = {2020}
}

@article{chang2023booookscore,
 author = {Chang, Yapei and Huang, Kyle and Chen, Xuanhui and Li, Zixuan and Luo, Fei and Qiu, Yujie and Wang, Hang and Chen, Chen},
 journal = {ArXiv preprint},
 title = {BookScore: A systematic exploration of book-length summarization in the era of LLMs},
 url = {https://arxiv.org/abs/2310.00785},
 volume = {abs/2310.00785},
 year = {2023}
}

@article{wu2021recursively,
 author = {Wu, Jeff and Ouyang, Long and Ziegler, Daniel M and Stiennon, Nisan and Lowe, Ryan and Leike, Jan and Christiano, Paul},
 journal = {ArXiv preprint},
 title = {Recursively summarizing books with human feedback},
 url = {https://arxiv.org/abs/2109.10862},
 volume = {abs/2109.10862},
 year = {2021}
}

@inproceedings{ding2402longrope,
 author = {Yiran Ding and
Li Lyna Zhang and
Chengruidong Zhang and
Yuanyuan Xu and
Ning Shang and
Jiahang Xu and
Fan Yang and
Mao Yang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/DingZZXSX0Y24.bib},
 booktitle = {Forty-first International Conference on Machine Learning, {ICML} 2024,
Vienna, Austria, July 21-27, 2024},
 publisher = {OpenReview.net},
 timestamp = {Mon, 21 Oct 2024 01:00:00 +0200},
 title = {LongRoPE: Extending {LLM} Context Window Beyond 2 Million Tokens},
 url = {https://openreview.net/forum?id=ONOtpXLqqw},
 year = {2024}
}

@inproceedings{liu2024chatqa,
 author = {Zihan Liu and
Wei Ping and
Rajarshi Roy and
Peng Xu and
Chankyu Lee and
Mohammad Shoeybi and
Bryan Catanzaro},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/0001P00LSC24.bib},
 booktitle = {Advances in Neural Information Processing Systems 38: Annual Conference
on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver,
BC, Canada, December 10 - 15, 2024},
 editor = {Amir Globersons and
Lester Mackey and
Danielle Belgrave and
Angela Fan and
Ulrich Paquet and
Jakub M. Tomczak and
Cheng Zhang},
 timestamp = {Thu, 13 Feb 2025 00:00:00 +0100},
 title = {ChatQA: Surpassing {GPT-4} on Conversational {QA} and {RAG}},
 url = {http://papers.nips.cc/paper\_files/paper/2024/hash/1c0d54ebd0a6e58c4eca7d591e374b9d-Abstract-Conference.html},
 year = {2024}
}

@inproceedings{chen2024benchmarking,
 author = {Jiawei Chen and
Hongyu Lin and
Xianpei Han and
Le Sun},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/aaai/0011LH024.bib},
 booktitle = {Thirty-Eighth {AAAI} Conference on Artificial Intelligence, {AAAI}
2024, Thirty-Sixth Conference on Innovative Applications of Artificial
Intelligence, {IAAI} 2024, Fourteenth Symposium on Educational Advances
in Artificial Intelligence, {EAAI} 2014, February 20-27, 2024, Vancouver,
Canada},
 doi = {10.1609/AAAI.V38I16.29728},
 editor = {Michael J. Wooldridge and
Jennifer G. Dy and
Sriraam Natarajan},
 pages = {17754--17762},
 publisher = {{AAAI} Press},
 timestamp = {Tue, 02 Apr 2024 01:00:00 +0200},
 title = {Benchmarking Large Language Models in Retrieval-Augmented Generation},
 url = {https://doi.org/10.1609/aaai.v38i16.29728},
 year = {2024}
}

@article{su2024roformer,
 author = {Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
 journal = {Neurocomputing},
 pages = {127063},
 publisher = {Elsevier},
 title = {Roformer: Enhanced transformer with rotary position embedding},
 volume = {568},
 year = {2024}
}

@inproceedings{wei2022chainofthought,
 author = {Jason Wei and
Xuezhi Wang and
Dale Schuurmans and
Maarten Bosma and
Brian Ichter and
Fei Xia and
Ed H. Chi and
Quoc V. Le and
Denny Zhou},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/Wei0SBIXCLZ22.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
 year = {2022}
}

@inproceedings{kojima2022large,
 author = {Takeshi Kojima and
Shixiang Shane Gu and
Machel Reid and
Yutaka Matsuo and
Yusuke Iwasawa},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/KojimaGRMI22.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {Large Language Models are Zero-Shot Reasoners},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html},
 year = {2022}
}

@inproceedings{nye2021show,
 author = {Nye, Maxwell and Andreas, Jacob},
 booktitle = {NeurIPS},
 title = {Show Your Work: Scratchpads for Intermediate Computation with Language Models},
 year = {2021}
}

@inproceedings{yao2023tree,
 author = {Shunyu Yao and
Dian Yu and
Jeffrey Zhao and
Izhak Shafran and
Tom Griffiths and
Yuan Cao and
Karthik Narasimhan},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/YaoYZS00N23.bib},
 booktitle = {Advances in Neural Information Processing Systems 36: Annual Conference
on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023},
 editor = {Alice Oh and
Tristan Naumann and
Amir Globerson and
Kate Saenko and
Moritz Hardt and
Sergey Levine},
 timestamp = {Fri, 01 Mar 2024 00:00:00 +0100},
 title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
 url = {http://papers.nips.cc/paper\_files/paper/2023/hash/271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html},
 year = {2023}
}

@inproceedings{shinn2023reflexion,
 author = {Noah Shinn and
Federico Cassano and
Ashwin Gopinath and
Karthik Narasimhan and
Shunyu Yao},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/ShinnCGNY23.bib},
 booktitle = {Advances in Neural Information Processing Systems 36: Annual Conference
on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023},
 editor = {Alice Oh and
Tristan Naumann and
Amir Globerson and
Kate Saenko and
Moritz Hardt and
Sergey Levine},
 timestamp = {Fri, 01 Mar 2024 00:00:00 +0100},
 title = {Reflexion: language agents with verbal reinforcement learning},
 url = {http://papers.nips.cc/paper\_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html},
 year = {2023}
}
