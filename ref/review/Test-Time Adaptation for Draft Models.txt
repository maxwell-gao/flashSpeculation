Adaptive Speculation: Test-Time Adaptation of Auxiliary Cross-Attention Modules Reading Frozen Backbone Representations
1. Introduction: The Convergence of Efficiency and Adaptation
The deployment of Large Language Models (LLMs) in production environments is currently constrained by a fundamental tension between model capability and inference latency. Scaling laws consistently demonstrate that increasing parameter counts—from 0.5B to 4B, 70B, and beyond—yields predictable and necessary improvements in reasoning, world knowledge, and coherence. However, the autoregressive nature of standard decoding, where the generation of each token is conditioned on the complete history of previous tokens, imposes a sequential dependency that binds latency linearly to the output length. This creates a "latency wall" that cannot be scaled simply by adding more parallel compute resources, as the generation process is inherently serial.
To circumvent this, Speculative Decoding has emerged as a dominant paradigm. This technique employs a smaller "draft" model to propose a sequence of tokens, which are then verified in parallel by the larger "target" model. The efficiency of this system is governed by the acceptance rate ($\alpha$)—the probability that the target model accepts the draft model's proposal. Standard speculative decoding relies on a static draft model, trained offline to approximate the target model's general distribution. While effective for generic inputs, this approach suffers from a "static alignment problem": when the input context shifts to a specific domain (e.g., specialized code, medical texts, or highly stylistic prose) that lies in the tail of the training distribution, the static draft model's predictions diverge from the target model, $\alpha$ collapses, and the system reverts to the speed of the target model, incurring the overhead of the drafter without the benefit.
This report investigates a novel architectural evolution of this paradigm: Adaptive Speculative Decoding. The proposed system comprises a 0.5B parameter draft model that generates text by reading intermediate hidden states from 5 specific layers of a frozen 4B target model via Cross-Attention. Crucially, the system employs Test-Time Training (TTT), adapting the draft model's weights (specifically via Low-Rank Adaptation, LoRA) to the specific input context at inference time using a self-supervised loss.
This architecture represents a convergence of three distinct research streams: Parameter-Efficient Transfer Learning (PETL), specifically the reading of frozen representations; Test-Time Adaptation (TTA), utilizing the input prompt as a training signal; and Online Knowledge Distillation, where the frozen target model acts as a real-time teacher. This report provides an exhaustive analysis of the theoretical underpinnings, prior art, training dynamics, and evaluation metrics for this system, synthesizing findings from recent literature (2023–2026) to validate the design choices and identify critical implementation strategies.
________________
2. Architectural Foundations: The Reader Mechanism
The first critical component of the proposed system is the mechanism by which the 0.5B draft model accesses information from the 4B target model. The proposal specifies utilizing Cross-Attention to read intermediate hidden states from 5 specific layers of the frozen backbone. This section analyzes the theoretical justification for this choice, contrasting it with alternative methods like linear probing, and grounding it in the Introspective Cross-Attention (InCA) framework.
2.1 The Theoretical Limit of Linear Probing
Historically, accessing information from frozen pre-trained backbones has been achieved through linear probes or simple concatenation. In architectures like Ladder Side-Tuning (LST) 1, a side network receives inputs from the backbone via "ladder" connections that typically employ linear projections followed by gating mechanisms. While computationally efficient ($O(N)$), linear probing operates under a restrictive assumption: that the task-relevant information is spatially aligned or linearly separable within the aggregated representation.
However, recent theoretical work challenges the sufficiency of linear methods for complex tasks. When a backbone is frozen, its internal representations are fixed high-dimensional vectors. For a specific downstream task—in this case, predicting the next token in a specific context—the relevant signal (e.g., a variable definition mentioned 1000 tokens ago, or a stylistic cue) may be localized in a small subset of tokens, effectively buried in "noise" from the perspective of the current query. A linear probe, which computes a weighted sum of the input features, inevitably dilutes this sparse signal with the noise from irrelevant tokens. As the sequence length ($T$) increases, the Signal-to-Noise Ratio (SNR) of a linear probe degrades, leading to a theoretical upper bound on its performance.3
2.2 Introspective Cross-Attention (InCA): Prior Art and Validation
The most direct validation of the user's "Cross-Attention Reader" architecture is found in the work on InCA (Introspective Cross-Attention) by Zancato et al. (2023).4 InCA was proposed as a method for transfer learning that decouples the adaptation parameters from the backbone entirely. Instead of inserting adapters inside the transformer blocks (which requires backpropagating through the backbone or storing activations), InCA operates as an external module that "looks into" the frozen model.
2.2.1 Mechanism of Action
The InCA module treats the frozen intermediate representations of the backbone as a database of Keys ($K$) and Values ($V$), while the adapter generates learnable Queries ($Q$).
Formally, let $H_l \in \mathbb{R}^{T \times D}$ be the hidden states of the target model at layer $l$. The InCA adapter computes:


$$\text{Attention}(Q, H_l) = \text{softmax}\left( \frac{Q (H_l W_K)^T}{\sqrt{d}} \right) (H_l W_V)$$
This mechanism allows the adapter to perform a content-based search over the frozen representation. Unlike a linear probe, the softmax operation is non-linear and competitive. It allows the model to assign near-zero weight to irrelevant tokens and focus almost exclusively on the "needle in the haystack."
2.2.2 Theoretical Proof of Expressivity (Theorem D.1)
The InCA paper provides a rigorous proof, Theorem D.1 3, which establishes the superior expressivity of cross-attention over linear layers for reading frozen representations. The theorem defines a class of tasks called Permutable Token-Separable (PTS) tasks. A task is PTS if the information required to solve it is contained within a specific token (or set of tokens) whose position is variable (permutable) across different inputs.
* Linear Limitation: The theorem proves that for PTS tasks, a linear model has a probability of error that is bounded away from zero as the sequence length grows. The linear model cannot dynamically "shift its gaze" to find the permutable token; it must learn a fixed weight for every position, which fails when the relevant position varies.
* Cross-Attention Advantage: The theorem demonstrates that a cross-attention mechanism can solve PTS tasks with probability approaching 1. The learnable query $Q$ acts as a semantic search vector, allowing the model to locate and retrieve the relevant token regardless of its position in the sequence $T$.
Implication for the Proposed System: In the context of the user's system, the "task" is predicting the next token based on the prompt. This is inherently a PTS task. For example, to predict the closing brace of a function in code, the model must attend to the opening brace, the position of which varies with every script. A linear ladder connection (as in LST) would struggle to isolate this distant signal amidst the intervening code. The Cross-Attention reader, however, can explicitly query the frozen backbone for "opening brace features," retrieving the exact hidden state required for accurate prediction. This confirms that the user's choice of cross-attention is not just an architectural preference but a theoretical necessity for maximizing the information extracted from the frozen target.3
2.3 Layer Selection and Information Bottleneck
The proposal specifies reading from "5 specific layers." The selection of these layers is critical. The theory of Information Bottleneck (IB) in deep learning suggests that deep networks compress information as it propagates through layers, discarding nuisance variables to retain only task-relevant features.6
* Early Layers: Contain high-fidelity local information (syntax, surface forms).
* Middle Layers: Often exhibit the highest "effective rank" or entropy, representing a transition from lexical to semantic processing.8
* Late Layers: Highly compressed, task-specific semantic representations (abstract concepts).
Research into Ladder Side-Tuning and InCA suggests that reading from a distributed set of layers (e.g., layers 5, 10, 15, 20, 25 in a 32-layer model) is optimal.2 This "multi-scale" reading allows the draft model to access both the syntactic scaffolding (early layers) required for grammatical generation and the semantic reasoning (late layers) required for coherence. If the draft model only read from the final layer, it might miss the fine-grained syntactic cues necessary for exact token matching; if it only read from early layers, it would lack the reasoning context.
2.4 Comparison to DFlash and Modern Drafters
The architecture proposed aligns with the cutting edge of speculative decoding research as of 2026. Specifically, the DFlash architecture 9, introduced in February 2026, employs a "Block Diffusion" drafter that is explicitly conditioned on "context features extracted from the target model." DFlash utilizes a technique called KV Injection, where the target model's hidden states are projected and injected into the draft model's attention mechanism.9 This is functionally identical to the user's cross-attention proposal. The DFlash authors empirically demonstrate that this conditioning is essential for achieving high acceptance rates, further validating the user's architectural premise. The primary difference is that DFlash uses this for a diffusion drafter, whereas the user proposes an autoregressive drafter (implied by the 0.5B parameter specification). However, the principle of Target-Augmented Drafting remains the valid core.
________________
3. Theoretical Framework: Test-Time Training (TTT)
The second pillar of the proposed system is Test-Time Training (TTT). The user intends to update the LoRA weights of the draft model on the input context before generation. This moves the system from a static paradigm (fixed weights) to a dynamic paradigm (context-dependent weights). This section explores the theoretical necessity of TTT, focusing on "Score Dilution" and "Distribution Shift."
3.1 The Problem of Score Dilution (qTTT)
Why is a static attention mechanism insufficient for long or complex contexts? Recent research on qTTT (Query-only Test-Time Training) 13 provides a mathematical answer centered on the phenomenon of Score Dilution.
In a standard dot-product attention mechanism:


$$\alpha_i = \frac{\exp(q^T k_i)}{\sum_{j} \exp(q^T k_j)}$$
As the context length $N$ increases, the denominator (partition function) grows. Unless the query $q$ is perfectly aligned with the key $k_i$ of the relevant token (the "needle") and perfectly orthogonal to all other keys (distractors), the probability mass $\alpha_i$ assigned to the relevant token decreases roughly as $1/N$. Eventually, the score of the correct token becomes indistinguishable from the scores of the varied distractors. The "Target-Distractor Margin" collapses, and the model begins to hallucinate or lose focus.15
The TTT Solution:
The qTTT paper proves that updating the Query projection matrix ($W_Q$) on the specific context can resolve this. By minimizing an entropy-based or self-supervised loss on the context, the model effectively "rotates" the query vectors $q$ to maximize their alignment with the salient keys present in the current context.
This provides a theoretical basis for the user's plan. It suggests that the TTT updates should focus primarily on the Cross-Attention Query Projections of the draft model. These are the parameters that control what the draft model looks for in the frozen target. By updating them, the draft model "calibrates" its search mechanism for the specific document at hand.
3.2 Distribution Shift and the "Tail" Problem
LLMs are trained on a massive distribution $\mathcal{D}_{train}$. At test time, the user provides a specific prompt $x \sim \mathcal{D}_{user}$. Often, $\mathcal{D}_{user}$ is a specific subset (a "shard") of the training distribution, or even an out-of-distribution sample.
For a small draft model (0.5B), the capacity to model the entire $\mathcal{D}_{train}$ is limited. It likely underfits the tails of the distribution.
* Static Drafter: Must compromise to minimize average loss over all domains. It is a "Jack of all trades, master of none."
* Adaptive Drafter (TTT): By training on the prompt $x$, the draft model effectively performs Instance-Specific Domain Adaptation. It shifts its parameters to locally minimize loss on the manifold of the current document.
This is particularly relevant for the Auxiliary Reader setup. The frozen target model (4B) has a rich representation of the specific prompt. The 0.5B draft model might typically struggle to interpret these complex representations. TTT allows the 0.5B model to learn a transient mapping (via LoRA) that decodes the 4B model's specific internal state for this specific prompt.
3.3 TTT-E2E: The Meta-Learning Framework
The user asks about "Meta-Learning." This is crucial because standard pre-trained weights are not optimized for fast adaptation. If one simply takes a standard model and runs Gradient Descent (GD) on a prompt, the updates might be unstable or require too many steps to be feasible for inference latency. The paper "End-to-End Test-Time Training" (TTT-E2E) 16 formulates this as a meta-learning problem.
* Standard Training: $\min_\theta \mathbb{E}[\mathcal{L}(x)]$
* TTT-E2E Training: $\min_\theta \mathbb{E}[\mathcal{L}(\theta - \eta \nabla \mathcal{L}_{internal}(x), x_{future})]$
In TTT-E2E, the model's initialization $\theta$ is explicitly trained such that a single step (or few steps) of gradient descent on the context $x$ results in a model that is optimal for predicting the continuation of $x$. For the user's system, this implies that the LoRA adapter should not be initialized randomly or just pre-trained as a static module. It should be meta-trained using the TTT-E2E objective. The LoRA weights should be learned to be "malleable"—predisposed to snap into alignment with the context after just one or two gradient updates.18
________________
4. Training Signals: The Self-Supervised Loss
The core operational question in the user's request is: "Using a self-supervised loss computed on the input context alone." Since the ground truth (the future response) is unknown at inference time, what signal drives the adaptation? We identify and analyze three distinct classes of training signals suitable for this architecture.
4.1 Causal Language Modeling (CLM) on the Prompt
This is the most straightforward signal and the one utilized by TTT-E2E.18
* Mechanism: The draft model is tasked with predicting the tokens of the prompt itself autoregressively.
$$\mathcal{L}_{CLM} = - \sum_{t=1}^{T_{prompt}} \log P_{draft}(x_t | x_{<t}, H_{target})$$
* Logic: If the draft model can accurately predict the tokens in the prompt (using the frozen target's representations), it has likely adapted its internal state to the syntax, vocabulary, and style of the document.
* Pros: Directly aligns with the generation task. No need for proxy targets.
* Cons: Computational cost. Calculating the loss requires a forward pass over the prompt. For long prompts, this is expensive, though it can be batched.
4.2 Online Knowledge Distillation (Feature Reconstruction)
Since the system includes a frozen 4B target model that also processes the prompt (to populate its KV cache), we have access to a powerful "Teacher" for free. This allows for Online Knowledge Distillation.19 Instead of predicting tokens, the draft model can be trained to reconstruct the hidden states of the target model.
   * Mechanism: Let $h_{draft}^{(L)}$ be the final hidden state of the draft model. Let $h_{target}^{(L)}$ be the final (or intermediate) hidden state of the target.
$$ \mathcal{L}_{Distill} = |
| h_{draft}^{(L)} - \text{Proj}(h_{target}^{(L)}) ||^2 $$
      * Logic: The target model's hidden states contain rich semantic information about the prompt (e.g., disambiguated entities, resolved coreference). By forcing the draft model to match these states, we align the drafter's "thought process" with the target's.
      * Connection to Prior Art: This aligns with the "Feature Alignment" strategies in Online Distillation literature, where student and teacher are aligned at intermediate layers.21 In this "Instance-Specific" case, we are distilling the teacher's understanding of this specific prompt into the student.
      * Advantage: This signal is often denser and more informative than the sparse cross-entropy signal of CLM. It forces the draft model to capture the full dimensionality of the target's reasoning.
4.3 Query Entropy Minimization (From qTTT)
As derived from the qTTT paper 15, another unsupervised signal is the entropy of the attention distribution.
      * Mechanism:
$$\mathcal{L}_{Entropy} = \sum_{h=1}^{H} \text{Entropy}(\text{Attn}_h)$$
      * Logic: Minimizing entropy forces the model to make "confident" decisions about which tokens in the backbone to attend to. This directly combats Score Dilution.
      * Usage: Typically used as a regularizer alongside CLM or Distillation, rather than a standalone objective.
4.4 Synthesis: The Recommended Hybrid Loss
For the specific architecture described (0.5B reading 4B), a Hybrid Loss is theoretically optimal:


$$\mathcal{L}_{TTT} = \mathcal{L}_{CLM}(x_{prompt}) + \lambda \mathcal{L}_{Distill}(H_{draft}, H_{target})$$
The CLM loss ensures the model remains a valid language generator. The Distillation loss (Feature Reconstruction) leverages the "free" supervision from the frozen target to accelerate adaptation. Since the target model's states are already computed during the prefill phase, computing $\mathcal{L}_{Distill}$ adds minimal overhead relative to the value of the signal.
________________
5. Connection to Online Knowledge Distillation
The user explicitly requests an investigation into the connection to Online Knowledge Distillation (OKD).
5.1 Redefining OKD for Inference Adaptation
Traditional OKD 19 involves training a student and teacher simultaneously (or a student against a dynamic teacher) on a training dataset. The user's setup can be rigorously framed as Instance-Specific Online Distillation.
         * The "Curriculum": The curriculum is reduced to a single document: the user's prompt.
         * The "Teacher": The frozen 4B target model acts as a fixed teacher for this curriculum.
         * The "Student": The 0.5B draft model is the student.
         * The "Online" Aspect: The training happens during the inference request life-cycle.
5.2 Theoretical Implications
This connection allows us to apply findings from OKD literature to the user's system:
         1. Teacher-Student Gap: OKD literature suggests that if the teacher is too strong (too large), the student may struggle to mimic it. However, the use of Cross-Attention mitigates this. By allowing the student to directly read the teacher's internal states (InCA), we bridge the capacity gap. The student doesn't need to generate the teacher's thoughts; it only needs to retrieve them.
         2. Dark Knowledge: Distillation allows the transfer of "dark knowledge" (the relations between incorrect classes in the logits). By using a distillation loss (matching logits or features) during TTT, the draft model adapts not just to the correct tokens in the prompt, but to the target model's uncertainty and nuance regarding the prompt. This improves the acceptance rate because the draft model learns to mimic the target's probability distribution, including its entropy.
5.3 Adaptation vs. Generalization
Standard distillation aims for generalization. Here, we aim for overfitting (in a beneficial sense). We want the draft model to overfit the prompt's specific distribution because that distribution is the only one that matters for the next $N$ generated tokens. This flips the standard OKD objective on its head: we prioritize low bias on the current instance over low variance across the population.
________________
6. Meta-Learning: The Engine of Initialization
The success of TTT relies heavily on the Meta-Learning of the draft model's parameters (specifically the LoRA weights).
6.1 The Bi-Level Optimization Problem
Standard pre-training produces weights $\theta_{pre}$ that minimize the loss $\mathcal{L}(x)$. However, we want weights $\theta_0$ that minimize the loss after an update step. This is a bi-level optimization problem formalized in TTT-E2E 18 and MAML (Model-Agnostic Meta-Learning).23


$$\theta^* = \text{argmin}_\theta \sum_{x \in \mathcal{D}} \mathcal{L}_{val} ( U(\theta, \nabla \mathcal{L}_{train}(x^{ctx})), x^{fut} )$$
Where:
         * $U$ is the update function (e.g., one step of SGD).
         * $x^{ctx}$ is the context (prompt) used for TTT.
         * $x^{fut}$ is the future (continuation) used for evaluating the adaptation.
6.2 The "Fast Weights" View
In this framework, the LoRA weights of the draft model act as Fast Weights or "short-term memory".25
         * Slow Weights: The fixed parameters of the cross-attention module (if frozen) or the meta-learned initialization $\theta_0$. These capture general linguistic rules.
         * Fast Weights: The LoRA parameters $\theta'$ after TTT. These capture the specific variable names, writing style, and topic of the current prompt.
The "0.5B draft model" essentially becomes a programmable network. The input prompt acts as the "code" that programs the network (via gradient updates) to behave in a specific way for the duration of the session.
6.3 Implementation for the User
To make this system work, the user must not skip the meta-training stage.
         1. Phase 1 (Meta-Training): Train the 0.5B draft model (with LoRA) on a large corpus. For each batch:
         * Split sequence into Context and Target.
         * Perform $k$ gradient steps on Context (using CLM or Distillation loss).
         * Compute loss on Target using the updated weights.
         * Backpropagate the target loss all the way to the initial weights (requires computing gradients through gradients, or using first-order approximations like Reptile).
         2. Phase 2 (Inference):
         * Initialize LoRA with the meta-learned $\theta^*$.
         * Run TTT on the prompt.
         * Generate.
________________
7. Evaluation and Feasibility Analysis
The proposed system introduces significant complexity. Is it worth it? This section evaluates the trade-offs using data from the literature.
7.1 The Latency Equation
The goal of speculative decoding is to reduce latency.
Let $C_{target}$ be the cost of the target model per token.
Let $C_{draft}$ be the cost of the draft model per token.
Let $\gamma$ be the speculation length (e.g., 5 tokens).
Let $\alpha$ be the acceptance rate.
Let $C_{TTT}$ be the one-time cost of Test-Time Training.
The time per token $T$ is:


$$T_{adaptive} = \frac{C_{TTT}}{L_{gen}} + \frac{C_{draft}(\gamma+1) + C_{target}}{\gamma \alpha' + 1}$$
Where $L_{gen}$ is the number of tokens generated, and $\alpha'$ is the acceptance rate after adaptation.
The Feasibility Condition:
The adaptive system beats a static speculative system if:


$$\frac{C_{TTT}}{L_{gen}} < \frac{C_{draft}(\gamma+1) + C_{target}}{\gamma \alpha + 1} - \frac{C_{draft}(\gamma+1) + C_{target}}{\gamma \alpha' + 1}$$
Essentially, the overhead of TTT must be amortized by the generated length $L_{gen}$ and the improvement in acceptance rate $(\alpha' - \alpha)$.
7.2 Data Comparison
         * Static Speculative Decoding: Typical $\alpha \approx 0.5 - 0.7$ for in-distribution tasks. For out-of-distribution (e.g., code), $\alpha$ can drop to $< 0.3$, making speculation slower than standard decoding.
         * Adaptive Improvement: TTT-E2E and qTTT papers report that adaptation can improve perplexity (and thus acceptance) significantly on long-context or shifted data. For example, qTTT improves accuracy on LongBench by +12.6%.15 In speculative terms, this could translate to increasing $\alpha$ from 0.3 to 0.6 on hard prompts.
         * TTT Overhead: Performing 1-2 steps of SGD on a LoRA module (0.5B params) is relatively cheap compared to a full 4B forward pass, especially if using optimized kernels. If $C_{TTT} \approx 2 \times C_{target}$ (a conservative estimate for a few steps of backprop on small LoRA), and $L_{gen} = 500$ tokens, the amortized cost is negligible ($0.004 \times C_{target}$).
Conclusion: The system is highly viable for long-form generation (summarization, story generation, code completion). It is likely not viable for short conversational turns (e.g., "Hi, how are you?") where $L_{gen}$ is small.
7.3 Comparative Analysis with DFlash
The DFlash (2026) paper provides a benchmark. DFlash achieves 6x acceleration using a block diffusion drafter with KV injection.9 The user's system is an autoregressive counterpart to DFlash.
         * DFlash: Parallel generation (Diffusion), Static weights (conditioned on target features).
         * User System: Serial generation (Autoregressive), Dynamic weights (TTT).
         * Trade-off: DFlash is faster per draft step (parallel). The User System is potentially more accurate ($\alpha$) because it adapts the weights to the prompt, not just the activations.
________________
8. Strategic Implementation Roadmap
Based on the exhaustive investigation, the following roadmap is recommended for building the system:
         1. Architecture:
         * Backbone: 4B Frozen Target.
         * Reader: Cross-Attention connected to layers $\{L/5, 2L/5, 3L/5, 4L/5, L\}$ (approx. layers 6, 12, 18, 24, 30 for a 32-layer model) to satisfy the Information Bottleneck requirement for multi-scale features.
         * Drafter: 0.5B model with LoRA adapters injected into Attention Query and MLP projections.
         2. Meta-Training (Offline):
         * Use the TTT-E2E objective.
         * Loss: Hybrid (CLM + Feature Distillation).
         * Optimize the LoRA initialization to be sensitive to the gradients of the prompt.
         3. Inference Pipeline:
         * Step 1: Run Target Prefill on Prompt $\to$ Get $KV_{target}$ and $H_{target}$.
         * Step 2: Run Draft TTT.
         * Input: Prompt.
         * Target: $H_{target}$ (Feature Reconstruction Loss).
         * Update: 2-5 steps of SGD on LoRA weights.
         * Step 3: Run Speculative Loop.
         * Draft generates 5 tokens using updated weights and cross-attention.
         * Target verifies.
9. Conclusion
The proposed system—a Test-Time Training Augmented Auxiliary Cross-Attention Drafter—is a scientifically robust architecture that addresses the primary failure modes of modern inference: latency and distribution shift.
By grounding the design in the Permutable Token-Separability Theorem (validating Cross-Attention), Score Dilution Theory (validating TTT), and Online Knowledge Distillation (validating the training signal), this report confirms that the system is not merely a heuristic combination of parts, but a coherent solution to the problem of Instance-Specific Inference Optimization.
The synthesis of InCA's reading capability with TTT-E2E's adaptive learning creates a "smart student" that not only reads the teacher's notes (the frozen layers) but effectively "crams" for the specific test question (the prompt) immediately before answering. While implementation complexity is high—requiring a bespoke meta-training pipeline—the potential gains in acceptance rate and latency reduction for long-context tasks are substantiated by the leading research of 2023–2026.
Component
	Recommendation
	Theoretical Basis
	Reader
	Cross-Attention
	InCA / Theorem D.1: Superior extraction of permutable tokens compared to linear probes.
	Layers
	Multi-scale (5 layers)
	Information Bottleneck: Capture syntactic (early) and semantic (late) features.
	Adaptation
	TTT on LoRA (Query Proj)
	qTTT / Score Dilution: Restores target-distractor margin in long contexts.
	Signal
	Hybrid (CLM + Distill)
	Online Distillation: Leverages frozen target as "free" teacher for feature alignment.
	Training
	Meta-Learning (Bi-level)
	TTT-E2E: Optimizes initialization for fast, stable few-shot adaptation.
	引用的著作
         1. LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning - OpenReview, 访问时间为 二月 18, 2026， https://openreview.net/pdf?id=isPnnaTZaP5
         2. LST: Ladder Side-Tuning for Parameter and Memory Efficient ..., 访问时间为 二月 18, 2026， https://arxiv.org/pdf/2206.06522
         3. Your representations are in the network: composable and parallel ..., 访问时间为 二月 18, 2026， https://arxiv.org/abs/2303.04105
         4. Alessandro ACHILLE | University of California, Los Angeles, Los Angeles | UCLA | Department of Computer Science | Research profile - ResearchGate, 访问时间为 二月 18, 2026， https://www.researchgate.net/profile/Alessandro-Achille-2
         5. Your representations are in the network: composable and ... - NeurIPS, 访问时间为 二月 18, 2026， https://proceedings.neurips.cc/paper_files/paper/2023/file/5be3783ea9d43d7add5409c101d87d83-Paper-Conference.pdf
         6. Molecular graph-based invariant representation learning with environmental inference and subgraph generation for out-of-distribution generalization - PMC, 访问时间为 二月 18, 2026， https://pmc.ncbi.nlm.nih.gov/articles/PMC12866046/
         7. DARN: Dynamic Adaptive Regularization Networks for Efficient and Robust Foundation Model Adaptation - arXiv, 访问时间为 二月 18, 2026， https://arxiv.org/html/2511.04766v1
         8. Layer by Layer: Uncovering Hidden Representations in Language Models - arXiv, 访问时间为 二月 18, 2026， https://arxiv.org/html/2502.02013v2
         9. arxiv.org, 访问时间为 二月 18, 2026， https://arxiv.org/html/2602.06036v1
         10. z-lab/dflash: DFlash: Block Diffusion for Flash Speculative ... - GitHub, 访问时间为 二月 18, 2026， https://github.com/z-lab/dflash
         11. DFlash: Block Diffusion for Flash Speculative Decoding - arXiv, 访问时间为 二月 18, 2026， https://arxiv.org/abs/2602.06036
         12. DFlash: Block Diffusion for Flash Speculative Decoding - arXiv, 访问时间为 二月 18, 2026， https://arxiv.org/pdf/2602.06036
         13. Test-Time Training for Long-Context LLMs - Emergent Mind, 访问时间为 二月 18, 2026， https://www.emergentmind.com/papers/2512.13898
         14. Let's (not) just put things in Context: Test-Time Training for Long-Context LLMs - arXiv, 访问时间为 二月 18, 2026， https://arxiv.org/html/2512.13898v1
         15. Let's (not) just put things in Context: Test-time Training for Long ..., 访问时间为 二月 18, 2026， https://openreview.net/forum?id=H0bcEdPCoc
         16. Hardware-Aligned and Natively Trainable Sparse Attention | Request PDF - ResearchGate, 访问时间为 二月 18, 2026， https://www.researchgate.net/publication/394300346_Native_Sparse_Attention_Hardware-Aligned_and_Natively_Trainable_Sparse_Attention
         17. [R] End-to-End Test-Time Training for Long Context : r/MachineLearning - Reddit, 访问时间为 二月 18, 2026， https://www.reddit.com/r/MachineLearning/comments/1pz1wmb/r_endtoend_testtime_training_for_long_context/
         18. End-to-End Test-Time Training for Long Context, 访问时间为 二月 18, 2026， https://test-time-training.github.io/e2e.pdf
         19. Knowledge Distillation Based Recommendation Systems: A Comprehensive Survey - MDPI, 访问时间为 二月 18, 2026， https://www.mdpi.com/2079-9292/14/8/1538
         20. BookKD: A novel knowledge distillation for reducing distillation costs by decoupling knowledge generation and learning - Xidian, 访问时间为 二月 18, 2026， https://web.xidian.edu.cn/rhshang/files/66797e8f335bd.pdf
         21. A Comprehensive Survey on Knowledge Distillation - arXiv.org, 访问时间为 二月 18, 2026， https://arxiv.org/html/2503.12067v1
         22. Exploring Public Data Vulnerabilities in Semi-Supervised Learning Models through Gray-box Adversarial Attack - MDPI, 访问时间为 二月 18, 2026， https://www.mdpi.com/2079-9292/13/5/940
         23. A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond - arXiv, 访问时间为 二月 18, 2026， https://arxiv.org/html/2503.21614v1
         24. Track: Poster Session 1 East - ICML 2026, 访问时间为 二月 18, 2026， https://icml.cc/virtual/2025/session/50263
         25. Speed Always Wins: A Survey on Efficient Architectures for Large Language Models - arXiv, 访问时间为 二月 18, 2026， https://arxiv.org/html/2508.09834v1