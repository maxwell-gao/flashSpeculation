\section{Related Work}

\paragraph{Large Language Models Multi-Step Reasoning}

One of the key focus areas for LLMs is understanding and enhancing their reasoning capabilities. Recent advancements in this area focused on developing methods that improve LLMs' ability to handle complex tasks in domains like code generation and mathematical problem-solving. Chain-of-Thought (CoT) \citep{wei2023chainofthoughtpromptingelicitsreasoning} reasoning has been instrumental in helping LLMs break down intricate problems into a sequence of manageable steps, making them more adept at handling tasks that require logical reasoning. Building upon this, Tree-of-Thought (ToT) \citep{yao2023treethoughtsdeliberateproblem} reasoning extends CoT by allowing models to explore multiple reasoning paths concurrently, thereby enhancing their ability to evaluate different solutions simultaneously. Complementing these approaches, Monte Carlo Tree Search (MCTS) has emerged as a powerful reasoning method for decision-making in LLMs. Originally successful in AlphaGo's victory~\citep{silver}, MCTS has been adapted to guide model-based planning by balancing exploration and exploitation through tree-based search and random sampling, and later to large language model reasoning~\citep{hao2023reasoninglanguagemodelplanning}, showing great results. This adaptation has proven particularly effective in areas requiring strategic planning. Notable implementations like ReST-MCTS$^*$~\citep{ReST-MCTS}, rStar~\citep{mutualReasoning}, MCTSr~\citep{zhang2024accessinggpt4levelmathematical} and \citet{Boostreasoing} have shown that integrating MCTS with reinforced self-training, self-play mutual reasoning or Direct Preference Optimization~\citep{dpo} can significantly improve reasoning capabilities in LLMs. Furthermore, recent advancements such as Deepseek Prover~\citep{Xin2024DeepSeekProverAT, deepprover1.5} demonstrates the potential of these models to understand complex instructions such as formal mathematical proof.


\paragraph{Decoding Strategies}
Contrastive decoding and speculative decoding both require Smaller Language Models (SLMs), yet few have realized that these two clever decoding methods can be seamlessly combined without any additional cost. The only work that noticed this was \citet{yuan-etal-2024-speculative}, but their proposed speculative contrastive decoding focused on token-level decoding. In contrast, we designed a new action-level contrastive decoding to guide MCTS reasoning, the distinction will be discussed further in Section~\ref{eq:JS-divergence}. For more detailed related work please refer to Appendix~\ref{related_of_decoding}.


