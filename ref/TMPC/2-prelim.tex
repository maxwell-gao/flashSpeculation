\section{Background}
\label{sec:background}
In the general setup of RL for LLMs, text generation can be formally modeled as a finite-horizon Markov Decision Process (MDP). We adopt a general notion of a \textit{step} as the basic unit of temporal progression, which can represent a token, a segment at various granularities (\eg phrase, sentence, or paragraph), or other linguistically or structurally meaningful units. Then, an MDP can be defined as $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, R, \mu, T)$, 
where (i) the state space \(\mathcal{S}\) consists of all possible text prefixes, (ii) the action space $\mathcal{A}$ corresponds to the set of all possible generation units, (iii) $\mathcal{P}$ denotes the transition function, (iv) $R:\mathcal{S}\times\mathcal{A}\rightarrow \mathbb{R}$ is the reward function that assigns scalar feedback to step-level or trajectory-level outcomes (\eg measuring fluency, factuality, or alignment with user preferences), (v) $\mu$ denotes the initial state distribution, and (vi) $T\in\mathbb{N}$ is the episode length.

We define the initial state $s_0$ as the initial prompt and let $a_t$ denote the partial response generated at step $t$. At each step $t$, the current state is the set of tokens from the initial prompt and the partial responses generated up to step $t$, \ie $s_t = (s_0,a_1, \cdots, a_{t-1})$.
Based on this construction, we know that the transition function is deterministic with $\mathcal{P}(s_{t+1} \rvert s_t, a_t) = 1$. A policy $\pi_\theta(a \rvert s)$, parameterized by the language model, defines a probability distribution over actions given the prefix $s\in\mathcal{S}$. 
The generation of a full text sequence of length $T$ can therefore be viewed as a trajectory $\tau=(s_0,a_0,\cdots,s_{T-1},a_{T-1},s_T)$ with the cumulative reward given by $\mathcal{J}(\tau):=\sum_{t=0}^{T-1} R(s_t, a_t)$. %

This perspective enables the application of RL methods to text generation in LLMs. Rather than relying solely on maximum likelihood estimation, which optimizes local token-level likelihoods, the MDP formulation allows optimization with respect to long-horizon objectives such as coherence and alignment with human preferences. This provides the foundation for recent advances in preference-based fine-tuning and test-time alignment.
