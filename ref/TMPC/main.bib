@article{cagienard2007move,
  title={Move blocking strategies in receding horizon control},
  author={Cagienard, Raphael and Grieder, Pascal and Kerrigan, Eric C and Morari, Manfred},
  journal={Journal of Process Control},
  volume={17},
  number={6},
  pages={563--570},
  year={2007},
  publisher={Elsevier},
  url={https://www.sciencedirect.com/science/article/pii/S0959152407000030}
}% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").




@article{mbpp,
  author       = {Jacob Austin and
                  Augustus Odena and
                  Maxwell I. Nye and
                  Maarten Bosma and
                  Henryk Michalewski and
                  David Dohan and
                  Ellen Jiang and
                  Carrie J. Cai and
                  Michael Terry and
                  Quoc V. Le and
                  Charles Sutton},
  title        = {Program Synthesis with Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2108.07732},
  year         = {2021},
  url          = {https://arxiv.org/abs/2108.07732},
  eprinttype    = {arXiv},
  eprint       = {2108.07732},
  timestamp    = {Fri, 29 Apr 2022 17:42:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2108-07732.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{grm,
  author       = {Rui Yang and
                  Ruomeng Ding and
                  Yong Lin and
                  Huan Zhang and
                  Tong Zhang},
  title        = {Regularizing Hidden States Enables Learning Generalizable Reward Model
                  for LLMs},
  booktitle    = {NeurIPS},
  year         = {2024}
}

@article{horizon,
  author       = {Seohong Park and
                  Kevin Frans and
                  Deepinder Mann and
                  Benjamin Eysenbach and
                  Aviral Kumar and
                  Sergey Levine},
  title        = {Horizon Reduction Makes {RL} Scalable},
  journal      = {CoRR},
  volume       = {abs/2506.04168},
  year         = {2025}
}


@article{tpo,
  author       = {Yafu Li and
                  Xuyang Hu and
                  Xiaoye Qu and
                  Linjie Li and
                  Yu Cheng},
  title        = {Test-Time Preference Optimization: On-the-Fly Alignment via Iterative
                  Textual Feedback},
  journal      = {CoRR},
  volume       = {abs/2501.12895},
  year         = {2025}
}


@inproceedings{ouyang2022training,
  title={{Training Language Models to Follow Instructions with Human Feedback}},
  author={Long Ouyang and Jeffrey Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul F. Christiano and Jan Leike and Ryan Lowe},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022},
url={https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf}
}

@inproceedings{
rafailov2023direct,
title={{Direct Preference Optimization: Your Language Model is Secretly a Reward Model}},
author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Christopher D Manning and Stefano Ermon and Chelsea Finn},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=HPuSIXJaa9}
}

@inproceedings{gao2021simcse,
  title={SimCSE: Simple Contrastive Learning of Sentence Embeddings},
  author={Tianyu Gao and Xingcheng Yao and Danqi Chen},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={6894--6910},
  publisher={Association for Computational Linguistics},
  year={2021}
}

@inproceedings{wang2025segale,
      title={Extending Automatic Machine Translation Evaluation to Book-Length Documents}, 
      author={Kuang-Da Wang and Shuoyang Ding and Chao-Han Huck Yang and Ping-Chun Hsieh and Wen-Chih Peng and Vitaly Lavrukhin and Boris Ginsburg},
      year={2025},
      month = nov,
      url={https://arxiv.org/abs/2509.17249}, 
      booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
}

@misc{openai2024gpt4technicalreport,
  title={GPT-4 Technical Report},
  author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
  year={2024},
  eprint={2303.08774},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2303.08774}
}



@inproceedings{xue-etal-2021-mt5,
    title = "{m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer}",
    author = "Xue, Linting  and
      Constant, Noah  and
      Roberts, Adam  and
      Kale, Mihir  and
      Al-Rfou, Rami  and
      Siddhant, Aditya  and
      Barua, Aditya  and
      Raffel, Colin",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.41/",
    doi = "10.18653/v1/2021.naacl-main.41",
    pages = "483--498",
    abstract = "The recent {\textquotedblleft}Text-to-Text Transfer Transformer{\textquotedblright} (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent {\textquotedblleft}accidental translation{\textquotedblright} in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available."
}

@article{M2M-100,
  author       = {Angela Fan and
                  Shruti Bhosale and
                  Holger Schwenk and
                  Zhiyi Ma and
                  Ahmed El{-}Kishky and
                  Siddharth Goyal and
                  Mandeep Baines and
                  Onur Celebi and
                  Guillaume Wenzek and
                  Vishrav Chaudhary and
                  Naman Goyal and
                  Tom Birch and
                  Vitaliy Liptchinsky and
                  Sergey Edunov and
                  Michael Auli and
                  Armand Joulin},
  title        = {{Beyond English-Centric Multilingual Machine Translation}},
  journal      = {J. Mach. Learn. Res.},
  year         = {2021},
    url          = {https://ahelk.github.io/papers/fan_beyond.pdf}
}

@inproceedings{wu-etal-2024-word,
    title = "{Word Alignment as Preference for Machine Translation}",
    author = "Wu, Qiyu  and
      Nagata, Masaaki  and
      Miao, Zhongtao  and
      Tsuruoka, Yoshimasa",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.188/",
    doi = "10.18653/v1/2024.emnlp-main.188",
    pages = "3223--3239",
    abstract = "The problem of hallucination and omission, a long-standing problem in machine translation (MT), is more pronounced when a large language model (LLM) is used in MT because an LLM itself is susceptible to these phenomena. In this work, we mitigate the problem in an LLM-based MT model by guiding it to better word alignment. We first study the correlation between word alignment and the phenomena of hallucination and omission in MT. Then we propose to utilize word alignment as preference to optimize the LLM-based MT model. The preference data are constructed by selecting chosen and rejected translations from multiple MT tools. Subsequently, direct preference optimization is used to optimize the LLM-based model towards the preference signal. Given the absence of evaluators specifically designed for hallucination and omission in MT, we further propose selecting hard instances and utilizing GPT-4 to directly evaluate the performance of the models in mitigating these issues. We verify the rationality of these designed evaluation methods by experiments, followed by extensive results demonstrating the effectiveness of word alignment-based preference optimization to mitigate hallucination and omission. On the other hand, although it shows promise in mitigating hallucination and omission, the overall performance of MT in different language directions remains mixed, with slight increases in BLEU and decreases in COMET."
}

@inproceedings{zheng2024llamafactory,
  title={{LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models}},
  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},
  year={2024},
    url={https://aclanthology.org/2024.acl-demos.38.pdf}
}

@inproceedings{
khanov2024args,
title={{{ARGS}: Alignment as Reward-Guided Search}},
author={Maxim Khanov and Jirayu Burapacheep and Yixuan Li},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=shgx0eqdw6}
}

@inproceedings{
li2024rain,
title={{{RAIN}: Your Language Models Can Align Themselves without Finetuning}},
author={Yuhui Li and Fangyun Wei and Jinjing Zhao and Chao Zhang and Hongyang Zhang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=pETSfWMUzy}
}

@inproceedings{
meng2024simpo,
title={{Sim{PO}: Simple Preference Optimization With a Reference-Free Reward}},
author={Yu Meng and Mengzhou Xia and Danqi Chen},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=3Tzcot1LKb}
}


@inproceedings{juraska-etal-2024-metricx,
    title = "{{M}etric{X}-24: The {G}oogle Submission to the {WMT} 2024 Metrics Shared Task}",
    author = "Juraska, Juraj  and
      Deutsch, Daniel  and
      Finkelstein, Mara  and
      Freitag, Markus",
    editor = "Haddow, Barry  and
      Kocmi, Tom  and
      Koehn, Philipp  and
      Monz, Christof",
    booktitle = "Proceedings of the Ninth Conference on Machine Translation",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.wmt-1.35/",
    doi = "10.18653/v1/2024.wmt-1.35",
    pages = "492--504",
    abstract = "In this paper, we present the MetricX-24 submissions to the WMT24 Metrics Shared Task and provide details on the improvements we made over the previous version of MetricX. Our primary submission is a hybrid reference-based/-free metric, which can score a translation irrespective of whether it is given the source segment, the reference, or both. The metric is trained on previous WMT data in a two-stage fashion, first on the DA ratings only, then on a mixture of MQM and DA ratings. The training set in both stages is augmented with synthetic examples that we created to make the metric more robust to several common failure modes, such as fluent but unrelated translation, or undertranslation. We demonstrate the benefits of the individual modifications via an ablation study, and show a significant performance increase over MetricX-23 on the WMT23 MQM ratings, as well as our new synthetic challenge set."
}

@inproceedings{hu-etal-2024-gentranslate,
    title = "{{G}en{T}ranslate: Large Language Models are Generative Multilingual Speech and Machine Translators}",
    author = "Hu, Yuchen  and
      Chen, Chen  and
      Yang, Chao-Han  and
      Li, Ruizhe  and
      Zhang, Dong  and
      Chen, Zhehuai  and
      Chng, EngSiong",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.5/",
    doi = "10.18653/v1/2024.acl-long.5",
    pages = "74--90",
    abstract = "Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse N-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely GenTranslate, which builds upon LLMs to generate better results from the diverse translation versions in N-best list. Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the diverse N-best candidates to generate a higher-quality translation result. Furthermore, to support LLM finetuning, we build and release a HypoTranslate dataset that contains over 592K hypotheses-translation pairs in 11 languages. Experiments on various speech and machine translation benchmarks (e.g., FLEURS, CoVoST-2, WMT) demonstrate that our GenTranslate significantly outperforms the state-of-the-art model."
}

@inproceedings{
loshchilov2019decoupled,
title={{Decoupled Weight Decay Regularization}},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@article{achiam2023gpt,
  title={{GPT-4 Technical Report}},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023},
url={https://arxiv.org/pdf/2303.08774}
}

@article{team2024gemma,
  title={{Gemma 2: Improving Open Language Models at a Practical Size}},
  author={Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024},
url={https://arxiv.org/pdf/2408.00118}
}

@article{touvron2023llama1,
  title={{LLaMA: Open and Efficient Foundation Language Models}},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023},
url={https://arxiv.org/pdf/2302.13971}
}

@article{touvron2023llama,
  title={{Llama 2: Open Foundation and Fine-tuned Chat Models}},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023},
url={https://arxiv.org/pdf/2307.09288}
}

@article{brown2020language,
  title={{Language Models are Few-shot Learners}},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020},
url={https://papers.nips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}
}

@inproceedings{
hu2022lora,
title={{Lo{RA}: Low-Rank Adaptation of Large Language Models}},
author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@article{ziegler2019finetuning,
  title={{Fine-Tuning Language Models from Human Preferences}},
  author={Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019},
url={https://arxiv.org/pdf/1909.08593}
}

@inproceedings{wang-etal-2024-inferaligner,
    title = "{{I}nfer{A}ligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance}",
    author = "Wang, Pengyu  and
      Zhang, Dong  and
      Li, Linyang  and
      Tan, Chenkun  and
      Wang, Xinghao  and
      Zhang, Mozhi  and
      Ren, Ke  and
      Jiang, Botian  and
      Qiu, Xipeng",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.585/",
    doi = "10.18653/v1/2024.emnlp-main.585",
    pages = "10460--10479",
    abstract = "As large language models (LLMs) rapidly evolve, they are increasingly being customized through fine-tuning to suit the specific needs of various applications. A critical aspect of this advancement is the alignment process, which ensures that these models perform tasks in ways that align with human values and expectations. Current alignment methods, such as direct preference optimization (DPO) and reinforcement learning from human feedback (RLHF), focus primarily on alignment during training phase. However, these methods often involve complex and resource-intensive training processes, posing significant challenge for their implementation. Therefore, we propose \textbf{InferAligner}, a simple yet effective method for harmlessness alignment during inference phase. InferAligner decouples harmlessness from helpfulness. During the training phase, it focuses solely on enhancing the target model`s capabilities on downstream tasks. In the inference phase, it utilizes safety steering vectors extracted from the aligned model to guide the target model towards harmlessness alignment. Experimental results show that our method can be very effectively applied to domain-specific models in finance, medicine, and mathematics, as well as to multimodal large language models (MLLMs) such as LLaVA. It significantly diminishes the attack success rate (ASR) of both harmful instructions and jailbreak instructions, while maintaining almost unchanged performance in downstream tasks."
}

@inproceedings{lin2024unlocking,
title={{The Unlocking Spell on Base {LLM}s:  Rethinking Alignment via In-Context Learning}},
author={Bill Yuchen Lin and Abhilasha Ravichander and Ximing Lu and Nouha Dziri and Melanie Sclar and Khyathi Chandu and Chandra Bhagavatula and Yejin Choi},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=wxJ0eXwwda}
}

@article{MPPI,
title        = {Model Predictive Path Integral Control using Covariance Variable Importance
                  Sampling},  
author       = {Grady Williams and
                  Andrew Aldrich and
                  Evangelos A. Theodorou},
  journal      = {CoRR},
  year         = {2015},
  url          = {http://arxiv.org/abs/1509.01149},
}

@inproceedings{re-control,
  title        = {Aligning Large Language Models with Representation Editing: {A} Control
                  Perspective},  
author       = {Lingkai Kong and
                  Haorui Wang and
                  Wenhao Mu and
                  Yuanqi Du and
                  Yuchen Zhuang and
                  Yifei Zhou and
                  Yue Song and
                  Rongzhi Zhang and
                  Kai Wang and
                  Chao Zhang},

  booktitle    = {NeurIPS},
  year         = {2024},
url          = {http://papers.nips.cc/paper\_files/paper/2024/hash/41bba7b0f5c81e789a20bb16a370aeeb-Abstract-Conference.html}
}

@article{qiu2024treebon,
  title={{TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling}},
  author={Qiu, Jiahao and Lu, Yifu and Zeng, Yifan and Guo, Jiacheng and Geng, Jiayi and Wang, Huazheng and Huang, Kaixuan and Wu, Yue and Wang, Mengdi},
  journal={Computing Research Repository},
  year={2024},
url={https://arxiv.org/pdf/2410.16033}
}



@inproceedings{GenARM,
  title        = {GenARM: Reward Guided Generation with Autoregressive Reward Model
                  for Test-Time Alignment},
  author       = {Yuancheng Xu and
                  Udari Madhushani Sehwag and
                  Alec Koppel and
                  Sicheng Zhu and
                  Bang An and
                  Furong Huang and
                  Sumitra Ganesh},

  booktitle    = {The Thirteenth International Conference on Learning Representations,
                  {ICLR} 2025, Singapore, April 24-28, 2025},
  year         = {2025},
  url          = {https://openreview.net/forum?id=J0qTpmbSbh},
}


@inproceedings{freitag-etal-2024-llms,
    title = "Are {LLM}s Breaking {MT} Metrics? Results of the {WMT}24 Metrics Shared Task",
    author = "Freitag, Markus  and
      Mathur, Nitika  and
      Deutsch, Daniel  and
      Lo, Chi-Kiu  and
      Avramidis, Eleftherios  and
      Rei, Ricardo  and
      Thompson, Brian  and
      Blain, Frederic  and
      Kocmi, Tom  and
      Wang, Jiayi  and
      Adelani, David Ifeoluwa  and
      Buchicchio, Marianna  and
      Zerva, Chrysoula  and
      Lavie, Alon",
    booktitle = "Proceedings of the Ninth Conference on Machine Translation",
    year = "2024",
    url = "https://aclanthology.org/2024.wmt-1.2/",
}

@article{shahriar2024putting,
  title={{Putting GPT-4o to the Sword: A Comprehensive Evaluation of Language, Vision, Speech, and Multimodal Proficiency}},
  author={Sakib Shahriar and Brady D. Lund and Nishith Reddy Mannuru and Muhammad Arbab Arshad and Kadhim Hayawi and Ravi Varma Kumar Bevara and Aashrith Mannuru and Laiba Batool},
  journal={Computing Research Repository},
  volume={abs/2407.09519},
  year={2024},
url={https://arxiv.org/pdf/2407.09519}
}

@inproceedings{wang2024findings,
    title = "{Findings of the {WMT} 2024 Shared Task on Discourse-Level Literary Translation}",
    author = "Wang, Longyue  and
      Liu, Siyou  and
      Lyu, Chenyang  and
      Jiao, Wenxiang  and
      Wang, Xing  and
      Xu, Jiahao  and
      Tu, Zhaopeng  and
      Gu, Yan  and
      Chen, Weiyu  and
      Wu, Minghao  and
      Zhou, Liting  and
      Koehn, Philipp  and
      Way, Andy  and
      Yuan, Yulin",
    editor = "Haddow, Barry  and
      Kocmi, Tom  and
      Koehn, Philipp  and
      Monz, Christof",
    booktitle = "Proceedings of the Ninth Conference on Machine Translation",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.wmt-1.58/",
    doi = "10.18653/v1/2024.wmt-1.58",
    pages = "699--700",
    abstract = "Translating literary works has perennially stood as an elusive dream in machine translation (MT), a journey steeped in intricate challenges. To foster progress in this domain, we hold a new shared task at WMT 2023, the second edition of the \textit{Discourse-Level Literary Translation}. First, we (Tencent AI Lab and China Literature Ltd.) release a copyrighted and document-level Chinese-English web novel corpus. Furthermore, we put forth an industry-endorsed criteria to guide human evaluation process. This year, we totally received 10 submissions from 5 academia and industry teams. We employ both automatic and human evaluations to measure the performance of the submitted systems. The official ranking of the systems is based on the overall human judgments. In addition, our extensive analysis reveals a series of interesting findings on literary and discourse-aware MT. We release data, system outputs, and leaderboard at \url{https://www2.statmt.org/wmt24/literary-translation-task.html}."
}


@inproceedings{fernandes-etal-2023-translation,
    title = "{{When Does Translation Require Context? A Data-driven, Multilingual Exploration}}",
    author = "Fernandes, Patrick  and
      Yin, Kayo  and
      Liu, Emmy  and
      Martins, Andr{\'e}  and
      Neubig, Graham",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.36/",
    doi = "10.18653/v1/2023.acl-long.36",
    pages = "606--626",
    abstract = "Although proper handling of discourse significantly contributes to the quality of machine translation (MT), these improvements are not adequately measured in common translation quality metrics. Recent works in context-aware MT attempt to target a small set of discourse phenomena during evaluation, however not in a fully systematic way. In this paper, we develop the Multilingual Discourse-Aware (MuDA) benchmark, a series of taggers that identify and evaluate model performance on discourse phenomena in any given dataset. The choice of phenomena is inspired by a novel methodology to systematically identify translations that require context. This methodology confirms the difficulty of previously studied phenomena while uncovering others which were not previously addressed. We find that commonly studied context-aware MT models make only marginal improvements over context-agnostic models, which suggests these models do not handle these ambiguities effectively. We release code and data for 14 language pairs to encourage the MT community to focus on accurately capturing discourse phenomena. Code available at \url{https://github.com/neulab/contextual-mt}"
}

@inproceedings{wu2023document,
    title = {{Document Flattening: Beyond Concatenating Context for Document-Level Neural Machine Translation}},
    author = "Wu, Minghao  and
      Foster, George  and
      Qu, Lizhen  and
      Haffari, Gholamreza",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.33/",
    doi = "10.18653/v1/2023.eacl-main.33",
    pages = "448--462",
    abstract = "Existing work in document-level neural machine translation commonly concatenates several consecutive sentences as a pseudo-document, and then learns inter-sentential dependencies. This strategy limits the model`s ability to leverage information from distant context. We overcome this limitation with a novel Document Flattening (DocFlat) technique that integrates Flat-Batch Attention (FBA) and Neural Context Gate (NCG) into Transformer model to utilizes information beyond the pseudo-document boundaries. FBA allows the model to attend to all the positions in the batch and model the relationships between positions explicitly and NCG identifies the useful information from the distant context. We conduct comprehensive experiments and analyses on three benchmark datasets for English-German translation, and validate the effectiveness of two variants of DocFlat. Empirical results show that our approach outperforms strong baselines with statistical significance on BLEU, COMET and accuracy on the contrastive test set. The analyses highlight that DocFlat is highly effective in capturing the long-range information."
}
@inproceedings{voita2018context,
    title = "{Context-Aware Neural Machine Translation Learns Anaphora Resolution}",
    author = "Voita, Elena  and
      Serdyukov, Pavel  and
      Sennrich, Rico  and
      Titov, Ivan",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1117/",
    doi = "10.18653/v1/P18-1117",
    pages = "1264--1274",
    abstract = "Standard machine translation systems process sentences in isolation and hence ignore extra-sentential information, even though extended context can both prevent mistakes in ambiguous cases and improve translation coherence. We introduce a context-aware neural machine translation model designed in such way that the flow of information from the extended context to the translation model can be controlled and analyzed. We experiment with an English-Russian subtitles dataset, and observe that much of what is captured by our model deals with improving pronoun translation. We measure correspondences between induced attention distributions and coreference relations and observe that the model implicitly captures anaphora. It is consistent with gains for sentences where pronouns need to be gendered in translation. Beside improvements in anaphoric cases, the model also improves in overall BLEU, both over its context-agnostic version (+0.7) and over simple concatenation of the context and source sentences (+0.6)."
}

@article{maruf2022survey,
  title={{A Survey on Document-level Neural Machine Translation: Methods and Evaluation}},
  author={Sameen Maruf and Fahimeh Saleh and Gholamreza Haffari},
  journal={ACM Computing Surveys},
  volume={54},
  number={2},
  pages={45:1--45:36},
  year={2022},
url={https://dl.acm.org/doi/10.1145/3441691}
}
@inproceedings{
wang2023documentlevel,
title={{Document-Level Machine Translation with Large Language Models}},
author={Longyue Wang and Chenyang Lyu and Tianbo Ji and Zhirui Zhang and Dian Yu and Shuming Shi and Zhaopeng Tu},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
url={https://openreview.net/forum?id=sXErPfdA7Q}
}

%%% Citations for trajectory optimization
@article{chua2018deep,
  title={{Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models}},
  author={Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018},
url={https://proceedings.neurips.cc/paper_files/paper/2018/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf}
}
@inproceedings{
lowrey2019plan,
title={{Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control}},
author={Kendall Lowrey and Aravind Rajeswaran and Sham Kakade and Emanuel Todorov and Igor Mordatch},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Byey7n05FQ},
}
@inproceedings{hansen2022temporal,
  title={{Temporal Difference Learning for Model Predictive Control}},
  author={Hansen, Nicklas A and Su, Hao and Wang, Xiaolong},
  booktitle={International Conference on Machine Learning},
  pages={8387--8406},
  year={2022},
url={https://proceedings.mlr.press/v162/hansen22a/hansen22a.pdf}
}

@inproceedings{
alves2024tower,
title={{Tower: An Open Multilingual Large Language Model for Translation-Related Tasks}},
author={Duarte Miguel Alves and Jos{\'e} Pombal and Nuno M Guerreiro and Pedro Henrique Martins and Jo{\~a}o Alves and Amin Farajian and Ben Peters and Ricardo Rei and Patrick Fernandes and Sweta Agrawal and Pierre Colombo and Jos{\'e} G. C. de Souza and Andre Martins},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=EHPns3hVkj}
}

@inproceedings{
    xu2024a,
    title={{A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models}},
    author={Haoran Xu and Young Jin Kim and Amr Sharaf and Hany Hassan Awadalla},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=farT6XXntP}
}

@inproceedings{
    xu2024contrastive,
    title={{Contrastive Preference Optimization: Pushing the Boundaries of {LLM} Performance in Machine Translation}},
    author={Haoran Xu and Amr Sharaf and Yunmo Chen and Weiting Tan and Lingfeng Shen and Benjamin Van Durme and Kenton Murray and Young Jin Kim},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024},
    url={https://openreview.net/forum?id=51iwkioZpn}
}

@inproceedings{post-junczys-dowmunt-2024-evaluation,
    title = {{Evaluation and Large-Scale Training for Contextual Machine Translation}},
    author = "Post, Matt  and
      Junczys-Dowmunt, Marcin",
    editor = "Haddow, Barry  and
      Kocmi, Tom  and
      Koehn, Philipp  and
      Monz, Christof",
    booktitle = "Proceedings of the Ninth Conference on Machine Translation",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.wmt-1.112/",
    doi = "10.18653/v1/2024.wmt-1.112",
    pages = "1125--1139",
    abstract = "Despite the fact that context is known to be vital for resolving a range of translation ambiguities, most traditional machine translation systems continue to be trained and to operate at the sentence level. A common explanation is the lack of document-level annotations for existing training data. This work investigates whether having such annotations would be helpful for training traditional MT systems at scale. We build large-scale, state-of-the-art contextual MT systems into German, French, and Russian, fixing the datasets while comparing the effect of sourcing contextual training samples from both parallel and back-translated data. We then evaluate these contextual models across a range of contextual test sets from the literature, where we find that (a) document annotations from both mined parallel and back-translated monolingual data are helpful, but that the best contextual MT systems do not draw contextual samples from the parallel data. We also make two points related to evaluation: (b) contrastive score-based metrics on challenge sets are not discriminative; instead, models must be tested directly on their ability to generate correct outputs, and (c) standard corpus-level metrics such as COMET work best in settings that are dense in contextual phenomena."
}
@inproceedings{deutsch-etal-2023-training,
    title = {{Training and Meta-Evaluating Machine Translation Evaluation Metrics at the Paragraph Level}},
    author = "Deutsch, Daniel  and
      Juraska, Juraj  and
      Finkelstein, Mara  and
      Freitag, Markus",
    editor = "Koehn, Philipp  and
      Haddow, Barry  and
      Kocmi, Tom  and
      Monz, Christof",
    booktitle = "Proceedings of the Eighth Conference on Machine Translation",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.wmt-1.96/",
    doi = "10.18653/v1/2023.wmt-1.96",
    pages = "996--1013"
}
@inproceedings{vernikos-etal-2022-embarrassingly,
    title = {{Embarrassingly Easy Document-Level {MT} Metrics: How to Convert Any Pretrained Metric into a Document-Level Metric}},
    author = "Vernikos, Giorgos  and
      Thompson, Brian  and
      Mathur, Prashant  and
      Federico, Marcello",
    editor = {Koehn, Philipp  and
      Barrault, Lo{\"i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Jimeno Yepes, Antonio  and
      Kocmi, Tom  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Popel, Martin  and
      Turchi, Marco  and
      Zampieri, Marcos},
    booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wmt-1.6/",
    pages = "118--128"
}

@inproceedings{ahmad-etal-2024-findings,
    title = {{Findings of the IWSLT 2024 Evaluation Campaign}},
    author = {Ahmad, Ibrahim Said  and
      Anastasopoulos, Antonios  and
      Bojar, Ond{\v{r}}ej  and
      Borg, Claudia  and
      Carpuat, Marine  and
      Cattoni, Roldano  and
      Cettolo, Mauro  and
      Chen, William  and
      Dong, Qianqian  and
      Federico, Marcello  and
      Haddow, Barry  and
      Javorsk{\'y}, D{\'a}vid  and
      Krubi{\'n}ski, Mateusz  and
      Lam, Tsz Kin  and
      Ma, Xutai  and
      Mathur, Prashant  and
      Matusov, Evgeny  and
      Maurya, Chandresh  and
      McCrae, John  and
      Murray, Kenton  and
      Nakamura, Satoshi  and
      Negri, Matteo  and
      Niehues, Jan  and
      Niu, Xing  and
      Ojha, Atul Kr.  and
      Ortega, John  and
      Papi, Sara  and
      Pol{\'a}k, Peter  and
      Posp{\'i}{\v{s}}il, Adam  and
      Pecina, Pavel  and
      Salesky, Elizabeth  and
      Sethiya, Nivedita  and
      Sarkar, Balaram  and
      Shi, Jiatong  and
      Sikasote, Claytone  and
      Sperber, Matthias  and
      St{\"u}ker, Sebastian  and
      Sudoh, Katsuhito  and
      Thompson, Brian  and
      Waibel, Alex  and
      Watanabe, Shinji  and
      Wilken, Patrick  and
      Zem{\'a}nek, Petr  and
      Zevallos, Rodolfo},
    editor = "Salesky, Elizabeth  and
      Federico, Marcello  and
      Carpuat, Marine",
    booktitle = "Proceedings of the 21st International Conference on Spoken Language Translation (IWSLT 2024)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand (in-person and online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.iwslt-1.1/",
    doi = "10.18653/v1/2024.iwslt-1.1",
    pages = "1--11"
}

@inproceedings{kocmi-etal-2024-findings,
    title = "Findings of the {WMT}24 General Machine Translation Shared Task: The {LLM} Era Is Here but {MT} Is Not Solved Yet",
    author = "Kocmi, Tom  and
      Avramidis, Eleftherios  and
      Bawden, Rachel  and
      Bojar, Ond{\v{r}}ej  and
      Dvorkovich, Anton  and
      Federmann, Christian  and
      Fishel, Mark  and
      Freitag, Markus  and
      Gowda, Thamme  and
      Grundkiewicz, Roman  and
      Haddow, Barry  and
      Karpinska, Marzena  and
      Koehn, Philipp  and
      Marie, Benjamin  and
      Monz, Christof  and
      Murray, Kenton  and
      Nagata, Masaaki  and
      Popel, Martin  and
      Popovi{\'c}, Maja  and
      Shmatova, Mariya  and
      Steingr{\'i}msson, Steinth{\'o}r  and
      Zouhar, Vil{\'e}m",
    editor = "Haddow, Barry  and
      Kocmi, Tom  and
      Koehn, Philipp  and
      Monz, Christof",
    booktitle = "Proceedings of the Ninth Conference on Machine Translation",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.wmt-1.1/",
    doi = "10.18653/v1/2024.wmt-1.1",
    pages = "1--46",
    abstract = "This overview paper presents the results of the General Machine Translation Task organised as part of the 2024 Conference on Machine Translation (WMT). In the general MT task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting of three to five different domains. In addition to participating systems, we collected translations from 8 different large language models (LLMs) and 4 online translation providers. We evaluate system outputs with professional human annotators using a new protocol called Error Span Annotations (ESA)."
}

%%%
%%%

@article{stiennon2020learning,
  title={{Learning to Summarize with Human Feedback}},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020},
url={https://proceedings.neurips.cc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf}
}
@inproceedings{
hendrycks2021measuring,
title={{Measuring Massive Multitask Language Understanding}},
author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=d7KBjmI3GmQ}
}


@article{
srivastava2023beyond,
title={{Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models}},
author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adri{\`a} Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and Amanda Dsouza and Ambrose Slone and Ameet Rahane and Anantharaman S. Iyer and Anders Johan Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlm{\"u}ller and Andrew M. Dai and Andrew La and Andrew Kyle Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and others},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=uyTL5Bvosj},
note={Featured Certification}
}

@inproceedings{
yu2024kola,
title={{Ko{LA}: Carefully Benchmarking World Knowledge of Large Language Models}},
author={Jifan Yu and Xiaozhi Wang and Shangqing Tu and Shulin Cao and Daniel Zhang-Li and Xin Lv and Hao Peng and Zijun Yao and Xiaohan Zhang and Hanming Li and Chunyang Li and Zheyuan Zhang and Yushi Bai and Yantao Liu and Amy Xin and Kaifeng Yun and Linlu GONG and Nianyi Lin and Jianhui Chen and Zhili Wu and Yunjia Qi and Weikai Li and Yong Guan and Kaisheng Zeng and Ji Qi and Hailong Jin and Jinxin Liu and Yu Gu and Yuan Yao and Ning Ding and Lei Hou and Zhiyuan Liu and Xu Bin and Jie Tang and Juanzi Li},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=AqN23oqraW}
}

@inproceedings{zhong-etal-2024-agieval,
    title = "{{AGIE}val: A Human-Centric Benchmark for Evaluating Foundation Models}",
    author = "Zhong, Wanjun  and
      Cui, Ruixiang  and
      Guo, Yiduo  and
      Liang, Yaobo  and
      Lu, Shuai  and
      Wang, Yanlin  and
      Saied, Amin  and
      Chen, Weizhu  and
      Duan, Nan",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.149/",
    doi = "10.18653/v1/2024.findings-naacl.149",
    pages = "2299--2314",
    abstract = "Assessing foundation models' abilities for human-level tasks is crucial for Artificial General Intelligence (AGI) development.Traditional benchmarks, which rely on artificial datasets, may not accurately represent these capabilities. In this paper, we introduce AGIEval, a novel bilingual benchmark designed to assess foundation models in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models on our benchmark. Impressively, we show that GPT-4 exceeds the average human performance in SAT, LSAT, and math contests, with 95{\%} accuracy on SAT Math and 92.5{\%} on the Chinese college entrance English exam. This demonstrates the exceptional performance of contemporary foundation models. In contrast, we also find that GPT-4 is less proficient in tasks requiring complex reasoning or specific domain knowledge. Our comprehensive analyses of model capabilities (understanding, knowledge, reasoning, and calculation) reveal their strengths and limitations, providing valuable insights into future directions for enhancing general capabilities. By concentrating on tasks pertinent to human cognition and decision-making, our benchmark delivers a meaningful and robust evaluation of foundation models' performance in real-world scenarios."
}

@misc{openai2024gpt4o,
  author    = {OpenAI},
  title     = {{Hello GPT-4o (API version 4o-0211)}},
  year      = {2024},
  url       = {https://openai.com/index/hello-gpt-4o/},
  note      = {Accessed: 2025-02-14}
}

@inproceedings{mandya-etal-2020-history,
    title = "{{Do Not Let the History Haunt You: Mitigating Compounding Errors in Conversational Question Answering}}",
    author = "Mandya, Angrosh  and
      O{'} Neill, James  and
      Bollegala, Danushka  and
      Coenen, Frans",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.248/",
    pages = "2017--2025",
    language = "eng",
    ISBN = "979-10-95546-34-4",
    abstract = "The Conversational Question Answering (CoQA) task involves answering a sequence of inter-related conversational questions about a contextual paragraph. Although existing approaches employ human-written ground-truth answers for answering conversational questions at test time, in a realistic scenario, the CoQA model will not have any access to ground-truth answers for the previous questions, compelling the model to rely upon its own previously predicted answers for answering the subsequent questions. In this paper, we find that compounding errors occur when using previously predicted answers at test time, significantly lowering the performance of CoQA systems. To solve this problem, we propose a sampling strategy that dynamically selects between target answers and model predictions during training, thereby closely simulating the situation at test time. Further, we analyse the severity of this phenomena as a function of the question type, conversation length and domain type."
}

@article{hadj2019improving,
  title={{Improving Arabic Neural Machine Translation via N-best List Re-ranking}},
  author={Hadj Ameur, Mohamed Seghir and Guessoum, Ahmed and Meziane, Farid},
  journal={machine translation},
  volume={33},
  pages={279--314},
  year={2019},
  publisher={Springer},
url={https://doi.org/10.1007/s10590-019-09237-6}
}

@article{feng2024tear,
  title={{Tear: Improving LLM-based Machine Translation with Systematic Self-refinement}},
  author={Feng, Zhaopeng and Zhang, Yan and Li, Hao and Wu, Bei and Liao, Jiayu and Liu, Wenqiang and Lang, Jun and Feng, Yang and Wu, Jian and Liu, Zuozhu},
  journal={Preprint},
  year={2024},
url={https://arxiv.org/abs/2402.16379}
}

@misc{honnibal2020spacy,
  title={{spaCy: Industrial-strength Natural Language Processing in Python}},
  author={Matthew Honnibal and Ines Montani and Sofie Van Landeghem and Adriane Boyd},
  doi={10.5281/zenodo.1212303},
  year={2020},
url       = {https://spacy.io}
}


@inproceedings{thompson-koehn-2020-exploiting,
    title = "{{Exploiting Sentence Order in Document Alignment}}",
    author = "Thompson, Brian  and
      Koehn, Philipp",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.483/",
    doi = "10.18653/v1/2020.emnlp-main.483",
    pages = "5997--6007",
    abstract = "We present a simple document alignment method that incorporates sentence order information in both candidate generation and candidate re-scoring. Our method results in 61{\%} relative reduction in error compared to the best previously published result on the WMT16 document alignment shared task. Our method improves downstream MT performance on web-scraped Sinhala{--}English documents from ParaCrawl, outperforming the document alignment method used in the most recent ParaCrawl release. It also outperforms a comparable corpora method which uses the same multilingual embeddings, demonstrating that exploiting sentence order is beneficial even if the end goal is sentence-level bitext."
}

@Inbook{Rao2014,
author="Rao, Anil V.",
title="{{Trajectory Optimization: A Survey}}",
bookTitle="Optimization and Optimal Control in Automotive Systems",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="3--21",
abstract="A survey of numerical methods for trajectory optimization. The goal of this survey is to describe typical methods that have been developed over the years for optimal trajectory generation. In addition, this survey describes modern software tools that have been developed for solving trajectory optimization problems. Finally, a discussion is given on how to choose a method.",
isbn="978-3-319-05371-4",
doi="10.1007/978-3-319-05371-4_1",
url="https://doi.org/10.1007/978-3-319-05371-4_1"
}

@book{camacho2007constrained,
  title={{Constrained Model Predictive Control}},
  author={Camacho, Eduardo F and Bordons, Carlos and Camacho, Eduardo F and Bordons, Carlos},
  year={2007},
  publisher={Springer},
url={https://link.springer.com/chapter/10.1007/978-0-85729-398-5_7}
}

@Inbook{Camacho2007,
author="Camacho, E. F. and Bordons, C.",
title="{{Constrained Model Predictive Control}}",
bookTitle="Model Predictive Control",
year="2007",
publisher="Springer London",
address="London",
pages="177--216",
abstract="The control problem was formulated in the previous chapters considering all signals to possess an unlimited range. This is not very realistic because in practice all processes are subject to constraints. Actuators have a limited range of action and a limited slew rate, as is the case of control valves limited by a fully closed and fully open position and a maximum slew rate. Constructive or safety reasons, as well as sensor range, cause bounds in process variables, as in the case of levels in tanks, flows in pipes, and pressures in deposits. Furthermore, in practice, the operating points of plants are determined to satisfy economic goals and lie at the intersection of certain constraints. The control system normally operates close to the limits and constraint violations are likely to occur. The control system, especially for longrange predictive control, has to anticipate constraint violations and correct them in an appropriate way. Although input and output constraints are basically treated in the same way, as is shown in this chapter, the implications of the constraints differ. Output constraints are mainly due to safety reasons and must be controlled in advance because output variables are affected by process dynamics. Input (or manipulated) variables can always be kept in bound by the controller by clipping the control action to a value satisfying amplitude and slew rate constraints.",
isbn="978-0-85729-398-5",
doi="10.1007/978-0-85729-398-5_7",
url="https://doi.org/10.1007/978-0-85729-398-5_7"
}

@article{kouvaritakis2016model,
  title={{Model Predictive Control}},
  author={Kouvaritakis, Basil and Cannon, Mark},
  journal={Switzerland: Springer International Publishing},
  volume={38},
  pages={13--56},
  year={2016},
  publisher={Springer},
url={https://link.springer.com/content/pdf/10.1007/978-3-319-24853-0.pdf}
}

@inproceedings{tassa2014control,
  title={{Control-limited Differential Dynamic Programming}},
  author={Tassa, Yuval and Mansard, Nicolas and Todorov, Emo},
  booktitle={2014 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1168--1175},
  year={2014},
  organization={IEEE},
url={https://ieeexplore.ieee.org/abstract/document/6907001?casa_token=kpQZ0icP4vsAAAAA:96xQh2IckQOyfE0N-C4E-xSZFwcuIDHfEA0H41ETzA-AjrxImIDm8d2094xzFwYtMWArXsMang}
}

@inproceedings{tassa2012synthesis,
  title={{Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization}},
  author={Tassa, Yuval and Erez, Tom and Todorov, Emanuel},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={4906--4913},
  year={2012},
  organization={IEEE},
url={https://ieeexplore.ieee.org/abstract/document/6386025?casa_token=4lVix4Q39RwAAAAA:FOn758w8oWBrJT9js8zm0rWHbND2hfEo70YLBJo8ZC-hac6W_BZ-kXMDj6hUUM5D2_cOjXiu5w}
}

@inproceedings{NEURIPS2018_ba6d843e,
 author = {Amos, Brandon and Jimenez, Ivan and Sacks, Jacob and Boots, Byron and Kolter, J. Zico},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {{Differentiable MPC for End-to-end Planning and Control}},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/ba6d843eb4251a4526ce65d1807a9309-Paper.pdf},
 volume = {31},
 year = {2018}
}
@inproceedings{nagabandi2020deep,
  title={{Deep Dynamics Models for Learning Dexterous Manipulation}},
  author={Nagabandi, Anusha and Konolige, Kurt and Levine, Sergey and Kumar, Vikash},
  booktitle={Conference on Robot Learning},
  pages={1101--1112},
  year={2020},
  organization={PMLR},
  url={https://proceedings.mlr.press/v100/nagabandi20a.html}
}
@article{ji2016path,
  title={{Path Planning and Tracking for Vehicle Collision Avoidance Based on Model Predictive Control With Multiconstraints}},
  author={Ji, Jie and Khajepour, Amir and Melek, Wael William and Huang, Yanjun},
  journal={IEEE Transactions on Vehicular Technology},
  volume={66},
  number={2},
  pages={952--964},
  year={2016},
  url={https://ieeexplore.ieee.org/abstract/document/7458179},
  publisher={IEEE}
}
