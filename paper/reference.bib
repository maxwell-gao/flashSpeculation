
@misc{bansal_smaller_2024,
	title = {Smaller, Weaker, Yet Better: Training {LLM} Reasoners via Compute-Optimal Sampling},
	url = {http://arxiv.org/abs/2408.16737},
	doi = {10.48550/arXiv.2408.16737},
	shorttitle = {Smaller, Weaker, Yet Better},
	abstract = {Training on high-quality synthetic data from strong language models ({LMs}) is a common strategy to improve the reasoning performance of {LMs}. In this work, we revisit whether this strategy is compute-optimal under a fixed inference budget (e.g., {FLOPs}). To do so, we investigate the trade-offs between generating synthetic data using a stronger but more expensive ({SE}) model versus a weaker but cheaper ({WC}) model. We evaluate the generated data across three key metrics: coverage, diversity, and false positive rate, and show that the data from {WC} models may have higher coverage and diversity, but also exhibit higher false positive rates. We then finetune {LMs} on data from {SE} and {WC} models in different settings: knowledge distillation, self-improvement, and a novel weak-to-strong improvement setup where a weaker {LM} teaches reasoning to a stronger {LM}. Our findings reveal that models finetuned on {WC}-generated data consistently outperform those trained on {SE}-generated data across multiple benchmarks and multiple choices of {WC} and {SE} models. These results challenge the prevailing practice of relying on {SE} models for synthetic data generation, suggesting that {WC} may be the compute-optimal approach for training advanced {LM} reasoners.},
	number = {{arXiv}:2408.16737},
	publisher = {{arXiv}},
	author = {Bansal, Hritik and Hosseini, Arian and Agarwal, Rishabh and Tran, Vinh Q. and Kazemi, Mehran},
	urldate = {2026-01-25},
	date = {2024-10-07},
	eprinttype = {arxiv},
	eprint = {2408.16737 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {/2BGSWZMW/Bansal et al. - Smaller, Weaker, Yet Better Training LLM Reasoners via Compute-Optimal Sampling.pdf:application/pdf;Z27T27HU/Bansal et al. - 2024 - Smaller, Weaker, Yet Better Training LLM Reasoners via Compute-Optimal Sampling.pdf:application/pdf;Snapshot:/Users/maxwell/Library/CloudStorage/OneDrive-个人/Zotero/storage/X53XYTCD/2408.html:text/html},
}

@misc{karan_reasoning_2025,
	title = {Reasoning with Sampling: Your Base Model is Smarter Than You Think},
	url = {http://arxiv.org/abs/2510.14901},
	doi = {10.48550/arXiv.2510.14901},
	shorttitle = {Reasoning with Sampling},
	abstract = {Frontier reasoning models have exhibited incredible capabilities across a wide array of disciplines, driven by posttraining large language models ({LLMs}) with reinforcement learning ({RL}). However, despite the widespread success of this paradigm, much of the literature has been devoted to disentangling truly novel behaviors that emerge during {RL} but are not present in the base models. In our work, we approach this question from a different angle, instead asking whether comparable reasoning capabilites can be elicited from base models at inference time by pure sampling, without any additional training. Inspired by Markov chain Monte Carlo ({MCMC}) techniques for sampling from sharpened distributions, we propose a simple iterative sampling algorithm leveraging the base models' own likelihoods. Over different base models, we show that our algorithm offers substantial boosts in reasoning that nearly match and even outperform those from {RL} on a wide variety of single-shot tasks, including {MATH}500, {HumanEval}, and {GPQA}. Moreover, our sampler avoids the collapse in diversity over multiple samples that is characteristic of {RL}-posttraining. Crucially, our method does not require training, curated datasets, or a verifier, suggesting broad applicability beyond easily verifiable domains.},
	number = {{arXiv}:2510.14901},
	publisher = {{arXiv}},
	author = {Karan, Aayush and Du, Yilun},
	urldate = {2026-02-04},
	date = {2025-10-16},
	eprinttype = {arxiv},
	eprint = {2510.14901 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {9SEY85EU/Karan and Du - 2025 - Reasoning with Sampling Your Base Model is Smarter Than You Think.pdf:application/pdf;Snapshot:/Users/maxwell/Library/CloudStorage/OneDrive-个人/Zotero/storage/X8XRYXF7/2510.html:text/html},
}

@misc{chen_dflash_2026,
	title = {{DFlash}: Block Diffusion for Flash Speculative Decoding},
	url = {http://arxiv.org/abs/2602.06036},
	doi = {10.48550/arXiv.2602.06036},
	shorttitle = {{DFlash}},
	abstract = {Autoregressive large language models ({LLMs}) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor {GPU} utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target {LLM}; however, existing methods still rely on autoregressive drafting, which remains sequential and limits practical speedups. Diffusion {LLMs} offer a promising alternative by enabling parallel generation, but current diffusion models typically underperform compared with autoregressive models. In this paper, we introduce {DFlash}, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. By generating draft tokens in a single forward pass and conditioning the draft model on context features extracted from the target model, {DFlash} enables efficient drafting with high-quality outputs and higher acceptance rates. Experiments show that {DFlash} achieves over 6x lossless acceleration across a range of models and tasks, delivering up to 2.5x higher speedup than the state-of-the-art speculative decoding method {EAGLE}-3.},
	number = {{arXiv}:2602.06036},
	publisher = {{arXiv}},
	author = {Chen, Jian and Liang, Yesheng and Liu, Zhijian},
	urldate = {2026-02-11},
	date = {2026-02-05},
	eprinttype = {arxiv},
	eprint = {2602.06036 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {EFM8UUAF/Chen et al. - 2026 - DFlash Block Diffusion for Flash Speculative Decoding.pdf:application/pdf;Snapshot:/Users/maxwell/Library/CloudStorage/OneDrive-个人/Zotero/storage/LZUVC58J/2602.html:text/html},
}

@inproceedings{gera_benefits_2023,
	location = {Toronto, Canada},
	title = {The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers},
	url = {https://aclanthology.org/2023.acl-long.580},
	doi = {10.18653/v1/2023.acl-long.580},
	shorttitle = {The Benefits of Bad Advice},
	abstract = {Applying language models to natural language processing tasks typically relies on the representations in the final model layer, as intermediate hidden layer representations are presumed to be less informative. In this work, we argue that due to the gradual improvement across model layers, additional information can be gleaned from the contrast between higher and lower layers during inference. Specifically, in choosing between the probable next token predictions of a generative model, the predictions of lower layers can be used to highlight which candidates are best avoided. We propose a novel approach that utilizes the contrast between layers to improve text generation outputs, and show that it mitigates degenerative behaviors of the model in open-ended generation, significantly improving the quality of generated texts. Furthermore, our results indicate that contrasting between model layers at inference time can yield substantial benefits to certain aspects of general language model capabilities, more effectively extracting knowledge during inference from a given set of model parameters.},
	eventtitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	pages = {10406--10420},
	booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Gera, Ariel and Friedman, Roni and Arviv, Ofir and Gunasekara, Chulaka and Sznajder, Benjamin and Slonim, Noam and Shnarch, Eyal},
	urldate = {2026-02-16},
	date = {2023},
	langid = {english},
	file = {/3IDBMW8W/Gera et al. - 2023 - The Benefits of Bad Advice Autocontrastive Decoding across Model Layers.pdf:application/pdf},
}

@article{chang_real_2025,
	title = {{REAL} Sampling: Boosting Factuality and Diversity of Open-ended Generation by Extrapolating the Entropy of an Infinitely Large {LM}},
	volume = {13},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl_a_00757},
	doi = {10.1162/tacl_a_00757},
	shorttitle = {{REAL} Sampling},
	abstract = {Decoding methods for large language models ({LLMs}) usually struggle with the
tradeoff between ensuring factuality and maintaining diversity. In this paper,
we propose {REAL} (Residual Entropy from Asymptotic Line) sampling,1 which predicts the step-wise hallucination
likelihood of an {LLM}. When an {LLM} is likely to hallucinate, {REAL} lowers the p threshold in nucleus sampling. Otherwise, {REAL} sampling
increases the p threshold to boost the diversity. To predict
the step-wise hallucination likelihood without supervision, we construct a {THF}
(Token-level Hallucination Forecasting) model, which predicts the asymptotic entropy (i.e.,
inherent uncertainty) of the next token by extrapolating the next-token
entropies of an infinitely large language model from a series of {LLMs} with
different sizes. If an {LLM}’s entropy is higher than the asymptotic
entropy (i.e., the {LLM} is more uncertain than it should be), the {THF} model
predicts a high hallucination hazard, which leads to a lower p threshold in {REAL} sampling. In the {FactualityPrompts} benchmark (Lee et
al., 2022), we demonstrate that {REAL}
sampling based on a 70M {THF} model can substantially improve the factuality and
diversity of 7B {LLMs} simultaneously. After combined with contrastive decoding,
{REAL} sampling outperforms 13 sampling methods, and generates texts that are more
factual than the greedy sampling and more diverse than the nucleus sampling with p = 0.5.},
	pages = {760--783},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	shortjournal = {Transactions of the Association for Computational Linguistics},
	author = {Chang, Haw-Shiuan and Peng, Nanyun and Bansal, Mohit and Ramakrishna, Anil and Chung, Tagyoung},
	urldate = {2026-02-16},
	date = {2025-07-18},
	file = {Full Text /AAMN8N6R/Chang et al. - 2025 - REAL Sampling Boosting Factuality and Diversity of Open-ended Generation by Extrapolating the Entro.pdf:application/pdf;Snapshot:/Users/maxwell/Library/CloudStorage/OneDrive-个人/Zotero/storage/4RU8P9E9/tacl_a_00757.html:text/html},
}

@misc{song_evaluation_2024,
	title = {Evaluation of {LLMs} Should Not Ignore Non-Determinism},
	url = {http://arxiv.org/abs/2407.12004},
	author = {Song, Yifan and Wang, Guoyin and Li, Sujian and Lin, Bill Yuchen},
	date = {2024},
	langid = {english},
}

@misc{kang_scalable_2025,
	title = {Scalable Best-of-N Selection for Large Language Models via Self-Certainty},
	url = {http://arxiv.org/abs/2502.07390},
	author = {Kang, Zhewei and Zhao, Xuandong and Song, Dawn},
	date = {2025},
	langid = {english},
}

@misc{ghandeharioun_patchscopes_2024,
	title = {Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models},
	url = {http://arxiv.org/abs/2401.06102},
	doi = {10.48550/arXiv.2401.06102},
	shorttitle = {Patchscopes},
	abstract = {Understanding the internal representations of large language models ({LLMs}) can help explain models' behavior and verify their alignment with human values. Given the capabilities of {LLMs} in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an {LLM}'s computation. We show that many prior interpretability methods based on projecting representations into the vocabulary space and intervening on the {LLM} computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and multihop reasoning error correction.},
	number = {{arXiv}:2401.06102},
	publisher = {{arXiv}},
	author = {Ghandeharioun, Asma and Caciularu, Avi and Pearce, Adam and Dixon, Lucas and Geva, Mor},
	urldate = {2026-02-16},
	date = {2024-05-30},
	eprinttype = {arxiv},
	eprint = {2401.06102 [cs]},
	note = {version: 3},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {CVSEGQ7R/Ghandeharioun et al. - 2024 - Patchscopes A Unifying Framework for Inspecting Hidden Representations of Language Models.pdf:application/pdf;Snapshot:/Users/maxwell/Library/CloudStorage/OneDrive-个人/Zotero/storage/UFKN38S4/2401.html:text/html},
}

@inproceedings{chen_-context_2024,
	location = {Vienna, Austria},
	title = {In-context sharpness as alerts: an inner representation perspective for hallucination mitigation},
	volume = {235},
	series = {{ICML}'24},
	shorttitle = {In-context sharpness as alerts},
	abstract = {Large language models ({LLMs}) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited. In this study, we delve into the underlying mechanisms of {LLM} hallucinations from the perspective of inner representations, and discover a salient pattern associated with hallucinations: correct generations tend to have sharper context activations in the hidden states of the in-context tokens, compared to the incorrect ones. Leveraging this insight, we propose an entropy-based metric to quantify the "sharpness" among the in-context hidden states and incorporate it into the decoding process to formulate a constrained decoding approach. Experiments on various knowledge-seeking and hallucination benchmarks demonstrate our approach's consistent effectiveness, for example, achieving up to an 8.6 point improvement on {TruthfulQA}. We believe this study can improve our understanding of hallucinations and serve as a practical solution for hallucination mitigation. Code is publicly available at https://github.com/hkustnlp/Activation Decoding.},
	pages = {7553--7567},
	booktitle = {Proceedings of the 41st International Conference on Machine Learning},
	publisher = {{JMLR}.org},
	author = {Chen, Shiqi and Xiong, Miao and Liu, Junteng and Wu, {ZhengXuan} and Xiao, Teng and Gao, Siyang and He, Junxian},
	urldate = {2026-02-16},
	date = {2024},
}

@inproceedings{sanchez_stay_2024,
	title = {Stay on Topic with Classifier-Free Guidance},
	url = {https://openreview.net/forum?id=RiM3cl9MdK},
	abstract = {Classifier-Free Guidance ({CFG}) has recently emerged in as a lightweight technique to encourage prompt-adherence in generations, yet has not yet been successfully applied to language modeling. In this work, we demonstrate across a wide array of benchmarks that {CFG} can be used broadly as an inference-time technique in pure language modeling. We show that {CFG} (1) improves the performance of Pythia, {GPT}-2 and {LLaMA}-family models across: Q\&A, reasoning, code generation, and machine translation, achieving {SOTA} on {LAMBADA} with {LLaMA}-7B over {PaLM}-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in human evaluations we show a 75\% preference for using {CFG} over baseline.},
	eventtitle = {Forty-first International Conference on Machine Learning},
	author = {Sanchez, Guillaume and Spangher, Alexander and Fan, Honglu and Levi, Elad and Biderman, Stella},
	urldate = {2026-02-16},
	date = {2024-06-06},
	langid = {english},
	file = {Full Text /CMK7S6HL/Sanchez et al. - 2024 - Stay on Topic with Classifier-Free Guidance.pdf:application/pdf},
}

@inproceedings{chang_explaining_2024,
	location = {Miami, Florida, {USA}},
	title = {Explaining and Improving Contrastive Decoding by Extrapolating the Probabilities of a Huge and Hypothetical {LM}},
	url = {https://aclanthology.org/2024.emnlp-main.484},
	doi = {10.18653/v1/2024.emnlp-main.484},
	abstract = {Contrastive decoding ({CD}) (Li et al., 2023) improves the next-token distribution of a large expert language model ({LM}) using a small amateur {LM}. Although {CD} is applied to various {LMs} and domains to enhance open-ended text generation, it is still unclear why {CD} often works well, when it could fail, and how we can make it better. To deepen our understanding of {CD}, we first theoretically prove that {CD} could be viewed as linearly extrapolating the next-token logits from a huge and hypothetical {LM}. We also highlight that the linear extrapolation could make {CD} unable to output the most obvious answers that have already been assigned high probabilities by the amateur {LM}.},
	eventtitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
	pages = {8503--8526},
	booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Chang, Haw-Shiuan and Peng, Nanyun and Bansal, Mohit and Ramakrishna, Anil and Chung, Tagyoung},
	urldate = {2026-02-16},
	date = {2024},
	langid = {english},
	file = {/2FLZNRSD/Chang et al. - 2024 - Explaining and Improving Contrastive Decoding by Extrapolating the Probabilities of a Huge and Hypot.pdf:application/pdf},
}

@inproceedings{chuang_dola_2023,
	title = {{DoLa}: Decoding by Contrasting Layers Improves Factuality in Large Language Models},
	url = {https://openreview.net/forum?id=Th6NyL07na},
	shorttitle = {{DoLa}},
	abstract = {Despite their impressive capabilities, large language models ({LLMs}) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained {LLMs} that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an {LLMs} has generally been shown to be localized to particular transformer layers. We find that this **D**ecoding by C**o**ntrasting **La**yers ({DoLa}) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. {DoLa} consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of {LLaMA} family models on {TruthfulQA} by 12-17\% absolute points, demonstrating its potential in making {LLMs} reliably generate truthful facts.},
	eventtitle = {The Twelfth International Conference on Learning Representations},
	author = {Chuang, Yung-Sung and Xie, Yujia and Luo, Hongyin and Kim, Yoon and Glass, James R. and He, Pengcheng},
	urldate = {2026-02-16},
	date = {2023-10-13},
	langid = {english},
	file = {Full Text /JP8C2KCQ/Chuang et al. - 2023 - DoLa Decoding by Contrasting Layers Improves Factuality in Large Language Models.pdf:application/pdf},
}

@article{kuribayashi_large_2025,
	title = {Large Language Models Are Human-Like Internally},
	volume = {13},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/TACL.a.58},
	doi = {10.1162/TACL.a.58},
	abstract = {Recent cognitive modeling studies have reported that larger language models ({LMs}) exhibit a poorer fit to human reading behavior (Oh and Schuler, 2023b; Shain et al., 2024; Kuribayashi et al., 2024), leading to claims of their cognitive implausibility. In this paper, we revisit this argument through the lens of mechanistic interpretability and argue that prior conclusions were skewed by an exclusive focus on the final layers of {LMs}. Our analysis reveals that next-word probabilities derived from internal layers of larger {LMs} align with human sentence processing data as well as, or better than, those from smaller {LMs}. This alignment holds consistently across behavioral (self-paced reading times, gaze durations, {MAZE} task processing times) and neurophysiological (N400 brain potentials) measures, challenging earlier mixed results and suggesting that the cognitive plausibility of larger {LMs} has been underestimated. Furthermore, we first identify an intriguing relationship between {LM} layers and human measures: Earlier layers correspond more closely with fast gaze durations, while later layers better align with relatively slower signals such as N400 potentials and {MAZE} processing times. Our work opens new avenues for interdisciplinary research at the intersection of mechanistic interpretability and cognitive modeling.1},
	pages = {1743--1766},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	shortjournal = {Transactions of the Association for Computational Linguistics},
	author = {Kuribayashi, Tatsuki and Oseki, Yohei and Taieb, Souhaib Ben and Inui, Kentaro and Baldwin, Timothy},
	urldate = {2026-02-16},
	date = {2025-12-03},
	file = {Full Text /Y8M49YA8/Kuribayashi et al. - 2025 - Large Language Models Are Human-Like Internally.pdf:application/pdf;Snapshot:/Users/maxwell/Library/CloudStorage/OneDrive-个人/Zotero/storage/3TAGNI5U/TACL.a.html:text/html},
}

@misc{gao_interpretable_2024,
	title = {Interpretable Contrastive Monte Carlo Tree Search Reasoning},
	url = {http://arxiv.org/abs/2410.01707},
	doi = {10.48550/arXiv.2410.01707},
	abstract = {We propose {SC}-{MCTS}*: a novel Monte Carlo Tree Search ({MCTS}) reasoning algorithm for Large Language Models ({LLMs}), significantly improves both reasoning accuracy and speed. Our motivation comes from: 1. Previous {MCTS} {LLM} reasoning works often overlooked its biggest drawback--slower speed compared to {CoT}; 2. Previous research mainly used {MCTS} as a tool for {LLM} reasoning on various tasks with limited quantitative analysis or ablation studies of its components from reasoning interpretability perspective. 3. The reward model is the most crucial component in {MCTS}, however previous work has rarely conducted in-depth study or improvement of {MCTS}'s reward models. Thus, we conducted extensive ablation studies and quantitative analysis on components of {MCTS}, revealing the impact of each component on the {MCTS} reasoning performance of {LLMs}. Building on this, (i) we designed a highly interpretable reward model based on the principle of contrastive decoding and (ii) achieved an average speed improvement of 51.9\% per node using speculative decoding. Additionally, (iii) we improved {UCT} node selection strategy and backpropagation used in previous works, resulting in significant performance improvement. We outperformed o1-mini by an average of 17.4\% on the Blocksworld multi-step reasoning dataset using Llama-3.1-70B with {SC}-{MCTS}*. Our code is available at https://github.com/zitian-gao/{SC}-{MCTS}.},
	number = {{arXiv}:2410.01707},
	publisher = {{arXiv}},
	author = {Gao, Zitian and Niu, Boye and He, Xuzheng and Xu, Haotian and Liu, Hongzhang and Liu, Aiwei and Hu, Xuming and Wen, Lijie},
	urldate = {2026-02-16},
	date = {2024-12-25},
	eprinttype = {arxiv},
	eprint = {2410.01707 [cs]},
	note = {version: 3},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {TVSHG3NR/Gao et al. - 2024 - Interpretable Contrastive Monte Carlo Tree Search Reasoning.pdf:application/pdf;Snapshot:/Users/maxwell/Library/CloudStorage/OneDrive-个人/Zotero/storage/YA7TE23Y/2410.html:text/html},
}

@inproceedings{csordas_language_2025,
	title = {Do Language Models Use Their Depth Efficiently?},
	url = {https://openreview.net/forum?id=Kz6eUL86XP},
	abstract = {Modern {LLMs} are increasingly deep, and depth correlates with performance, albeit with diminishing returns. However, do these models use their depth efficiently? Do they compose more features to create higher-order computations that are impossible in shallow models, or do they merely spread the same kinds of computation out over more layers? To address these questions, we analyze the residual stream of the Llama 3.1, Qwen 3, and {OLMo} 2 family of models. We find: First, comparing the output of the sublayers to the residual stream reveals that layers in the second half contribute much less than those in the first half, with a clear phase transition between the two halves. Second, skipping layers in the second half has a much smaller effect on future computations and output predictions. Third, for multihop tasks, we are unable to find evidence that models are using increased depth to compose subresults in examples involving many hops. Fourth, we seek to directly address whether deeper models are using their additional layers to perform new kinds of computation. To do this, we train linear maps from the residual stream of a shallow model to a deeper one. We find that layers with the same relative depth map best to each other, suggesting that the larger model simply spreads the same computations out over its many layers. All this evidence suggests that deeper models are not using their depth to learn new kinds of computation, but only using the greater depth to perform more fine-grained adjustments to the residual. This may help explain why increasing scale leads to diminishing returns for stacked Transformer architectures.},
	eventtitle = {The Thirty-ninth Annual Conference on Neural Information Processing Systems},
	author = {Csordás, Róbert and Manning, Christopher D. and Potts, Christopher},
	urldate = {2026-02-16},
	date = {2025-10-29},
	langid = {english},
	file = {Full Text /LJMGDTNN/Csordás et al. - 2025 - Do Language Models Use Their Depth Efficiently.pdf:application/pdf},
}

@inproceedings{lu_reassessing_2025,
	title = {Reassessing Layer Pruning in {LLMs}: New Insights and Methods},
	url = {https://openreview.net/forum?id=04Tfwy3LLC},
	shorttitle = {Reassessing Layer Pruning in {LLMs}},
	abstract = {Although large language models ({LLMs}) have achieved remarkable success across various domains, their considerable scale necessitates substantial computational resources, posing significant challenges for deployment in resource-constrained environments. Layer pruning, as a simple yet effective compression method, removes layers of a model directly, reducing computational overhead. However, what are the best practices for layer pruning in {LLMs}? Are sophisticated layer selection metrics truly effective? Does the {LoRA} (Low-Rank Approximation) family, widely regarded as a leading method for pruned model fine-tuning, truly meet expectations when applied to post-pruning fine-tuning? To answer these questions, we dedicate thousands of {GPU} hours to benchmarking layer pruning in {LLMs} and gaining insights across multiple dimensions. Our results demonstrate that a simple approach, i.e., pruning the final layers followed by fine-tuning the lm{\textbackslash}\_head and the remaining last three layers, yields remarkably strong performance. These pruning strategies are further supported by theoretical analyses based on the gradient flow. Following this guide, our method surpasses existing state-of-the-art pruning methods by \$5.62{\textbackslash}\%\$–\$17.27{\textbackslash}\%\$ on Llama-3.1-8B-It, by \$2.36{\textbackslash}\%\$–\$19.45{\textbackslash}\%\$ on Llama-3-8B and by \$4.34{\textbackslash}\%\$–\$9.59{\textbackslash}\%\$ on Llama-3-70B. The code is available at at https://github.com/yaolu-zjut/Navigation\_LLM\_layer\_pruning.},
	eventtitle = {The Fourteenth International Conference on Learning Representations},
	author = {Lu, Yao and Cheng, Hao and Fang, Yujie and Wang, Zeyu and Wei, Jiaheng and Xu, Dongwei and Xuan, Qi and Zhu, Zhaowei},
	urldate = {2026-02-16},
	date = {2025-10-08},
	langid = {english},
	file = {Full Text /A2DLBFK3/Lu et al. - 2025 - Reassessing Layer Pruning in LLMs New Insights and Methods.pdf:application/pdf},
}

@inproceedings{yang_breaking_2018,
	title = {Breaking the Softmax Bottleneck: A High-Rank {RNN} Language Model},
	url = {https://openreview.net/forum?id=HkwZSG-CZ},
	booktitle = {International Conference on Learning Representations},
	author = {Yang, Zhilin and Dai, Zihang and Salakhutdinov, Ruslan and Cohen, William W.},
	date = {2018},
	langid = {english},
}

@misc{zhang_logit_2025,
	title = {Logit Arithmetic Elicits Long Reasoning Capabilities Without Training},
	url = {http://arxiv.org/abs/2510.09354},
	doi = {10.48550/arXiv.2510.09354},
	abstract = {Large reasoning models exhibit long chain-of-thought reasoning with strategies such as backtracking and self-correction, though recent studies suggest that these abilities typically require additional training. We first investigate whether such behaviors can be elicited without any training. To this end, we propose a decoding-time approach, {ThinkLogit}, which utilizes logit arithmetic to tune a target large non-reasoning model for long reasoning using a substantially smaller reasoning model as the guider. We then show that we can further boost its performance by training the guider model with preference optimization over correct/incorrect reasoning pairs sampled from both the target and guider model, a setup we refer to as {ThinkLogit}-{DPO}. Our experiments demonstrate that {ThinkLogit} and {ThinkLogit}-{DPO} achieve a relative improvement in average accuracy by 24.5\% and 29.1\%, respectively, over five reasoning benchmarks using the Qwen2.5-32B guided by R1-Distill-Qwen-1.5B, a model 21x smaller. Moreover, we find that {ThinkLogit} remains effective when the guider and target come from different model families. It is also orthogonal to post-training methods for small models, as guiders improved through supervised distillation or reinforcement learning can be directly plugged in to yield stronger large models, offering a practical path to unlock long reasoning in large-scale models without costly post-training.},
	number = {{arXiv}:2510.09354},
	publisher = {{arXiv}},
	author = {Zhang, Yunxiang and Khalifa, Muhammad and Zhang, Lechen and Liu, Xin and Lee, Ayoung and Zhang, Xinliang Frederick and Bayat, Farima Fatahi and Wang, Lu},
	urldate = {2026-02-16},
	date = {2025-10-10},
	eprinttype = {arxiv},
	eprint = {2510.09354 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {4P83H8CB/Zhang et al. - 2025 - Logit Arithmetic Elicits Long Reasoning Capabilities Without Training.pdf:application/pdf;Snapshot:/Users/maxwell/Library/CloudStorage/OneDrive-个人/Zotero/storage/T6BIUZ8F/2510.html:text/html},
}

@inproceedings{minh_turning_2024,
	title = {Turning Up the Heat: Min-p Sampling for Creative and Coherent {LLM} Outputs},
	url = {https://openreview.net/forum?id=FBkpCyujtS},
	shorttitle = {Turning Up the Heat},
	abstract = {Large Language Models ({LLMs}) generate text by sampling the next token from a probability distribution over the vocabulary at each decoding step. Popular sampling methods like top-p (nucleus sampling) often struggle to balance quality and diversity, especially at higher temperatures which lead to incoherent or repetitive outputs. We propose min-p sampling, a dynamic truncation method that adjusts the sampling threshold based on the model's confidence by using the top token's probability as a scaling factor. Our experiments on benchmarks including {GPQA}, {GSM}8K, and {AlpacaEval} Creative Writing show that min-p sampling improves both the quality and diversity of generated text across different model families (Mistral and Llama 3) and model sizes (1B to 123B parameters), especially at higher temperatures. Human evaluations further show a clear preference for min-p sampling, in both text quality and creativity. Min-p sampling has been adopted by popular open-source {LLM} frameworks, including Hugging Face Transformers, {VLLM}, and many others, highlighting its considerable impact on improving text generation quality.},
	eventtitle = {The Thirteenth International Conference on Learning Representations},
	author = {Minh, Nguyen Nhat and Baker, Andrew and Neo, Clement and Roush, Allen G. and Kirsch, Andreas and Shwartz-Ziv, Ravid},
	urldate = {2026-02-16},
	date = {2024-10-04},
	langid = {english},
	file = {Full Text /T5J3TR9H/Minh et al. - 2024 - Turning Up the Heat Min-p Sampling for Creative and Coherent LLM Outputs.pdf:application/pdf},
}